<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
    <channel>
        <title>docker - 标签 - Qingwave</title>
        <link>https://qingwave.github.io/tags/docker/</link>
        <description>docker - 标签 - Qingwave</description>
        <generator>Hugo -- gohugo.io</generator><language>zh-CN</language><managingEditor>isguory@gmail.com (Qingwave)</managingEditor>
            <webMaster>isguory@gmail.com (Qingwave)</webMaster><lastBuildDate>Wed, 03 Feb 2021 15:13:10 &#43;0000</lastBuildDate><atom:link href="https://qingwave.github.io/tags/docker/" rel="self" type="application/rss+xml" /><item>
    <title>k8s中shell脚本启动如何传递信号</title>
    <link>https://qingwave.github.io/docker-shell-signal/</link>
    <pubDate>Wed, 03 Feb 2021 15:13:10 &#43;0000</pubDate><author>
        <name>qinng</name>
    </author><guid>https://qingwave.github.io/docker-shell-signal/</guid>
    <description><![CDATA[<h2 id="背景" class="headerLink">
    <a href="#%e8%83%8c%e6%99%af" class="header-mark"></a>背景</h2><p>在k8s或docker中，有时候我们需要通过shell来启动程序，但是默认shell不会传递信号（sigterm）给子进程，当在pod终止时应用无法优雅退出，直到最大时间时间后强制退出（<code>kill -9</code>）。</p>
<h2 id="分析" class="headerLink">
    <a href="#%e5%88%86%e6%9e%90" class="header-mark"></a>分析</h2><p>普通情况下，大多业务的启动命令如下</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">command: <span class="o">[</span><span class="s2">&#34;binary&#34;</span>, <span class="s2">&#34;-flags&#34;</span>, ...<span class="o">]</span>
</span></span></code></pre></div><p>主进程做为1号进程会收到<code>sigterm</code>信号，优雅退出(需要程序捕获信号); 而通过脚本启动时，<code>shell</code>作为1号进程，不会显示传递信号给子进程，造成子进程无法优雅退出，直到最大退出时间后强制终止。</p>
<h2 id="解决方案" class="headerLink">
    <a href="#%e8%a7%a3%e5%86%b3%e6%96%b9%e6%a1%88" class="header-mark"></a>解决方案</h2><h3 id="exec" class="headerLink">
    <a href="#exec" class="header-mark"></a>exec</h3><p>如何只需一个进程收到信号，可通过<code>exec</code>，<code>exec</code>会替换当前shell进程，即<code>pid</code>不变</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl"><span class="cp">#! /bin/bash
</span></span></span><span class="line"><span class="cl"><span class="cp"></span><span class="c1"># do something</span>
</span></span><span class="line"><span class="cl"><span class="nb">exec</span> binay -flags ...
</span></span></code></pre></div><p>正常情况测试命令如下，使用sleep来模拟应用<code>sh -c 'echo &quot;start&quot;; sleep 100'</code>：
<code>pstree</code>展示如下，<code>sleep</code>进程会生成一个子进程</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">bash(28701)───sh(24588)───sleep(24589)
</span></span></code></pre></div><p>通过<code>exec</code>运行后，命令<code>sh -c 'echo &quot;start&quot;; exec sleep 100'</code></p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">bash(28701)───sleep(24664)
</span></span></code></pre></div><p>加入<code>exec</code>后，<code>sleep</code>进程替换了shell进程，没有生成子进程</p>
<p>此种方式可以收到信号，但只适用于一个子进程的情况</p>
<h3 id="trap" class="headerLink">
    <a href="#trap" class="header-mark"></a>trap</h3><p>在shell中可以显示通过<code>trap</code>捕捉信号传递给子进程</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl"><span class="cp">#!/bin/bash
</span></span></span><span class="line"><span class="cl"><span class="cp"></span><span class="nb">echo</span> <span class="s2">&#34;start&#34;</span>
</span></span><span class="line"><span class="cl">binary -flags... <span class="p">&amp;</span>
</span></span><span class="line"><span class="cl"><span class="nv">pid</span><span class="o">=</span><span class="s2">&#34;</span><span class="nv">$!</span><span class="s2">&#34;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">_kill<span class="o">()</span> <span class="o">{</span>
</span></span><span class="line"><span class="cl">  <span class="nb">echo</span> <span class="s2">&#34;receive sigterm&#34;</span>
</span></span><span class="line"><span class="cl">  <span class="nb">kill</span> <span class="nv">$pid</span> <span class="c1">#传递给子进程</span>
</span></span><span class="line"><span class="cl">  <span class="nb">wait</span> <span class="nv">$pid</span>
</span></span><span class="line"><span class="cl">  <span class="nb">exit</span> <span class="m">0</span>
</span></span><span class="line"><span class="cl"><span class="o">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="nb">trap</span> _kill SIGTERM <span class="c1">#捕获信号</span>
</span></span><span class="line"><span class="cl"><span class="nb">wait</span> <span class="c1">#等待子进程退出</span>
</span></span></code></pre></div><p>此种方式需要改动启动脚本，显示传递信号给子进程</p>
<h2 id="docker-init" class="headerLink">
    <a href="#docker-init" class="header-mark"></a>docker-init</h2><p><a href="https://docs.docker.com/engine/reference/run/#specify-an-init-process" target="_blank" rel="noopener noreffer">docker-init</a>即在docker启动时加入<code>--init</code>参数，docker-int会作为一号进程，会向子进程传递信号并且会回收僵尸进程。</p>
<p>遗憾的是k8s并不支持<code>--init</code>参数，用户可在镜像中声明init进程，更多可参考<a href="./container-init.md" rel="">container-init</a></p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-Dockerfile" data-lang="Dockerfile"><span class="line"><span class="cl"><span class="k">RUN</span> wget -O /usr/bin/dumb-init https://github.com/Yelp/dumb-init/releases/download/v1.2.2/dumb-init_1.2.2_amd64<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span><span class="k">RUN</span> chmod +x /usr/bin/dumb-init<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span><span class="k">ENTRYPOINT</span> <span class="p">[</span><span class="s2">&#34;/usr/bin/dumb-init&#34;</span><span class="p">,</span> <span class="s2">&#34;-v&#34;</span><span class="p">,</span> <span class="s2">&#34;--&#34;</span><span class="p">]</span><span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span><span class="k">CMD</span> <span class="p">[</span><span class="s2">&#34;nginx&#34;</span><span class="p">,</span> <span class="s2">&#34;-g&#34;</span><span class="p">,</span> <span class="s2">&#34;daemon off;&#34;</span><span class="p">]</span><span class="err">
</span></span></span></code></pre></div>]]></description>
</item><item>
    <title>开启shareProcessNamespace后容器异常</title>
    <link>https://qingwave.github.io/cotainer-init/</link>
    <pubDate>Tue, 28 Jul 2020 17:35:49 &#43;0000</pubDate><author>
        <name>qinng</name>
    </author><guid>https://qingwave.github.io/cotainer-init/</guid>
    <description><![CDATA[<h2 id="背景" class="headerLink">
    <a href="#%e8%83%8c%e6%99%af" class="header-mark"></a>背景</h2><p>目前k8s不支持容器启动顺序，部分业务通过开启<code>shareProcessNamespace</code>监控某些进程状态。当开启共享pid后，有用户反馈某个容器主进程退出，但是容器并没有重启，执行<code>exec</code>会卡住，现象参考<a href="3" rel="">issue</a></p>
<h2 id="复现" class="headerLink">
    <a href="#%e5%a4%8d%e7%8e%b0" class="header-mark"></a>复现</h2><ol>
<li>创建deployment</li>
</ol>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-yaml" data-lang="yaml"><span class="line"><span class="cl"><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l">apps/v1</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l">Deployment</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">metadata</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">labels</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">app</span><span class="p">:</span><span class="w"> </span><span class="l">nginx</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">nginx</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">spec</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">selector</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">matchLabels</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">      </span><span class="nt">app</span><span class="p">:</span><span class="w"> </span><span class="l">nginx</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">template</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">metadata</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">      </span><span class="nt">labels</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">        </span><span class="nt">app</span><span class="p">:</span><span class="w"> </span><span class="l">nginx</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">      </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">nginx</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">spec</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">      </span><span class="nt">shareProcessNamespace</span><span class="p">:</span><span class="w"> </span><span class="kc">true</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">      </span><span class="nt">containers</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">      </span>- <span class="nt">image</span><span class="p">:</span><span class="w"> </span><span class="l">nginx:alpine</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">        </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">nginx</span><span class="w">
</span></span></span></code></pre></div><ol start="2">
<li>查看进程信息
由于开启了<code>shareProcessNamespace</code>, <code>pause</code>变为<code>pid 1</code>, <code>nginx daemon</code>pid为<code>6</code>, ppid为<code>containerd-shim</code></li>
</ol>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl"><span class="c1"># 查看容器内进程</span>
</span></span><span class="line"><span class="cl">/ <span class="c1"># ps -efo &#34;pid,ppid,comm,args&#34;</span>
</span></span><span class="line"><span class="cl">PID   PPID  COMMAND          COMMAND
</span></span><span class="line"><span class="cl">    <span class="m">1</span>     <span class="m">0</span> pause            /pause
</span></span><span class="line"><span class="cl">    <span class="m">6</span>     <span class="m">0</span> nginx            nginx: master process nginx -g daemon off<span class="p">;</span>
</span></span><span class="line"><span class="cl">   <span class="m">11</span>     <span class="m">6</span> nginx            nginx: worker process
</span></span><span class="line"><span class="cl">   <span class="m">12</span>     <span class="m">6</span> nginx            nginx: worker process
</span></span><span class="line"><span class="cl">   <span class="m">13</span>     <span class="m">6</span> nginx            nginx: worker process
</span></span><span class="line"><span class="cl">   <span class="m">14</span>     <span class="m">6</span> nginx            nginx: worker process
</span></span><span class="line"><span class="cl">   <span class="m">15</span>     <span class="m">0</span> sh               sh
</span></span><span class="line"><span class="cl">   <span class="m">47</span>    <span class="m">15</span> ps               ps -efo pid,ppid,comm,args
</span></span></code></pre></div><ol start="3">
<li>删除主进程
子进程被<code>pid 1</code>回收, 有时也会被<code>containerd-shim</code>回收</li>
</ol>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">/ <span class="c1"># kill -9 6</span>
</span></span><span class="line"><span class="cl">/ <span class="c1"># </span>
</span></span><span class="line"><span class="cl">/ <span class="c1"># ps -efo &#34;pid,ppid,comm,args&#34;</span>
</span></span><span class="line"><span class="cl">PID   PPID  COMMAND          COMMAND
</span></span><span class="line"><span class="cl">    <span class="m">1</span>     <span class="m">0</span> pause            /pause
</span></span><span class="line"><span class="cl">   <span class="m">11</span>     <span class="m">1</span> nginx            nginx: worker process
</span></span><span class="line"><span class="cl">   <span class="m">12</span>     <span class="m">1</span> nginx            nginx: worker process
</span></span><span class="line"><span class="cl">   <span class="m">13</span>     <span class="m">1</span> nginx            nginx: worker process
</span></span><span class="line"><span class="cl">   <span class="m">14</span>     <span class="m">1</span> nginx            nginx: worker process
</span></span><span class="line"><span class="cl">   <span class="m">15</span>     <span class="m">0</span> sh               sh
</span></span><span class="line"><span class="cl">   <span class="m">48</span>    <span class="m">15</span> ps               ps -efo pid,ppid,comm,args
</span></span></code></pre></div><ol start="4">
<li>docker hang
此时对此容器执行docker命令(<code>inspect, logs, exec</code>)将卡住， 同样通过<code>kubectl</code>执行会超时。</li>
</ol>
<h2 id="分析" class="headerLink">
    <a href="#%e5%88%86%e6%9e%90" class="header-mark"></a>分析</h2><p>在未开启<code>shareProcessNamespace</code>的容器中，主进程退出<code>pid 1</code>, 此pid namespace销毁，系统会<code>kill</code>其下的所有进程。开启后，<code>pid 1</code>为<code>pause</code>进程，容器主进程退出，由于共享pid namespace，其他进程没有退出变成孤儿进程。此时调用docker相关接口去操作容器，docker首先去找主进程，但主进程已经不存在了，导致异常(待确认)。</p>
<p>清理掉这些孤儿进程容器便会正常退出，可以<code>kill</code>掉这些进程或者<code>kill</code>pause进程，即可恢复。</p>
<h2 id="方案" class="headerLink">
    <a href="#%e6%96%b9%e6%a1%88" class="header-mark"></a>方案</h2><p>有没有优雅的方式解决此种问题，如果主进程退出子进程也一起退出便符合预期，这就需要进程管理工具来实现，在宿主机中有<code>systemd</code>、<code>god</code>，容器中也有类似的工具即<code>init进程</code>(传递信息，回收子进程)，常见的有</p>
<ol>
<li><code>docker init</code>, docker自带的init进程(即<code>tini</code>)</li>
<li><a href="https://github.com/krallin/tini" target="_blank" rel="noopener noreffer"><code>tini</code></a>, 可回收孤儿进程/僵尸进程，<code>kill</code>进程组等</li>
<li><a href="https://github.com/Yelp/dumb-init" target="_blank" rel="noopener noreffer"><code>dumb-init</code></a>, 可管理进程，重写信号等</li>
</ol>
<p>经过测试，<code>tini</code>进程只能回收前台程序，对于后台程序则无能为力(例如<code>nohup</code>, <code>&amp;</code>启动的程序)，<code>dumb-init</code>在主进程退出时，会传递信号给子进程，符合预期。</p>
<p>开启<code>dumb-init</code>进程的<code>dockerfile</code>如下，<code>tini</code>也类似</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-Dockerfile" data-lang="Dockerfile"><span class="line"><span class="cl"><span class="k">FROM</span><span class="s"> nginx:alpine</span><span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span><span class="c"># tini</span><span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span><span class="c"># RUN apk add --no-cache tini</span><span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span><span class="c"># ENTRYPOINT [&#34;/sbin/tini&#34;, &#34;-s&#34;, &#34;-g&#34;, &#34;--&#34;]</span><span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span><span class="c"># dumb-init</span><span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span><span class="k">RUN</span> wget -O /usr/bin/dumb-init https://github.com/Yelp/dumb-init/releases/download/v1.2.2/dumb-init_1.2.2_amd64<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span><span class="k">RUN</span> chmod +x /usr/bin/dumb-init<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span><span class="k">ENTRYPOINT</span> <span class="p">[</span><span class="s2">&#34;/usr/bin/dumb-init&#34;</span><span class="p">,</span> <span class="s2">&#34;-v&#34;</span><span class="p">,</span> <span class="s2">&#34;--&#34;</span><span class="p">]</span><span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span><span class="k">CMD</span> <span class="p">[</span><span class="s2">&#34;nginx&#34;</span><span class="p">,</span> <span class="s2">&#34;-g&#34;</span><span class="p">,</span> <span class="s2">&#34;daemon off;&#34;</span><span class="p">]</span><span class="err">
</span></span></span></code></pre></div><p>init方式对于此问题是一种临时的解决方案，需要docker从根本上解决此种情况。容器推荐单进程运行，但某些情况必须要运行多进程，如果不想处理处理传递回收进程等，可以通过<code>init</code>进程，无需更改代码即可实现。</p>
<h2 id="参考" class="headerLink">
    <a href="#%e5%8f%82%e8%80%83" class="header-mark"></a>参考</h2><ul>
<li><a href="https://github.com/Yelp/dumb-init" target="_blank" rel="noopener noreffer">https://github.com/Yelp/dumb-init</a></li>
<li><a href="https://github.com/krallin/tini" target="_blank" rel="noopener noreffer">https://github.com/krallin/tini</a></li>
<li><a href="https://github.com/kubernetes/kubernetes/issues/92214" target="_blank" rel="noopener noreffer">https://github.com/kubernetes/kubernetes/issues/92214</a></li>
</ul>]]></description>
</item><item>
    <title>pod sandbox 创建失败</title>
    <link>https://qingwave.github.io/pod-sandbox-recreated/</link>
    <pubDate>Wed, 18 Mar 2020 18:22:05 &#43;0000</pubDate><author>
        <name>qinng</name>
    </author><guid>https://qingwave.github.io/pod-sandbox-recreated/</guid>
    <description><![CDATA[<h2 id="背景" class="headerLink">
    <a href="#%e8%83%8c%e6%99%af" class="header-mark"></a>背景</h2><p>今天在k8s更新服务时,发现pod启动失败,报错<code>failed to start sandbox container</code>,如下所示:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">Events:
</span></span><span class="line"><span class="cl">  Type     Reason                  Age                     From                                           Message
</span></span><span class="line"><span class="cl">  ----     ------                  ----                    ----                                           -------
</span></span><span class="line"><span class="cl">  Normal   Scheduled               28m                     default-scheduler                              Successfully assigned kube-system/k8s-proxy-7wkt4 to tj1-staging-com-ocean007-201812.kscn
</span></span><span class="line"><span class="cl">  Warning  FailedCreatePodSandBox  28m <span class="o">(</span>x13 over 28m<span class="o">)</span>      kubelet, tj1-staging-com-ocean007-201812.kscn  Failed create pod sandbox: rpc error: <span class="nv">code</span> <span class="o">=</span> Unknown <span class="nv">desc</span> <span class="o">=</span> failed to start sandbox container <span class="k">for</span> pod <span class="s2">&#34;k8s-proxy-7wkt4&#34;</span>: Error response from daemon: OCI runtime create failed: container_linux.go:345: starting container process caused <span class="s2">&#34;process_linux.go:297: getting the final child&#39;s pid from pipe caused \&#34;EOF\&#34;&#34;</span>: unknown
</span></span><span class="line"><span class="cl">  Normal   SandboxChanged          3m19s <span class="o">(</span>x1364 over 28m<span class="o">)</span>  kubelet, tj1-staging-com-ocean007-201812.kscn  Pod sandbox changed, it will be killed and re-created.
</span></span></code></pre></div><h2 id="分析" class="headerLink">
    <a href="#%e5%88%86%e6%9e%90" class="header-mark"></a>分析</h2><p>sandbox 创建失败只是表象,是宿主机其他异常导致的,一般是(cpu,diskio,mem)导致的.</p>
<p>首先,上节点看kubelet,docker有无异常,日志没有明显错误,通过<code>top</code>看到docker cpu占用非常高</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl"><span class="o">[</span>root@tj1-staging-com-ocean007-201812 ~<span class="o">]</span><span class="c1"># top</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">top - 17:55:00 up <span class="m">265</span> days,  3:41,  <span class="m">1</span> user,  load average: 10.71, 11.34, 10.76
</span></span><span class="line"><span class="cl">Tasks: <span class="m">816</span> total,   <span class="m">5</span> running, <span class="m">811</span> sleeping,   <span class="m">0</span> stopped,   <span class="m">0</span> zombie
</span></span><span class="line"><span class="cl">%Cpu<span class="o">(</span>s<span class="o">)</span>: 24.0 us, 34.5 sy,  0.0 ni, 41.4 id,  0.0 wa,  0.0 hi,  0.1 si,  0.0 st
</span></span><span class="line"><span class="cl">KiB Mem : <span class="m">65746380</span> total, <span class="m">20407940</span> free, <span class="m">11007040</span> used, <span class="m">34331400</span> buff/cache
</span></span><span class="line"><span class="cl">KiB Swap:        <span class="m">0</span> total,        <span class="m">0</span> free,        <span class="m">0</span> used. <span class="m">49134416</span> avail Mem 
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    PID USER      PR  NI    VIRT    RES    SHR S  %CPU %MEM     TIME+ COMMAND                                                                                                                                      
</span></span><span class="line"><span class="cl"> <span class="m">115483</span> root      <span class="m">20</span>   <span class="m">0</span> <span class="m">3965212</span> <span class="m">273188</span>  <span class="m">34564</span> S 489.7  0.4 382260:40 dockerd                                                                                                                                      
</span></span><span class="line"><span class="cl"><span class="m">1367523</span> root      <span class="m">20</span>   <span class="m">0</span>   <span class="m">18376</span>   <span class="m">2972</span>   <span class="m">2716</span> R  66.9  0.0  20163:45 bash                                                                                                                                         
</span></span><span class="line"><span class="cl"><span class="m">1367487</span> root      <span class="m">20</span>   <span class="m">0</span>   <span class="m">11856</span>   <span class="m">5616</span>   <span class="m">4512</span> S  54.0  0.0  16748:26 containerd-shim                                                                                                                              
</span></span><span class="line"><span class="cl"><span class="m">3200169</span> root      <span class="m">20</span>   <span class="m">0</span>    <span class="m">1300</span>      <span class="m">4</span>      <span class="m">0</span> R  53.3  0.0  14913:49 sh                                                                                                                                           
</span></span><span class="line"><span class="cl"><span class="m">2429952</span> root      <span class="m">20</span>   <span class="m">0</span>    <span class="m">1300</span>      <span class="m">4</span>      <span class="m">0</span> S  49.3  0.0   9620:56 sh                                                                                                                                           
</span></span><span class="line"><span class="cl"><span class="m">3200130</span> root      <span class="m">20</span>   <span class="m">0</span>    <span class="m">9392</span>   <span class="m">4756</span>   <span class="m">3884</span> S  47.7  0.0  13417:30 containerd-shim                                                                                                                              
</span></span><span class="line"><span class="cl"><span class="m">3718475</span> root      <span class="m">20</span>   <span class="m">0</span>    <span class="m">1300</span>      <span class="m">4</span>      <span class="m">0</span> R  47.4  0.0   8600:20 sh                                                                                                                                           
</span></span><span class="line"><span class="cl"><span class="m">3718440</span> root      <span class="m">20</span>   <span class="m">0</span>   <span class="m">10736</span>   <span class="m">5516</span>   <span class="m">4512</span> S  42.1  0.0   7575:31 containerd-shim                                                                                                                              
</span></span><span class="line"><span class="cl"><span class="m">2429917</span> root      <span class="m">20</span>   <span class="m">0</span>   <span class="m">11856</span>   <span class="m">5556</span>   <span class="m">4512</span> S  40.1  0.0   8313:22 containerd-shim                                                                                                                              
</span></span><span class="line"><span class="cl"><span class="m">3205493</span> root      <span class="m">20</span>   <span class="m">0</span> <span class="m">3775924</span> <span class="m">230996</span>  <span class="m">66704</span> S  18.9  0.4   2559:07 kubelet                                                                                                                                      
</span></span><span class="line"><span class="cl">      <span class="m">1</span> root      <span class="m">20</span>   <span class="m">0</span>  <span class="m">195240</span> <span class="m">157000</span>   <span class="m">3932</span> S   7.9  0.2   1417:46 systemd                                                                                                                                      
</span></span><span class="line"><span class="cl">    <span class="m">804</span> dbus      <span class="m">20</span>   <span class="m">0</span>   <span class="m">30308</span>   <span class="m">6460</span>   <span class="m">2464</span> S   1.7  0.0 462:18.84 dbus-daemon                                                                                                                                  
</span></span><span class="line"><span class="cl"><span class="m">1011737</span> root      <span class="m">20</span>   <span class="m">0</span>  <span class="m">277656</span> <span class="m">122788</span>  <span class="m">18428</span> S   1.3  0.2 768:03.00 cadvisor                                                                                                                                     
</span></span><span class="line"><span class="cl"> <span class="m">115508</span> root      <span class="m">20</span>   <span class="m">0</span> <span class="m">7139200</span>  <span class="m">32896</span>  <span class="m">24288</span> S   1.0  0.1 662:25.27 containerd                                                                                                                                   
</span></span><span class="line"><span class="cl">    <span class="m">806</span> root      <span class="m">20</span>   <span class="m">0</span>   <span class="m">24572</span>   <span class="m">3060</span>   <span class="m">2480</span> S   0.7  0.0 171:22.52 systemd-logind                                                                                                                               
</span></span><span class="line"><span class="cl"> <span class="m">511080</span> root       <span class="m">0</span> -20 <span class="m">2751348</span>  <span class="m">52552</span>  <span class="m">15744</span> S   0.7  0.1 178:27.51 sagent                                                                                                                                       
</span></span><span class="line"><span class="cl"><span class="m">1102507</span> root      <span class="m">20</span>   <span class="m">0</span>   <span class="m">11792</span>   <span class="m">7292</span>   <span class="m">4512</span> S   0.7  0.0  23:36.37 containerd-shim                                                                                                                              
</span></span><span class="line"><span class="cl"><span class="m">1272223</span> root      <span class="m">20</span>   <span class="m">0</span>  <span class="m">164800</span>   <span class="m">5296</span>   <span class="m">3824</span> R   0.7  0.0   0:00.38 top                                                                                                                                          
</span></span><span class="line"><span class="cl"><span class="m">2866292</span> root      <span class="m">20</span>   <span class="m">0</span> <span class="m">5045000</span> 1.983g   <span class="m">3080</span> S   0.7  3.2 230:09.47 redis
</span></span></code></pre></div><p>同时, cpu system异常高.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">%Cpu<span class="o">(</span>s<span class="o">)</span>: 24.0 us, 34.5 sy,  0.0 ni, 41.4 id,  0.0 wa,  0.0 hi,  0.1 si,  0.0 st
</span></span></code></pre></div><p>按照以前的经验,一般是由某些容器引起的,通过<code>top</code>看到个别<code>sh</code>进程占用cpu较高.</p>
<p>通过<code>ps</code>看到进程居然是个死循环</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl"><span class="o">[</span>root@tj1-staging-com-ocean007-201812 ~<span class="o">]</span><span class="c1"># ps -ef |grep 1367523</span>
</span></span><span class="line"><span class="cl">root     <span class="m">1287628</span> <span class="m">1247781</span>  <span class="m">0</span> 17:55 pts/1    00:00:00 grep --color<span class="o">=</span>auto <span class="m">1367523</span>
</span></span><span class="line"><span class="cl">root     <span class="m">1367523</span> <span class="m">1367504</span> <span class="m">72</span> Feb28 ?        14-00:04:17 /bin/bash -c <span class="k">while</span> true<span class="p">;</span> <span class="k">do</span> <span class="nb">echo</span> hello<span class="p">;</span> <span class="k">done</span>
</span></span></code></pre></div><p>通过<code>/proc/pid/cgroup</code>找到对应容器</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl"><span class="c1"># cat /proc/1367523/cgroup</span>
</span></span><span class="line"><span class="cl">11:freezer:/kubepods/besteffort/pod55d3adf2-67f7-11ea-93f2-246e968203b8/29842d5544b701dbb5ff647dba19bb4ebec821edc6ee1ffbd7aeee58fa5038fd
</span></span><span class="line"><span class="cl">10:devices:/kubepods/besteffort/pod55d3adf2-67f7-11ea-93f2-246e968203b8/29842d5544b701dbb5ff647dba19bb4ebec821edc6ee1ffbd7aeee58fa5038fd
</span></span><span class="line"><span class="cl">9:hugetlb:/kubepods/besteffort/pod55d3adf2-67f7-11ea-93f2-246e968203b8/29842d5544b701dbb5ff647dba19bb4ebec821edc6ee1ffbd7aeee58fa5038fd
</span></span><span class="line"><span class="cl">8:blkio:/kubepods/besteffort/pod55d3adf2-67f7-11ea-93f2-246e968203b8/29842d5544b701dbb5ff647dba19bb4ebec821edc6ee1ffbd7aeee58fa5038fd
</span></span><span class="line"><span class="cl">7:memory:/kubepods/besteffort/pod55d3adf2-67f7-11ea-93f2-246e968203b8/29842d5544b701dbb5ff647dba19bb4ebec821edc6ee1ffbd7aeee58fa5038fd
</span></span><span class="line"><span class="cl">6:perf_event:/kubepods/besteffort/pod55d3adf2-67f7-11ea-93f2-246e968203b8/29842d5544b701dbb5ff647dba19bb4ebec821edc6ee1ffbd7aeee58fa5038fd
</span></span><span class="line"><span class="cl">5:cpuset:/kubepods/besteffort/pod55d3adf2-67f7-11ea-93f2-246e968203b8/29842d5544b701dbb5ff647dba19bb4ebec821edc6ee1ffbd7aeee58fa5038fd
</span></span><span class="line"><span class="cl">4:pids:/kubepods/besteffort/pod55d3adf2-67f7-11ea-93f2-246e968203b8/29842d5544b701dbb5ff647dba19bb4ebec821edc6ee1ffbd7aeee58fa5038fd
</span></span><span class="line"><span class="cl">3:net_cls,net_prio:/kubepods/besteffort/pod55d3adf2-67f7-11ea-93f2-246e968203b8/29842d5544b701dbb5ff647dba19bb4ebec821edc6ee1ffbd7aeee58fa5038fd
</span></span><span class="line"><span class="cl">2:cpu,cpuacct:/kubepods/besteffort/pod55d3adf2-67f7-11ea-93f2-246e968203b8/29842d5544b701dbb5ff647dba19bb4ebec821edc6ee1ffbd7aeee58fa5038fd
</span></span><span class="line"><span class="cl">1:name<span class="o">=</span>systemd:/kubepods/besteffort/pod55d3adf2-67f7-11ea-93f2-246e968203b8/29842d5544b701dbb5ff647dba19bb4ebec821edc6ee1ffbd7aeee58fa5038fd
</span></span></code></pre></div><p>找到对应容器</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">docker ps <span class="p">|</span> grep 29842d554
</span></span></code></pre></div><p>清理完相关pod后,系统恢复正常</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">top - 18:25:57 up 265 days,  4:12,  1 user,  load average: 1.05, 1.24, 4.02
</span></span><span class="line"><span class="cl">Tasks: 769 total,   1 running, 768 sleeping,   0 stopped,   0 zombie
</span></span><span class="line"><span class="cl">%Cpu(s):  1.7 us,  0.9 sy,  0.0 ni, 97.3 id,  0.0 wa,  0.0 hi,  0.0 si,  0.0 st
</span></span><span class="line"><span class="cl">KiB Mem : 65746380 total, 22106960 free, 10759860 used, 32879560 buff/cache
</span></span><span class="line"><span class="cl">KiB Swap:        0 total,        0 free,        0 used. 49401576 avail Mem 
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    PID USER      PR  NI    VIRT    RES    SHR S  %CPU %MEM     TIME+ COMMAND                                                                                                                                      
</span></span><span class="line"><span class="cl">3205493 root      20   0 3775924 229844  66704 S   9.9  0.3   2563:18 kubelet                                                                                                                                      
</span></span><span class="line"><span class="cl"> 115483 root      20   0 3965468 249124  34564 S   7.9  0.4 382323:36 dockerd                                                                                                                                      
</span></span><span class="line"><span class="cl">      1 root      20   0  195240 157000   3932 S   6.3  0.2   1419:48 systemd                                                                                                                                      
</span></span><span class="line"><span class="cl">    804 dbus      20   0   30308   6460   2464 S   2.0  0.0 462:51.51 dbus-daemon                                                                                                                                  
</span></span><span class="line"><span class="cl">3085322 root      20   0 12.045g 1.578g  19028 S   1.3  2.5 767:51.19 java                                                                                                                                         
</span></span><span class="line"><span class="cl"> 115508 root      20   0 7139200  32264  24288 S   1.0  0.0 662:42.18 containerd                                                                                                                                   
</span></span><span class="line"><span class="cl"> 511080 root       0 -20 2751348  42116  15744 S   1.0  0.1 178:44.79 sagent                                                                                                                                       
</span></span><span class="line"><span class="cl">1011737 root      20   0  277656 111836  18428 S   1.0  0.2 768:49.01 cadvisor                                                                                                                                     
</span></span><span class="line"><span class="cl">1523167 root      20   0  164800   5436   4012 R   0.7  0.0   0:00.04 top                                                                                                                                          
</span></span><span class="line"><span class="cl">3199459 root      20   0 1554708  43668   9496 S   0.7  0.1  28:50.60 falcon-agent                                                                                                                                 
</span></span><span class="line"><span class="cl">      7 root      20   0       0      0      0 S   0.3  0.0 619:07.64 rcu_sched                                                                                                                                    
</span></span><span class="line"><span class="cl">    806 root      20   0   24572   3060   2480 S   0.3  0.0 171:33.69 systemd-logind                                                                                                                               
</span></span><span class="line"><span class="cl">  11921 root      20   0   94820  20480   5840 S   0.3  0.0   1402:42 consul                                                                                                                                       
</span></span><span class="line"><span class="cl"> 575838 root      20   0  411464  17092   7364 S   0.3  0.0  15:16.25 python                                                                                                                                       
</span></span><span class="line"><span class="cl"> 856593 root      20   0 1562392  37912   9612 S   0.3  0.1  21:34.23 falcon-agent                                                                                                                                 
</span></span><span class="line"><span class="cl"> 931957 33        20   0   90728   3392   1976 S   0.3  0.0   0:51.23 nginx                                                                                                                                        
</span></span><span class="line"><span class="cl">1212186 root      20   0       0      0      0 S   0.3  0.0   0:01.12 kworker/14:1                                                                                                                                 
</span></span><span class="line"><span class="cl">1726228 root      20   0    9392   4496   3808 S   0.3  0.0   0:00.67 containerd-shim                                                                                                                              
</span></span><span class="line"><span class="cl">1887128 root      20   0  273160   7932   3128 S   0.3  0.0  46:05.23 redis-server                                                                                                                                 
</span></span><span class="line"><span class="cl">2788111 root      20   0  273160   6300   3080 S   0.3  0.0  25:18.55 redis-server                                                                                                                                 
</span></span><span class="line"><span class="cl">3199297 root      20   0 1563160  44812   9624 S   0.3  0.1  31:13.73 falcon-agent     
</span></span></code></pre></div><h2 id="总结" class="headerLink">
    <a href="#%e6%80%bb%e7%bb%93" class="header-mark"></a>总结</h2><p>sandox创建失败的原因是各种各样的, 如[memory设置错误触发的异常][1],[dockerd异常][2].</p>
<p>针对此处问题是由于某些测试pod通过<code>while true; do echo hello; done</code>启动,死循环一直<code>echo hello</code>产生大量<code>read()</code>系统调用,所在cpu飙升.多个类似pod导致系统非常繁忙,无法正常处理其他请求.</p>
<p>此类问题不容易在pod创建时直接检测到,只能通过添加物理节点相关报警(dockerd cpu使用率, node cpu.sys使用率)及时发现问题.</p>
<h2 id="引用" class="headerLink">
    <a href="#%e5%bc%95%e7%94%a8" class="header-mark"></a>引用</h2><ul>
<li><a href="https://github.com/kubernetes/kubernetes/issues/56996" target="_blank" rel="noopener noreffer">https://github.com/kubernetes/kubernetes/issues/56996</a></li>
<li><a href="https://plugaru.org/2018/05/21/pod-sandbox-changed-it-will-be-killed-and-re-created/" target="_blank" rel="noopener noreffer">https://plugaru.org/2018/05/21/pod-sandbox-changed-it-will-be-killed-and-re-created/</a></li>
</ul>
]]></description>
</item><item>
    <title>k8s中fd与thread限制(二)</title>
    <link>https://qingwave.github.io/k8s-limit-fd-and-thread2/</link>
    <pubDate>Tue, 16 Jul 2019 19:21:47 &#43;0000</pubDate><author>
        <name>Qingwave</name>
    </author><guid>https://qingwave.github.io/k8s-limit-fd-and-thread2/</guid>
    <description><![CDATA[<h2 id="背景" class="headerLink">
    <a href="#%e8%83%8c%e6%99%af" class="header-mark"></a>背景</h2><p>在上线fd隔离后，多个用户反馈部署有问题，日志显示 <code>su could not open session</code>，dolphin（主进程） 启动用户程序时如果用户部署账号为work，会通过su切换到work下启动用户程序，报错正是这时产生。</p>
<h2 id="探究" class="headerLink">
    <a href="#%e6%8e%a2%e7%a9%b6" class="header-mark"></a>探究</h2><p>通过复现问题，确实存在su切换失败，通过<code>strace su work</code>显示：</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-sh" data-lang="sh"><span class="line"><span class="cl">sh-4.1# strace -o strace.log su work
</span></span><span class="line"><span class="cl">could not open session
</span></span><span class="line"><span class="cl">sh-4.1# vim strace.log
</span></span><span class="line"><span class="cl">execve<span class="o">(</span><span class="s2">&#34;/bin/su&#34;</span>, <span class="o">[</span><span class="s2">&#34;su&#34;</span>, <span class="s2">&#34;work&#34;</span><span class="o">]</span>, <span class="o">[</span>/* <span class="m">18</span> vars */<span class="o">])</span> <span class="o">=</span> <span class="m">0</span>
</span></span><span class="line"><span class="cl">brk<span class="o">(</span>0<span class="o">)</span>
</span></span><span class="line"><span class="cl">/su
</span></span><span class="line"><span class="cl">...
</span></span><span class="line"><span class="cl">stat<span class="o">(</span><span class="s2">&#34;/etc/pam.d&#34;</span>, <span class="o">{</span><span class="nv">st_mode</span><span class="o">=</span>S_IFDIR<span class="p">|</span>0755, <span class="nv">st_size</span><span class="o">=</span>4096, ...<span class="o">})</span> <span class="o">=</span> <span class="m">0</span>
</span></span><span class="line"><span class="cl">open<span class="o">(</span><span class="s2">&#34;/etc/pam.d/su&#34;</span>, O_RDONLY<span class="o">)</span>         <span class="o">=</span> <span class="m">3</span>
</span></span><span class="line"><span class="cl">...
</span></span><span class="line"><span class="cl">open<span class="o">(</span><span class="s2">&#34;/etc/pam.d/system-auth&#34;</span>, O_RDONLY<span class="o">)</span> <span class="o">=</span> <span class="m">4</span>
</span></span><span class="line"><span class="cl">...
</span></span><span class="line"><span class="cl">getrlimit<span class="o">(</span>RLIMIT_CPU, <span class="o">{</span><span class="nv">rlim_cur</span><span class="o">=</span>RLIM_INFINITY, <span class="nv">rlim_max</span><span class="o">=</span>RLIM_INFINITY<span class="o">})</span> <span class="o">=</span> <span class="m">0</span> <span class="c1"># 通过getrlimit获取当前ulimit设置</span>
</span></span><span class="line"><span class="cl">getrlimit<span class="o">(</span>RLIMIT_FSIZE, <span class="o">{</span><span class="nv">rlim_cur</span><span class="o">=</span>RLIM_INFINITY, <span class="nv">rlim_max</span><span class="o">=</span>RLIM_INFINITY<span class="o">})</span> <span class="o">=</span> <span class="m">0</span>
</span></span><span class="line"><span class="cl">getrlimit<span class="o">(</span>RLIMIT_DATA, <span class="o">{</span><span class="nv">rlim_cur</span><span class="o">=</span>RLIM_INFINITY, <span class="nv">rlim_max</span><span class="o">=</span>RLIM_INFINITY<span class="o">})</span> <span class="o">=</span> <span class="m">0</span>
</span></span><span class="line"><span class="cl">getrlimit<span class="o">(</span>RLIMIT_STACK, <span class="o">{</span><span class="nv">rlim_cur</span><span class="o">=</span>8192*1024, <span class="nv">rlim_max</span><span class="o">=</span>RLIM_INFINITY<span class="o">})</span> <span class="o">=</span> <span class="m">0</span>
</span></span><span class="line"><span class="cl">getrlimit<span class="o">(</span>RLIMIT_CORE, <span class="o">{</span><span class="nv">rlim_cur</span><span class="o">=</span>RLIM_INFINITY, <span class="nv">rlim_max</span><span class="o">=</span>RLIM_INFINITY<span class="o">})</span> <span class="o">=</span> <span class="m">0</span>
</span></span><span class="line"><span class="cl">getrlimit<span class="o">(</span>RLIMIT_RSS, <span class="o">{</span><span class="nv">rlim_cur</span><span class="o">=</span>RLIM_INFINITY, <span class="nv">rlim_max</span><span class="o">=</span>RLIM_INFINITY<span class="o">})</span> <span class="o">=</span> <span class="m">0</span>
</span></span><span class="line"><span class="cl">getrlimit<span class="o">(</span>RLIMIT_NPROC, <span class="o">{</span><span class="nv">rlim_cur</span><span class="o">=</span>2048*1024, <span class="nv">rlim_max</span><span class="o">=</span>2048*1024<span class="o">})</span> <span class="o">=</span> <span class="m">0</span>
</span></span><span class="line"><span class="cl">getrlimit<span class="o">(</span>RLIMIT_NOFILE, <span class="o">{</span><span class="nv">rlim_cur</span><span class="o">=</span>10*1024, <span class="nv">rlim_max</span><span class="o">=</span>20*1024<span class="o">})</span> <span class="o">=</span> <span class="m">0</span>
</span></span><span class="line"><span class="cl">getrlimit<span class="o">(</span>RLIMIT_MEMLOCK, <span class="o">{</span><span class="nv">rlim_cur</span><span class="o">=</span>RLIM_INFINITY, <span class="nv">rlim_max</span><span class="o">=</span>RLIM_INFINITY<span class="o">})</span> <span class="o">=</span> <span class="m">0</span>
</span></span><span class="line"><span class="cl">getrlimit<span class="o">(</span>RLIMIT_AS, <span class="o">{</span><span class="nv">rlim_cur</span><span class="o">=</span>RLIM_INFINITY, <span class="nv">rlim_max</span><span class="o">=</span>RLIM_INFINITY<span class="o">})</span> <span class="o">=</span> <span class="m">0</span>
</span></span><span class="line"><span class="cl">getrlimit<span class="o">(</span>RLIMIT_LOCKS, <span class="o">{</span><span class="nv">rlim_cur</span><span class="o">=</span>RLIM_INFINITY, <span class="nv">rlim_max</span><span class="o">=</span>RLIM_INFINITY<span class="o">})</span> <span class="o">=</span> <span class="m">0</span>
</span></span><span class="line"><span class="cl">getrlimit<span class="o">(</span>RLIMIT_SIGPENDING, <span class="o">{</span><span class="nv">rlim_cur</span><span class="o">=</span>256736, <span class="nv">rlim_max</span><span class="o">=</span>256736<span class="o">})</span> <span class="o">=</span> <span class="m">0</span>
</span></span><span class="line"><span class="cl">getrlimit<span class="o">(</span>RLIMIT_MSGQUEUE, <span class="o">{</span><span class="nv">rlim_cur</span><span class="o">=</span>800*1024, <span class="nv">rlim_max</span><span class="o">=</span>800*1024<span class="o">})</span> <span class="o">=</span> <span class="m">0</span>
</span></span><span class="line"><span class="cl">getrlimit<span class="o">(</span>RLIMIT_NICE, <span class="o">{</span><span class="nv">rlim_cur</span><span class="o">=</span>0, <span class="nv">rlim_max</span><span class="o">=</span>0<span class="o">})</span> <span class="o">=</span> <span class="m">0</span>
</span></span><span class="line"><span class="cl">getrlimit<span class="o">(</span>RLIMIT_RTPRIO, <span class="o">{</span><span class="nv">rlim_cur</span><span class="o">=</span>0, <span class="nv">rlim_max</span><span class="o">=</span>0<span class="o">})</span> <span class="o">=</span> <span class="m">0</span>
</span></span><span class="line"><span class="cl">getpriority<span class="o">(</span>PRIO_PROCESS, 0<span class="o">)</span>            <span class="o">=</span> <span class="m">20</span>
</span></span><span class="line"><span class="cl">open<span class="o">(</span><span class="s2">&#34;/etc/security/limits.conf&#34;</span>, O_RDONLY<span class="o">)</span> <span class="o">=</span> <span class="m">3</span> <span class="c1"># 读取limits.conf配置</span>
</span></span><span class="line"><span class="cl">fstat<span class="o">(</span>3, <span class="o">{</span><span class="nv">st_mode</span><span class="o">=</span>S_IFREG<span class="p">|</span>0644, <span class="nv">st_size</span><span class="o">=</span>1973, ...<span class="o">})</span> <span class="o">=</span> <span class="m">0</span>
</span></span><span class="line"><span class="cl">mmap<span class="o">(</span>NULL, 4096, PROT_READ<span class="p">|</span>PROT_WRITE, MAP_PRIVATE<span class="p">|</span>MAP_ANONYMOUS, -1, 0<span class="o">)</span> <span class="o">=</span> 0x7f2b03deb000
</span></span><span class="line"><span class="cl">read<span class="o">(</span>3, <span class="s2">&#34;# /etc/security/limits.conf\n#\n#E&#34;</span>..., 4096<span class="o">)</span> <span class="o">=</span> <span class="m">1973</span>
</span></span><span class="line"><span class="cl">read<span class="o">(</span>3, <span class="s2">&#34;&#34;</span>, 4096<span class="o">)</span>                       <span class="o">=</span> <span class="m">0</span>
</span></span><span class="line"><span class="cl">close<span class="o">(</span>3<span class="o">)</span>                                <span class="o">=</span> <span class="m">0</span>
</span></span><span class="line"><span class="cl">munmap<span class="o">(</span>0x7f2b03deb000, 4096<span class="o">)</span>            <span class="o">=</span> <span class="m">0</span>
</span></span><span class="line"><span class="cl">open<span class="o">(</span><span class="s2">&#34;/etc/security/limits.d&#34;</span>, O_RDONLY<span class="p">|</span>O_NONBLOCK<span class="p">|</span>O_DIRECTORY<span class="p">|</span>O_CLOEXEC<span class="o">)</span> <span class="o">=</span> <span class="m">3</span>
</span></span><span class="line"><span class="cl">fcntl<span class="o">(</span>3, F_GETFD<span class="o">)</span>                       <span class="o">=</span> 0x1 <span class="o">(</span>flags FD_CLOEXEC<span class="o">)</span>
</span></span><span class="line"><span class="cl">getdents<span class="o">(</span>3, /* <span class="m">2</span> entries */, 32768<span class="o">)</span>     <span class="o">=</span> <span class="m">48</span>
</span></span><span class="line"><span class="cl">open<span class="o">(</span><span class="s2">&#34;/usr/lib64/gconv/gconv-modules.cache&#34;</span>, O_RDONLY<span class="o">)</span> <span class="o">=</span> <span class="m">4</span>
</span></span><span class="line"><span class="cl">fstat<span class="o">(</span>4, <span class="o">{</span><span class="nv">st_mode</span><span class="o">=</span>S_IFREG<span class="p">|</span>0644, <span class="nv">st_size</span><span class="o">=</span>26060, ...<span class="o">})</span> <span class="o">=</span> <span class="m">0</span>
</span></span><span class="line"><span class="cl">mmap<span class="o">(</span>NULL, 26060, PROT_READ, MAP_SHARED, 4, 0<span class="o">)</span> <span class="o">=</span> 0x7f2b03de5000
</span></span><span class="line"><span class="cl">close<span class="o">(</span>4<span class="o">)</span>                                <span class="o">=</span> <span class="m">0</span>
</span></span><span class="line"><span class="cl">futex<span class="o">(</span>0x7f2b037b6f60, FUTEX_WAKE_PRIVATE, 2147483647<span class="o">)</span> <span class="o">=</span> <span class="m">0</span> 
</span></span><span class="line"><span class="cl">getdents<span class="o">(</span>3, /* <span class="m">0</span> entries */, 32768<span class="o">)</span>     <span class="o">=</span> <span class="m">0</span>
</span></span><span class="line"><span class="cl">close<span class="o">(</span>3<span class="o">)</span>                                <span class="o">=</span> <span class="m">0</span>
</span></span><span class="line"><span class="cl">setrlimit<span class="o">(</span>RLIMIT_CORE, <span class="o">{</span><span class="nv">rlim_cur</span><span class="o">=</span>RLIM_INFINITY, <span class="nv">rlim_max</span><span class="o">=</span>RLIM_INFINITY<span class="o">})</span> <span class="o">=</span> <span class="m">0</span>  
</span></span><span class="line"><span class="cl">setrlimit<span class="o">(</span>RLIMIT_NOFILE, <span class="o">{</span><span class="nv">rlim_cur</span><span class="o">=</span>150240, <span class="nv">rlim_max</span><span class="o">=</span>300240<span class="o">})</span> <span class="o">=</span> -1 EPERM <span class="o">(</span>Operation not permitted<span class="o">)</span> <span class="c1"># 设置nofile失败，返回权限不足，经查证setrlimit需要CAP_SYS_RESOURCE</span>
</span></span><span class="line"><span class="cl">...
</span></span></code></pre></div><p>整理下执行su的流程</p>
<ol>
<li>
<p>进行pam认证，su配置文件在/etc/pam.d/su，更多pam信息可参考pam.d</p>
</li>
<li>
<p>根据文件内容逐行认证，下面是线上centos6基础镜像的配置</p>
</li>
</ol>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-sh" data-lang="sh"><span class="line"><span class="cl"><span class="c1">#%PAM-1.0</span>
</span></span><span class="line"><span class="cl">auth sufficient pam_rootok.so
</span></span><span class="line"><span class="cl"><span class="c1"># Uncomment the following line to implicitly trust users in the &#34;wheel&#34; group.</span>
</span></span><span class="line"><span class="cl"><span class="c1">#auth sufficient pam_wheel.so trust use_uid</span>
</span></span><span class="line"><span class="cl"><span class="c1"># Uncomment the following line to require a user to be in the &#34;wheel&#34; group.</span>
</span></span><span class="line"><span class="cl"><span class="c1">#auth required pam_wheel.so use_uid</span>
</span></span><span class="line"><span class="cl">auth include system-auth
</span></span><span class="line"><span class="cl">account sufficient pam_succeed_if.so <span class="nv">uid</span> <span class="o">=</span> <span class="m">0</span> use_uid quiet
</span></span><span class="line"><span class="cl">account include system-auth
</span></span><span class="line"><span class="cl">password include system-auth
</span></span><span class="line"><span class="cl">session include system-auth <span class="c1">#认证失败出现在这步</span>
</span></span><span class="line"><span class="cl">session optional pam_xauth.so
</span></span></code></pre></div><ol start="3">
<li>system-auth 真实内容存放在 system-auth-ac，内容为</li>
</ol>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-sh" data-lang="sh"><span class="line"><span class="cl"><span class="c1"># User changes will be destroyed the next time authconfig is run.</span>
</span></span><span class="line"><span class="cl">auth required pam_env.so
</span></span><span class="line"><span class="cl">auth sufficient pam_fprintd.so
</span></span><span class="line"><span class="cl">auth sufficient pam_unix.so nullok try_first_pass
</span></span><span class="line"><span class="cl">auth requisite pam_succeed_if.so uid &gt;<span class="o">=</span> <span class="m">500</span> quiet
</span></span><span class="line"><span class="cl">auth required pam_deny.so
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">account required pam_unix.so
</span></span><span class="line"><span class="cl">account sufficient pam_localuser.so
</span></span><span class="line"><span class="cl">account sufficient pam_succeed_if.so uid &lt; <span class="m">500</span> quiet
</span></span><span class="line"><span class="cl">account required pam_permit.so
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">password requisite pam_cracklib.so try_first_pass <span class="nv">retry</span><span class="o">=</span><span class="m">3</span> <span class="nv">type</span><span class="o">=</span>
</span></span><span class="line"><span class="cl">password sufficient pam_unix.so md5 shadow nullok try_first_pass use_authtok
</span></span><span class="line"><span class="cl">password required pam_deny.so
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">session optional pam_keyinit.so revoke
</span></span><span class="line"><span class="cl">session required pam_limits.so <span class="c1"># limit 认证</span>
</span></span><span class="line"><span class="cl">session <span class="o">[</span><span class="nv">success</span><span class="o">=</span><span class="m">1</span> <span class="nv">default</span><span class="o">=</span>ignore<span class="o">]</span> pam_succeed_if.so service in crond quiet use_uid
</span></span><span class="line"><span class="cl">session required pam_unix.so 
</span></span></code></pre></div><ol start="4">
<li>
<p>system-auth调用pam_limit.so认证，并且类型为required，及若认证失败则继续执行最后返回失败信息</p>
</li>
<li>
<p>pam_limit会调用getrlimit获取当前ulimit信息，通过读取/etc/security/limits.conf，调用setrlimit设置ulimit，并且setrlimit有一定限制</p>
</li>
</ol>
<ul>
<li>任何进程可以将软限制改为小于或等于硬限制</li>
<li>任何进程都可以将硬限制降低，但普通用户降低了就无法提高，该值必须等于或大于软限制</li>
<li>只有超级用户（拥有CAP_SYS_RESOURCE权限）可以提高硬限制</li>
</ul>
<p>由于显示docker设置nofile最大hard限制为20480， 而/etc/security/limits.cof文件中为300240，在docker中root用户缺少<code>CAP_SYS_RESOURCE</code>，所以出现上述问题。</p>
<h2 id="解决办法" class="headerLink">
    <a href="#%e8%a7%a3%e5%86%b3%e5%8a%9e%e6%b3%95" class="header-mark"></a>解决办法</h2><p>由于limits.conf，以及pam.so等配置文件是镜像中的配置，解决冲突必须修改对应配置,有两种方式</p>
<ul>
<li>通过dolphin将对应limits.conf以及limits.d目录下有关nofile的配置删除</li>
<li>基础镜像修改limits.conf配置</li>
</ul>]]></description>
</item><item>
    <title>k8s中fd与thread限制(一)</title>
    <link>https://qingwave.github.io/k8s-limit-fd-and-thread1/</link>
    <pubDate>Tue, 16 Jul 2019 18:44:47 &#43;0000</pubDate><author>
        <name>Qingwave</name>
    </author><guid>https://qingwave.github.io/k8s-limit-fd-and-thread1/</guid>
    <description><![CDATA[<h2 id="背景" class="headerLink">
    <a href="#%e8%83%8c%e6%99%af" class="header-mark"></a>背景</h2><p>linux中为了防止进程恶意使用资源，系统使用ulimit来限制进程的资源使用情况（包括文件描述符，线程数，内存大小等）。同样地在容器化场景中，需要限制其系统资源的使用量。</p>
<h2 id="限制方法" class="headerLink">
    <a href="#%e9%99%90%e5%88%b6%e6%96%b9%e6%b3%95" class="header-mark"></a>限制方法</h2><ul>
<li><strong>ulimit</strong>: docker 默认支持ulimit设置，可以在dockerd中配置 default-ulimits 可为宿主机所有容器配置默认的ulimit，docker启动时可添加 &ndash;ulimit 为每个容器配置ulimit会覆盖默认的设置；目前k8s暂不支持ulimit</li>
<li><strong>cgroup</strong>: docker 默认支持cgroup中内存、cpu、pid等的限制，对于线程限制可通过 &ndash;pids-limit 可限制每个容器的pid总数，dockerd暂无默认的pid limit设置；k8s 限制线程数，可通过在kubelet中开启SupportPodPidsLimit特性，设置pod级别pid limit</li>
<li><strong>/etc/securiy/limits.conf,systcl.confg</strong>: 通过ulimit命令设置只对当前登录用户有效，永久设置可通过limits.conf配置文件实现，以及系统级别限制可通过systcl.confg配置文件</li>
</ul>
<h2 id="实验对比" class="headerLink">
    <a href="#%e5%ae%9e%e9%aa%8c%e5%af%b9%e6%af%94" class="header-mark"></a>实验对比</h2><h3 id="环境" class="headerLink">
    <a href="#%e7%8e%af%e5%a2%83" class="header-mark"></a>环境</h3><p><strong>本地环境</strong>：
os: Ubuntu 16.04.6 LTS 4.4.0-154-generic
docker: 18.09.7
base-image: alpine:v3.9</p>
<p><strong>k8s环境</strong>：
kubelet: v1.10.11.1
docker: 18.09.6</p>
<h3 id="ulimit" class="headerLink">
    <a href="#ulimit" class="header-mark"></a>ulimit</h3><p>用户级别资源限制，分为soft限制与hard限制</p>
<ul>
<li>soft ： 用户可修改，但不能超过硬限制</li>
<li>hard：只有root用户可修改</li>
</ul>
<p><strong>修改方式</strong>： ulimit命令，临时修改；/etc/security/limits.conf，永久修改</p>
<p><strong>工作原理</strong>： 根据 PAM （ Pluggable Authentication Modules 简称 PAM）机制，应用程序启动时，按 /etc/pam.d 配置加载 pam_xxxx.so 模块。 /etc/pam.d 下包含了 login 、sshd 、su 、sudo 等程序的 PAM 配置文件， 因此用户重新登录时，将调用 pam_limits.so 加载 limits.conf 配置文件</p>
<h4 id="文件描述符限制" class="headerLink">
    <a href="#%e6%96%87%e4%bb%b6%e6%8f%8f%e8%bf%b0%e7%ac%a6%e9%99%90%e5%88%b6" class="header-mark"></a>文件描述符限制</h4><div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">RLIMIT_NOFILE
</span></span><span class="line"><span class="cl">              This specifies a value one greater than the maximum file
</span></span><span class="line"><span class="cl">              descriptor number that can be opened by this process.
</span></span><span class="line"><span class="cl">              Attempts (open(2), pipe(2), dup(2), etc.)  to exceed this
</span></span><span class="line"><span class="cl">              limit yield the error EMFILE.  (Historically, this limit was
</span></span><span class="line"><span class="cl">              named RLIMIT_OFILE on BSD.)
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">              Since Linux 4.5, this limit also defines the maximum number of
</span></span><span class="line"><span class="cl">              file descriptors that an unprivileged process (one without the
</span></span><span class="line"><span class="cl">              CAP_SYS_RESOURCE capability) may have &#34;in flight&#34; to other
</span></span><span class="line"><span class="cl">              processes, by being passed across UNIX domain sockets.  This
</span></span><span class="line"><span class="cl">              limit applies to the sendmsg(2) system call.  For further
</span></span><span class="line"><span class="cl">              details, see unix(7).
</span></span></code></pre></div><p>根据定义，nofile 限制进程所能最多打开的文件数量，作用范围进程。</p>
<ol>
<li>设置 ulimit nofile限制soft 100/hard 200，默认启动为root用户</li>
</ol>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">$ docker run -d --ulimit nofile=100:200  cr.d.xiaomi.net/containercloud/alpine:webtool top
</span></span></code></pre></div><ol start="2">
<li>进入容器查看， fd soft限制为100个</li>
</ol>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">/ # ulimit -a
</span></span><span class="line"><span class="cl">-f: file size (blocks)             unlimited
</span></span><span class="line"><span class="cl">-t: cpu time (seconds)             unlimited
</span></span><span class="line"><span class="cl">-d: data seg size (kb)             unlimited
</span></span><span class="line"><span class="cl">-s: stack size (kb)                8192
</span></span><span class="line"><span class="cl">-c: core file size (blocks)        unlimited
</span></span><span class="line"><span class="cl">-m: resident set size (kb)         unlimited
</span></span><span class="line"><span class="cl">-l: locked memory (kb)             64
</span></span><span class="line"><span class="cl">-p: processes                      unlimited
</span></span><span class="line"><span class="cl">-n: file descriptors               100
</span></span><span class="line"><span class="cl">-v: address space (kb)             unlimited
</span></span><span class="line"><span class="cl">-w: locks                          unlimited
</span></span><span class="line"><span class="cl">-e: scheduling priority            0
</span></span><span class="line"><span class="cl">-r: real-time priority             0
</span></span></code></pre></div><ol start="3">
<li>使用ab测试，并发90个http请求，创建90个socket，正常运行</li>
</ol>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">/ <span class="c1"># ab -n 1000000 -c 90 http://61.135.169.125:80/ &amp;</span>
</span></span><span class="line"><span class="cl">/ <span class="c1"># lsof | wc -l </span>
</span></span><span class="line"><span class="cl"><span class="m">108</span>
</span></span><span class="line"><span class="cl">/ <span class="c1"># lsof | grep -c ab</span>
</span></span><span class="line"><span class="cl"><span class="m">94</span>
</span></span></code></pre></div><ol start="4">
<li>并发100个http请求，受到ulimit限制
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">/ <span class="c1">#  ab -n 1000000 -c 100 http://61.135.169.125:80/</span>
</span></span><span class="line"><span class="cl">This is ApacheBench, Version 2.3 &lt;<span class="nv">$Revision</span>: <span class="m">1843412</span> $&gt;
</span></span><span class="line"><span class="cl">Copyright <span class="m">1996</span> Adam Twiss, Zeus Technology Ltd, http://www.zeustech.net/
</span></span><span class="line"><span class="cl">Licensed to The Apache Software Foundation, http://www.apache.org/
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">Benchmarking 61.135.169.125 <span class="o">(</span>be patient<span class="o">)</span>
</span></span><span class="line"><span class="cl">socket: No file descriptors available <span class="o">(</span>24<span class="o">)</span>
</span></span></code></pre></div></li>
</ol>
<h4 id="线程限制" class="headerLink">
    <a href="#%e7%ba%bf%e7%a8%8b%e9%99%90%e5%88%b6" class="header-mark"></a>线程限制</h4><div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">RLIMIT_NPROC
</span></span><span class="line"><span class="cl">              This is a limit on the number of extant process (or, more pre‐
</span></span><span class="line"><span class="cl">              cisely on Linux, threads) for the real user ID of the calling
</span></span><span class="line"><span class="cl">              process.  So long as the current number of processes belonging
</span></span><span class="line"><span class="cl">              to this process&#39;s real user ID is greater than or equal to
</span></span><span class="line"><span class="cl">              this limit, fork(2) fails with the error EAGAIN.
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">              The RLIMIT_NPROC limit is not enforced for processes that have
</span></span><span class="line"><span class="cl">              either the CAP_SYS_ADMIN or the CAP_SYS_RESOURCE capability.
</span></span></code></pre></div><p>由定义可知，nproc进程限制的范围是对于每个uid，并且对于root用户无效。</p>
<h5 id="容器uid" class="headerLink">
    <a href="#%e5%ae%b9%e5%99%a8uid" class="header-mark"></a>容器uid</h5><p>同一主机上运行的所有容器共享同一个内核(主机的内核)，docker通过namspace对pid/utc/network等进行了隔离，虽然docker中已经实现了user namespace，但由于各种原因，默认没有开启，见<a href="https://docs.docker.com/engine/security/userns-remap/" target="_blank" rel="noopener noreffer">docker user namespace</a></p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">$ docker run -d  cr.d.xiaomi.net/containercloud/alpine:webtool top
</span></span></code></pre></div><p>宿主机中查看top进程，显示root用户</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">$ ps -ef |grep top
</span></span><span class="line"><span class="cl">root      4096  4080  0 15:01 ?        00:00:01 top
</span></span></code></pre></div><p>容器中查看id，uid为0对应宿主机的root用户,虽然同为root用户，但Linux Capabilities不同，实际权限与宿主机root要少很多</p>
<p>在容器中切换用户到operator(uid为11)，执行sleep命令，主机中查看对应进程用户为app，对应uid同样为11</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">/ # id
</span></span><span class="line"><span class="cl">uid=0(root) gid=0(root) groups=0(root),1(bin),2(daemon),3(sys),4(adm),6(disk),10(wheel),11(floppy),20(dialout),26(tape),27(video)
</span></span><span class="line"><span class="cl">/ # su operator
</span></span><span class="line"><span class="cl">/ $ id
</span></span><span class="line"><span class="cl">uid=11(operator) gid=0(root) groups=0(root)
</span></span><span class="line"><span class="cl">/ $ sleep 100
</span></span><span class="line"><span class="cl">$ ps -ef |grep &#39;sleep 100&#39;
</span></span><span class="line"><span class="cl">app      19302 19297  0 16:39 pts/0    00:00:00 sleep 100
</span></span><span class="line"><span class="cl">$ cat /etc/passwd | grep app
</span></span><span class="line"><span class="cl">app❌11:0::/home/app:
</span></span></code></pre></div><h5 id="验证不同用户下ulimit的限制" class="headerLink">
    <a href="#%e9%aa%8c%e8%af%81%e4%b8%8d%e5%90%8c%e7%94%a8%e6%88%b7%e4%b8%8bulimit%e7%9a%84%e9%99%90%e5%88%b6" class="header-mark"></a>验证不同用户下ulimit的限制</h5><p>设置 ulimit nproc限制soft 10/hard 20，默认启动为root用户</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">$ docker run -d --ulimit nproc=10:20  cr.d.xiaomi.net/containercloud/alpine:webtool top
</span></span></code></pre></div><p>进入容器查看， fd soft限制为100个</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">/ # ulimit -a
</span></span><span class="line"><span class="cl">-f: file size (blocks)             unlimited
</span></span><span class="line"><span class="cl">-t: cpu time (seconds)             unlimited
</span></span><span class="line"><span class="cl">-d: data seg size (kb)             unlimited
</span></span><span class="line"><span class="cl">-s: stack size (kb)                8192
</span></span><span class="line"><span class="cl">-c: core file size (blocks)        unlimited
</span></span><span class="line"><span class="cl">-m: resident set size (kb)         unlimited
</span></span><span class="line"><span class="cl">-l: locked memory (kb)             64
</span></span><span class="line"><span class="cl">-p: processes                      10
</span></span><span class="line"><span class="cl">-n: file descriptors               1048576
</span></span><span class="line"><span class="cl">-v: address space (kb)             unlimited
</span></span><span class="line"><span class="cl">-w: locks                          unlimited
</span></span><span class="line"><span class="cl">-e: scheduling priority            0
</span></span><span class="line"><span class="cl">-r: real-time priority             0
</span></span></code></pre></div><p>启动30个进程</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">/ # for i in `seq 30`;do sleep 100 &amp;; done
</span></span><span class="line"><span class="cl">/ # ps | wc -l 
</span></span><span class="line"><span class="cl">36
</span></span></code></pre></div><p>切换到operator用户</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">/ # su operator
</span></span><span class="line"><span class="cl"># 启动多个进程，到第11个进程无法进行fork
</span></span><span class="line"><span class="cl">/ $ for i in `seq 8`; do
</span></span><span class="line"><span class="cl">&gt; sleep 100 &amp;
</span></span><span class="line"><span class="cl">&gt; done
</span></span><span class="line"><span class="cl">/ $ sleep 100 &amp;
</span></span><span class="line"><span class="cl">/ $ sleep 100 &amp;
</span></span><span class="line"><span class="cl">sh: can&#39;t fork: Resource temporarily unavailable
</span></span></code></pre></div><p>root下查看</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">/ # ps -ef | grep operator
</span></span><span class="line"><span class="cl">   79 operator  0:00 sh
</span></span><span class="line"><span class="cl">   99 operator  0:00 sleep 100
</span></span><span class="line"><span class="cl">  100 operator  0:00 sleep 100
</span></span><span class="line"><span class="cl">  101 operator  0:00 sleep 100
</span></span><span class="line"><span class="cl">  102 operator  0:00 sleep 100
</span></span><span class="line"><span class="cl">  103 operator  0:00 sleep 100
</span></span><span class="line"><span class="cl">  104 operator  0:00 sleep 100
</span></span><span class="line"><span class="cl">  105 operator  0:00 sleep 100
</span></span><span class="line"><span class="cl">  106 operator  0:00 sleep 100
</span></span><span class="line"><span class="cl">  107 operator  0:00 sleep 100
</span></span><span class="line"><span class="cl">  109 root      0:00 grep operator
</span></span><span class="line"><span class="cl">/ # ps -ef | grep operator| wc -l
</span></span><span class="line"><span class="cl">10
</span></span></code></pre></div><h5 id="验证ulimit在不同容器相同uid下的限制" class="headerLink">
    <a href="#%e9%aa%8c%e8%af%81ulimit%e5%9c%a8%e4%b8%8d%e5%90%8c%e5%ae%b9%e5%99%a8%e7%9b%b8%e5%90%8cuid%e4%b8%8b%e7%9a%84%e9%99%90%e5%88%b6" class="header-mark"></a>验证ulimit在不同容器相同uid下的限制</h5><p>设置 ulimit nproc限制soft 3/hard 3，默认启动为operator用户,起4个容器，第四个启动失败</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">$ docker run -d --ulimit nproc=3:3 --name nproc1 -u operator  cr.d.xiaomi.net/containercloud/alpine:webtool top
</span></span><span class="line"><span class="cl">eeb1551bf757ad4f112c61cc48d7cbe959185f65109e4b44f28085f246043e65
</span></span><span class="line"><span class="cl">$ docker run -d --ulimit nproc=3:3 --name nproc2 -u operator  cr.d.xiaomi.net/containercloud/alpine:webtool top
</span></span><span class="line"><span class="cl">42ff29844565a9cb3af2c8dd560308b1f31306041d3dbd929011d65f1848a262
</span></span><span class="line"><span class="cl">$ docker run -d --ulimit nproc=3:3 --name nproc3 -u operator  cr.d.xiaomi.net/containercloud/alpine:webtool top
</span></span><span class="line"><span class="cl">b7c9b469e73f969d922841dd77265467959eda28ed06301af8bf83bcf18e8c23
</span></span><span class="line"><span class="cl">$ docker run -d --ulimit nproc=3:3 --name nproc4 -u operator  cr.d.xiaomi.net/containercloud/alpine:webtool top
</span></span><span class="line"><span class="cl">b49d8bb58757c88f69903059af2ee7e2a6cc2fa5774bc531941194c52edfd763
</span></span><span class="line"><span class="cl">$
</span></span><span class="line"><span class="cl">$ docker ps -a |grep nproc
</span></span><span class="line"><span class="cl">b49d8bb58757        cr.d.xiaomi.net/containercloud/alpine:webtool      &#34;top&#34;                    16 seconds ago      Exited (1) 15 seconds ago                               nproc4
</span></span><span class="line"><span class="cl">b7c9b469e73f        cr.d.xiaomi.net/containercloud/alpine:webtool      &#34;top&#34;                    23 seconds ago      Up 22 seconds                                           nproc3
</span></span><span class="line"><span class="cl">42ff29844565        cr.d.xiaomi.net/containercloud/alpine:webtool      &#34;top&#34;                    31 seconds ago      Up 29 seconds                                           nproc2
</span></span><span class="line"><span class="cl">eeb1551bf757        cr.d.xiaomi.net/containercloud/alpine:webtool      &#34;top&#34;                    38 seconds ago      Up 36 seconds                                           nproc1
</span></span></code></pre></div><h5 id="总结" class="headerLink">
    <a href="#%e6%80%bb%e7%bb%93" class="header-mark"></a>总结</h5><ul>
<li>ulimit限制fd总数，限制级别进程，可对所有用户生效</li>
<li>ulimit限制线程总数，限制级别用户（uid)，限制同一个uid下所有线程/进程数，对于root账号无效</li>
<li>对于目前线上情况，有较小的概率因ulimit限制导致fork失败，如同一个宿主机中有多个work容器且基础镜像相同（即uid相同），若一个容器线程泄露，由于ulimit限制会影响其他容器正常运行</li>
</ul>
<h3 id="cgroup" class="headerLink">
    <a href="#cgroup" class="header-mark"></a>cgroup</h3><p>cgroup中对pid进行了隔离，通过更改docker/kubelet配置，可以限制pid总数，从而达到限制线程总数的目的。线程数限制与系统中多处配置有关，取最小值，参考<a href="https://stackoverflow.com/questions/34452302/how-to-increase-maximum-number-of-jvm-threads-linux-64bit" target="_blank" rel="noopener noreffer">stackoverflow上线程数的设置</a></p>
<ul>
<li>docker，容器启动时设置 &ndash;pids-limit 参数，限制容器级别pid总数</li>
<li>kubelet，开启SupportPodPidsLimit特性，设置&ndash;pod-max-pids参数，限制node每个pod的pid总数</li>
</ul>
<p>以kubelet为例，开启SupportPodPidsLimit，<code>--feature-gates=SupportPodPidsLimit=true</code></p>
<ol>
<li>配置kubelet，每个pod允许最大pid数目为150</li>
</ol>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl"><span class="o">[</span>root@node01 ~<span class="o">]</span><span class="c1"># ps -ef |grep kubelet</span>
</span></span><span class="line"><span class="cl">root     <span class="m">18735</span>     <span class="m">1</span> <span class="m">14</span> 11:19 ?        00:53:28 ./kubelet --v<span class="o">=</span><span class="m">1</span> --address<span class="o">=</span>0.0.0.0 --feature-gates<span class="o">=</span><span class="nv">SupportPodPidsLimit</span><span class="o">=</span><span class="nb">true</span> --pod-max-pids<span class="o">=</span><span class="m">150</span> --allow-privileged<span class="o">=</span><span class="nb">true</span> --pod-infra-container-image<span class="o">=</span>cr.d.xiaomi.net/kubernetes/pause-amd64:3.1 --root-dir<span class="o">=</span>/home/kubelet --node-status-update-frequency<span class="o">=</span>5s --kubeconfig<span class="o">=</span>/home/xbox/kubelet/conf/kubelet-kubeconfig --fail-swap-on<span class="o">=</span><span class="nb">false</span> --max-pods<span class="o">=</span><span class="m">254</span> --runtime-cgroups<span class="o">=</span>/systemd/system.slice/frigga.service --kubelet-cgroups<span class="o">=</span>/systemd/system.slice/frigga.service --make-iptables-util-chains<span class="o">=</span><span class="nb">false</span>
</span></span></code></pre></div><ol start="2">
<li>在pod中起测试线程，root下起100个线程</li>
</ol>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">/ # for i in `seq 100`; do
</span></span><span class="line"><span class="cl">&gt; sleep 1000 &amp;
</span></span><span class="line"><span class="cl">&gt; done
</span></span><span class="line"><span class="cl">/ # ps | wc -l
</span></span><span class="line"><span class="cl">106
</span></span></code></pre></div><ol start="3">
<li>operator 下，创建线程受到限制，系统最多只能创建150个</li>
</ol>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">/ # su operator
</span></span><span class="line"><span class="cl">/ $ 
</span></span><span class="line"><span class="cl">/ $ for i in `seq 100`; do
</span></span><span class="line"><span class="cl">&gt; sleep 1000 &amp;
</span></span><span class="line"><span class="cl">&gt; done
</span></span><span class="line"><span class="cl">sh: can&#39;t fork: Resource temporarily unavailable
</span></span><span class="line"><span class="cl">/ $ ps | wc -l
</span></span><span class="line"><span class="cl">150
</span></span></code></pre></div><ol start="4">
<li>在cgroup中查看，pids达到最大限制</li>
</ol>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">[root@node01 ~]# cat /sys/fs/cgroup/pids/kubepods/besteffort/pod8b61d4de-a7ad-11e9-b5b9-246e96ad0900/pids.current 
</span></span><span class="line"><span class="cl">150
</span></span><span class="line"><span class="cl">[root@node01 ~]# cat /sys/fs/cgroup/pids/kubepods/besteffort/pod8b61d4de-a7ad-11e9-b5b9-246e96ad0900/pids.max 
</span></span><span class="line"><span class="cl">150
</span></span></code></pre></div><ol start="5">
<li>总结
cgroup对于pid的限制能够达到限制线程数目的，目前docker只支持对每个容器的限制，不支持全局配置；kubelet只支持对于node所有pod的全局配置，不支持具体每个pod的配置</li>
</ol>
<h3 id="limitsconfsysctlconf" class="headerLink">
    <a href="#limitsconfsysctlconf" class="header-mark"></a>limits.conf/sysctl.conf</h3><p>limits.conf是ulimit的具体配置，目录项/etc/security/limit.d/中的配置会覆盖limits.conf。</p>
<p>sysctl.conf为机器级别的资源限制，root用户可修改，目录项/etc/security/sysctl.d/中的配置会覆盖sysctl.conf，在/etc/sysctl.conf中添加对应配置（fd: fs.file-max = {}; pid: kernel.pid_max = {}）</p>
<ol>
<li>
<p>测试容器中修改sysctl.conf文件</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">$ docker run -d --ulimit nofile=100:200 cr.d.xiaomi.net/containercloud/alpine:webtool top
</span></span><span class="line"><span class="cl">cb1250c8fd217258da51c6818fa2ce2e2f6e35bf1d52648f1f432e6ce579cf0d
</span></span><span class="line"><span class="cl">$ docker exec -it cb1250c sh
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">/ # ulimit -a
</span></span><span class="line"><span class="cl">-f: file size (blocks)             unlimited
</span></span><span class="line"><span class="cl">-t: cpu time (seconds)             unlimited
</span></span><span class="line"><span class="cl">-d: data seg size (kb)             unlimited
</span></span><span class="line"><span class="cl">-s: stack size (kb)                8192
</span></span><span class="line"><span class="cl">-c: core file size (blocks)        unlimited
</span></span><span class="line"><span class="cl">-m: resident set size (kb)         unlimited
</span></span><span class="line"><span class="cl">-l: locked memory (kb)             64
</span></span><span class="line"><span class="cl">-p: processes                      unlimited
</span></span><span class="line"><span class="cl">-n: file descriptors               100
</span></span><span class="line"><span class="cl">-v: address space (kb)             unlimited
</span></span><span class="line"><span class="cl">-w: locks                          unlimited
</span></span><span class="line"><span class="cl">-e: scheduling priority            0
</span></span><span class="line"><span class="cl">-r: real-time priority             0
</span></span><span class="line"><span class="cl">/ # 
</span></span><span class="line"><span class="cl">/ # echo 10 &gt; /proc/sys/kernel/pid_max
</span></span><span class="line"><span class="cl">sh: can&#39;t create /proc/sys/kernel/pid_max: Read-only file system
</span></span><span class="line"><span class="cl">/ # echo 10 &gt; /proc/sys/kernel/pid_max
</span></span><span class="line"><span class="cl">sh: can&#39;t create /proc/sys/kernel/pid_max: Read-only file system
</span></span><span class="line"><span class="cl">/ # echo &#34;fs.file-max=5&#34; &gt;&gt; /etc/sysctl.conf
</span></span><span class="line"><span class="cl">/ # sysctl -p
</span></span><span class="line"><span class="cl">sysctl: error setting key &#39;fs.file-max&#39;: Read-only file system
</span></span></code></pre></div></li>
<li>
<p>以priviledged模式测试，谨慎测试</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">$ cat /proc/sys/kernel/pid_max
</span></span><span class="line"><span class="cl">32768
</span></span><span class="line"><span class="cl">$ docker run -d -- --ulimit nofile=100:200 cr.d.xiaomi.net/containercloud/alpine:webtool top
</span></span><span class="line"><span class="cl">$ docker exec -it pedantic_vaughan sh
</span></span><span class="line"><span class="cl">/ # cat /proc/sys/kernel/pid_max
</span></span><span class="line"><span class="cl">32768
</span></span><span class="line"><span class="cl">/ # echo 50000 &gt; /proc/sys/kernel/pid_max
</span></span><span class="line"><span class="cl">/ # cat /proc/sys/kernel/pid_max
</span></span><span class="line"><span class="cl">50000
</span></span><span class="line"><span class="cl">/ # exit
</span></span><span class="line"><span class="cl">$ cat /proc/sys/kernel/pid_max
</span></span><span class="line"><span class="cl">50000 # 宿主机的文件也变成50000
</span></span></code></pre></div></li>
<li>
<p>总结
由于docker隔离的不彻底，在docker中修改sysctl会覆盖主机中的配置，不能用来实现容器级别资源限制
limits.conf可以在容器中设置，效果同ulimit</p>
</li>
</ol>
<h2 id="结论" class="headerLink">
    <a href="#%e7%bb%93%e8%ae%ba" class="header-mark"></a>结论</h2><p>

</p>
<p>推荐方案如下：</p>
<ul>
<li>fd限制： 修改dockerd配置<code>default-ulimits</code>，限制进程级别fd</li>
<li>thread限制：修改kubelet配置<code>--feature-gates=SupportPodPidsLimit=true --pod-max-pids={}</code>，cgroup级别限制pid，从而限制线程数</li>
<li>其他注意事项，调整节点pid.max参数；放开或者调大镜像中ulimit对非root账户nproc限制</li>
</ul>
<h2 id="引用" class="headerLink">
    <a href="#%e5%bc%95%e7%94%a8" class="header-mark"></a>引用</h2><ul>
<li><a href="https://docs.docker.com/engine/reference/commandline/run/#set-ulimits-in-container---ulimit" target="_blank" rel="noopener noreffer">https://docs.docker.com/engine/reference/commandline/run/#set-ulimits-in-container---ulimit</a></li>
<li><a href="http://man7.org/linux/man-pages/man2/getrlimit.2.html" target="_blank" rel="noopener noreffer">http://man7.org/linux/man-pages/man2/getrlimit.2.html</a></li>
<li><a href="https://feichashao.com/ulimit_demo/" target="_blank" rel="noopener noreffer">https://feichashao.com/ulimit_demo/</a></li>
<li><a href="https://medium.com/@mccode/understanding-how-uid-and-gid-work-in-docker-containers-c37a01d01cf" target="_blank" rel="noopener noreffer">https://medium.com/@mccode/understanding-how-uid-and-gid-work-in-docker-containers-c37a01d01cf</a></li>
<li><a href="https://docs.docker.com/engine/security/userns-remap/" target="_blank" rel="noopener noreffer">https://docs.docker.com/engine/security/userns-remap/</a></li>
</ul>]]></description>
</item><item>
    <title>容器内存分析</title>
    <link>https://qingwave.github.io/container-memory/</link>
    <pubDate>Wed, 29 May 2019 14:37:21 &#43;0000</pubDate><author>
        <name>Qingwave</name>
    </author><guid>https://qingwave.github.io/container-memory/</guid>
    <description><![CDATA[<h2 id="背景" class="headerLink">
    <a href="#%e8%83%8c%e6%99%af" class="header-mark"></a>背景</h2><p>在容器化环境中，平台需要提供准确的业务监控指标，已方便业务查看。那么如何准确计算容器或Pod的内存使用率，k8s/docker又是如何计算，本文通过实验与源码阅读相结合来分析容器的内存实际使用量。</p>
<h2 id="预备知识" class="headerLink">
    <a href="#%e9%a2%84%e5%a4%87%e7%9f%a5%e8%af%86" class="header-mark"></a>预备知识</h2><p>不管docker还是k8s(通过cadvisor)最终都通过cgroup的memory group来得到内存的原始文件，memory相关的主要文件如下:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">cgroup.event_control       #用于eventfd的接口
</span></span><span class="line"><span class="cl">memory.usage_in_bytes      #显示当前已用的内存
</span></span><span class="line"><span class="cl">memory.limit_in_bytes      #设置/显示当前限制的内存额度
</span></span><span class="line"><span class="cl">memory.failcnt             #显示内存使用量达到限制值的次数
</span></span><span class="line"><span class="cl">memory.max_usage_in_bytes  #历史内存最大使用量
</span></span><span class="line"><span class="cl">memory.soft_limit_in_bytes #设置/显示当前限制的内存软额度
</span></span><span class="line"><span class="cl">memory.stat                #显示当前cgroup的内存使用情况
</span></span><span class="line"><span class="cl">memory.use_hierarchy       #设置/显示是否将子cgroup的内存使用情况统计到当前cgroup里面
</span></span><span class="line"><span class="cl">memory.force_empty         #触发系统立即尽可能的回收当前cgroup中可以回收的内存
</span></span><span class="line"><span class="cl">memory.pressure_level      #设置内存压力的通知事件，配合cgroup.event_control一起使用
</span></span><span class="line"><span class="cl">memory.swappiness          #设置和显示当前的swappiness
</span></span><span class="line"><span class="cl">memory.move_charge_at_immigrate #设置当进程移动到其他cgroup中时，它所占用的内存是否也随着移动过去
</span></span><span class="line"><span class="cl">memory.oom_control         #设置/显示oom controls相关的配置
</span></span><span class="line"><span class="cl">memory.numa_stat           #显示numa相关的内存
</span></span></code></pre></div><p>更多信息可参考<a href="https://qingwave.github.io/2018/11/15/Pod-memory-usage-in-k8s/#Cadvisor%E4%B8%AD%E6%9C%89%E5%85%B3pod%E5%86%85%E5%AD%98%E4%BD%BF%E7%94%A8%E7%8E%87%E7%9A%84%E6%8C%87%E6%A0%87" target="_blank" rel="noopener noreffer">Pod memory usage in k8s</a></p>
<h2 id="查看源码" class="headerLink">
    <a href="#%e6%9f%a5%e7%9c%8b%e6%ba%90%e7%a0%81" class="header-mark"></a>查看源码</h2><h3 id="docker-stat" class="headerLink">
    <a href="#docker-stat" class="header-mark"></a>docker stat</h3><p>docker stat的源码在<a href="https://github.com/docker/cli/blob/37f9a88c696ae81be14c1697bd083d6421b4933c/cli/command/container/stats_helpers.go#L233" target="_blank" rel="noopener noreffer">stats_helpers.go</a>,如下：</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-golang" data-lang="golang"><span class="line"><span class="cl"><span class="kd">func</span> <span class="nf">calculateMemUsageUnixNoCache</span><span class="p">(</span><span class="nx">mem</span> <span class="nx">types</span><span class="p">.</span><span class="nx">MemoryStats</span><span class="p">)</span> <span class="kt">float64</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="nb">float64</span><span class="p">(</span><span class="nx">mem</span><span class="p">.</span><span class="nx">Usage</span> <span class="o">-</span> <span class="nx">mem</span><span class="p">.</span><span class="nx">Stats</span><span class="p">[</span><span class="s">&#34;cache&#34;</span><span class="p">])</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span></code></pre></div><p>内存使用量为<code>memory.usage=memory.usage_in_bytes-cache</code></p>
<h3 id="kubectl-top" class="headerLink">
    <a href="#kubectl-top" class="header-mark"></a>kubectl top</h3><p>在k8s中，<code>kubectl top</code>命令通过<code>metric-server/heapster</code>获取cadvisor中<code>working_set</code>的值，来表示Pod实例使用内存大小(不包括pause),metrics-server 中<a href="https://github.com/kubernetes-sigs/metrics-server/blob/d4432d67b2fc435b9c71a89c13659882008a4c54/pkg/sources/summary/summary.go#L206" target="_blank" rel="noopener noreffer">pod内存</a>获取如下：</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-golang" data-lang="golang"><span class="line"><span class="cl"><span class="kd">func</span> <span class="nf">decodeMemory</span><span class="p">(</span><span class="nx">target</span> <span class="o">*</span><span class="nx">resource</span><span class="p">.</span><span class="nx">Quantity</span><span class="p">,</span> <span class="nx">memStats</span> <span class="o">*</span><span class="nx">stats</span><span class="p">.</span><span class="nx">MemoryStats</span><span class="p">)</span> <span class="kt">error</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">	<span class="k">if</span> <span class="nx">memStats</span> <span class="o">==</span> <span class="kc">nil</span> <span class="o">||</span> <span class="nx">memStats</span><span class="p">.</span><span class="nx">WorkingSetBytes</span> <span class="o">==</span> <span class="kc">nil</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">		<span class="k">return</span> <span class="nx">fmt</span><span class="p">.</span><span class="nf">Errorf</span><span class="p">(</span><span class="s">&#34;missing memory usage metric&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">	<span class="p">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">	<span class="o">*</span><span class="nx">target</span> <span class="p">=</span> <span class="o">*</span><span class="nf">uint64Quantity</span><span class="p">(</span><span class="o">*</span><span class="nx">memStats</span><span class="p">.</span><span class="nx">WorkingSetBytes</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">	<span class="nx">target</span><span class="p">.</span><span class="nx">Format</span> <span class="p">=</span> <span class="nx">resource</span><span class="p">.</span><span class="nx">BinarySI</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">	<span class="k">return</span> <span class="kc">nil</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span></code></pre></div><p>cadvisor中<a href="https://github.com/google/cadvisor/blob/0ff17b8d0df3712923c46ca484701b876d02dfee/container/libcontainer/handler.go#L706" target="_blank" rel="noopener noreffer">working_set</a>计算如下：</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-golang" data-lang="golang"><span class="line"><span class="cl"><span class="kd">func</span> <span class="nf">setMemoryStats</span><span class="p">(</span><span class="nx">s</span> <span class="o">*</span><span class="nx">cgroups</span><span class="p">.</span><span class="nx">Stats</span><span class="p">,</span> <span class="nx">ret</span> <span class="o">*</span><span class="nx">info</span><span class="p">.</span><span class="nx">ContainerStats</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">	<span class="nx">ret</span><span class="p">.</span><span class="nx">Memory</span><span class="p">.</span><span class="nx">Usage</span> <span class="p">=</span> <span class="nx">s</span><span class="p">.</span><span class="nx">MemoryStats</span><span class="p">.</span><span class="nx">Usage</span><span class="p">.</span><span class="nx">Usage</span>
</span></span><span class="line"><span class="cl">	<span class="nx">ret</span><span class="p">.</span><span class="nx">Memory</span><span class="p">.</span><span class="nx">MaxUsage</span> <span class="p">=</span> <span class="nx">s</span><span class="p">.</span><span class="nx">MemoryStats</span><span class="p">.</span><span class="nx">Usage</span><span class="p">.</span><span class="nx">MaxUsage</span>
</span></span><span class="line"><span class="cl">	<span class="nx">ret</span><span class="p">.</span><span class="nx">Memory</span><span class="p">.</span><span class="nx">Failcnt</span> <span class="p">=</span> <span class="nx">s</span><span class="p">.</span><span class="nx">MemoryStats</span><span class="p">.</span><span class="nx">Usage</span><span class="p">.</span><span class="nx">Failcnt</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">	<span class="k">if</span> <span class="nx">s</span><span class="p">.</span><span class="nx">MemoryStats</span><span class="p">.</span><span class="nx">UseHierarchy</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">		<span class="nx">ret</span><span class="p">.</span><span class="nx">Memory</span><span class="p">.</span><span class="nx">Cache</span> <span class="p">=</span> <span class="nx">s</span><span class="p">.</span><span class="nx">MemoryStats</span><span class="p">.</span><span class="nx">Stats</span><span class="p">[</span><span class="s">&#34;total_cache&#34;</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">		<span class="nx">ret</span><span class="p">.</span><span class="nx">Memory</span><span class="p">.</span><span class="nx">RSS</span> <span class="p">=</span> <span class="nx">s</span><span class="p">.</span><span class="nx">MemoryStats</span><span class="p">.</span><span class="nx">Stats</span><span class="p">[</span><span class="s">&#34;total_rss&#34;</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">		<span class="nx">ret</span><span class="p">.</span><span class="nx">Memory</span><span class="p">.</span><span class="nx">Swap</span> <span class="p">=</span> <span class="nx">s</span><span class="p">.</span><span class="nx">MemoryStats</span><span class="p">.</span><span class="nx">Stats</span><span class="p">[</span><span class="s">&#34;total_swap&#34;</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">		<span class="nx">ret</span><span class="p">.</span><span class="nx">Memory</span><span class="p">.</span><span class="nx">MappedFile</span> <span class="p">=</span> <span class="nx">s</span><span class="p">.</span><span class="nx">MemoryStats</span><span class="p">.</span><span class="nx">Stats</span><span class="p">[</span><span class="s">&#34;total_mapped_file&#34;</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">	<span class="p">}</span> <span class="k">else</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">		<span class="nx">ret</span><span class="p">.</span><span class="nx">Memory</span><span class="p">.</span><span class="nx">Cache</span> <span class="p">=</span> <span class="nx">s</span><span class="p">.</span><span class="nx">MemoryStats</span><span class="p">.</span><span class="nx">Stats</span><span class="p">[</span><span class="s">&#34;cache&#34;</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">		<span class="nx">ret</span><span class="p">.</span><span class="nx">Memory</span><span class="p">.</span><span class="nx">RSS</span> <span class="p">=</span> <span class="nx">s</span><span class="p">.</span><span class="nx">MemoryStats</span><span class="p">.</span><span class="nx">Stats</span><span class="p">[</span><span class="s">&#34;rss&#34;</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">		<span class="nx">ret</span><span class="p">.</span><span class="nx">Memory</span><span class="p">.</span><span class="nx">Swap</span> <span class="p">=</span> <span class="nx">s</span><span class="p">.</span><span class="nx">MemoryStats</span><span class="p">.</span><span class="nx">Stats</span><span class="p">[</span><span class="s">&#34;swap&#34;</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">		<span class="nx">ret</span><span class="p">.</span><span class="nx">Memory</span><span class="p">.</span><span class="nx">MappedFile</span> <span class="p">=</span> <span class="nx">s</span><span class="p">.</span><span class="nx">MemoryStats</span><span class="p">.</span><span class="nx">Stats</span><span class="p">[</span><span class="s">&#34;mapped_file&#34;</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">	<span class="p">}</span>
</span></span><span class="line"><span class="cl">	<span class="k">if</span> <span class="nx">v</span><span class="p">,</span> <span class="nx">ok</span> <span class="o">:=</span> <span class="nx">s</span><span class="p">.</span><span class="nx">MemoryStats</span><span class="p">.</span><span class="nx">Stats</span><span class="p">[</span><span class="s">&#34;pgfault&#34;</span><span class="p">];</span> <span class="nx">ok</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">		<span class="nx">ret</span><span class="p">.</span><span class="nx">Memory</span><span class="p">.</span><span class="nx">ContainerData</span><span class="p">.</span><span class="nx">Pgfault</span> <span class="p">=</span> <span class="nx">v</span>
</span></span><span class="line"><span class="cl">		<span class="nx">ret</span><span class="p">.</span><span class="nx">Memory</span><span class="p">.</span><span class="nx">HierarchicalData</span><span class="p">.</span><span class="nx">Pgfault</span> <span class="p">=</span> <span class="nx">v</span>
</span></span><span class="line"><span class="cl">	<span class="p">}</span>
</span></span><span class="line"><span class="cl">	<span class="k">if</span> <span class="nx">v</span><span class="p">,</span> <span class="nx">ok</span> <span class="o">:=</span> <span class="nx">s</span><span class="p">.</span><span class="nx">MemoryStats</span><span class="p">.</span><span class="nx">Stats</span><span class="p">[</span><span class="s">&#34;pgmajfault&#34;</span><span class="p">];</span> <span class="nx">ok</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">		<span class="nx">ret</span><span class="p">.</span><span class="nx">Memory</span><span class="p">.</span><span class="nx">ContainerData</span><span class="p">.</span><span class="nx">Pgmajfault</span> <span class="p">=</span> <span class="nx">v</span>
</span></span><span class="line"><span class="cl">		<span class="nx">ret</span><span class="p">.</span><span class="nx">Memory</span><span class="p">.</span><span class="nx">HierarchicalData</span><span class="p">.</span><span class="nx">Pgmajfault</span> <span class="p">=</span> <span class="nx">v</span>
</span></span><span class="line"><span class="cl">	<span class="p">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">	<span class="nx">workingSet</span> <span class="o">:=</span> <span class="nx">ret</span><span class="p">.</span><span class="nx">Memory</span><span class="p">.</span><span class="nx">Usage</span>
</span></span><span class="line"><span class="cl">	<span class="k">if</span> <span class="nx">v</span><span class="p">,</span> <span class="nx">ok</span> <span class="o">:=</span> <span class="nx">s</span><span class="p">.</span><span class="nx">MemoryStats</span><span class="p">.</span><span class="nx">Stats</span><span class="p">[</span><span class="s">&#34;total_inactive_file&#34;</span><span class="p">];</span> <span class="nx">ok</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">		<span class="k">if</span> <span class="nx">workingSet</span> <span class="p">&lt;</span> <span class="nx">v</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">			<span class="nx">workingSet</span> <span class="p">=</span> <span class="mi">0</span>
</span></span><span class="line"><span class="cl">		<span class="p">}</span> <span class="k">else</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">			<span class="nx">workingSet</span> <span class="o">-=</span> <span class="nx">v</span>
</span></span><span class="line"><span class="cl">		<span class="p">}</span>
</span></span><span class="line"><span class="cl">	<span class="p">}</span>
</span></span><span class="line"><span class="cl">	<span class="nx">ret</span><span class="p">.</span><span class="nx">Memory</span><span class="p">.</span><span class="nx">WorkingSet</span> <span class="p">=</span> <span class="nx">workingSet</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span></code></pre></div><p><code>working_set=memory.usage_in_bytes-total_inactive_file (&gt;=0)</code>
在kubelet中节点内存不足时同样以<code>working_set</code>判断pod是否OOM的标准</p>
<h2 id="实验" class="headerLink">
    <a href="#%e5%ae%9e%e9%aa%8c" class="header-mark"></a>实验</h2><ol>
<li>创建Pod
Pod的资源申请如下：</li>
</ol>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-yaml" data-lang="yaml"><span class="line"><span class="cl"><span class="w">        </span><span class="nt">resources</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">          </span><span class="nt">limits</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">            </span><span class="nt">cpu</span><span class="p">:</span><span class="w"> </span><span class="s2">&#34;1&#34;</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">            </span><span class="nt">memory</span><span class="p">:</span><span class="w"> </span><span class="l">1Gi</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">          </span><span class="nt">requests</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">            </span><span class="nt">cpu</span><span class="p">:</span><span class="w"> </span><span class="s2">&#34;0&#34;</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">            </span><span class="nt">memory</span><span class="p">:</span><span class="w"> </span><span class="s2">&#34;0&#34;</span><span class="w">
</span></span></span></code></pre></div><ol start="2">
<li>查看cgroup内存情况
找到容器某个进程，查看memory cgroup</li>
</ol>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl"><span class="c1"># cat /proc/16062/cgroup </span>
</span></span><span class="line"><span class="cl">...
</span></span><span class="line"><span class="cl">8:memory:/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod21a55da5_f9f8_11e9_b051_fa163e7e981a.slice/docker-57ba1991ab4ba50a9b2eaf5bf90e2c20073198d767653becf77d55ee25e1a6f9.scope
</span></span></code></pre></div><p>进入容器memory cgroup对应的目录</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">docker-57ba1991ab4ba50a9b2eaf5bf90e2c20073198d767653becf77d55ee25e1a6f9.scope]# ls
</span></span><span class="line"><span class="cl">cgroup.clone_children  memory.kmem.failcnt             memory.kmem.tcp.limit_in_bytes      memory.max_usage_in_bytes        memory.move_charge_at_immigrate  memory.stat            tasks
</span></span><span class="line"><span class="cl">cgroup.event_control   memory.kmem.limit_in_bytes      memory.kmem.tcp.max_usage_in_bytes  memory.memsw.failcnt             memory.numa_stat                 memory.swappiness
</span></span><span class="line"><span class="cl">cgroup.procs           memory.kmem.max_usage_in_bytes  memory.kmem.tcp.usage_in_bytes      memory.memsw.limit_in_bytes      memory.oom_control               memory.usage_in_bytes
</span></span><span class="line"><span class="cl">memory.failcnt         memory.kmem.slabinfo            memory.kmem.usage_in_bytes          memory.memsw.max_usage_in_bytes  memory.pressure_level            memory.use_hierarchy
</span></span><span class="line"><span class="cl">memory.force_empty     memory.kmem.tcp.failcnt         memory.limit_in_bytes               memory.memsw.usage_in_bytes      memory.soft_limit_in_bytes       notify_on_release
</span></span></code></pre></div><p>查看主要memory文件</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl"><span class="c1"># cat memory.limit_in_bytes (容器memory limit值，即1Gi)</span>
</span></span><span class="line"><span class="cl"><span class="m">1073741824</span>
</span></span><span class="line"><span class="cl"><span class="o">[</span>root@node01 docker-57ba1991ab4ba50a9b2eaf5bf90e2c20073198d767653becf77d55ee25e1a6f9.scope<span class="o">]</span><span class="c1"># cat memory.kmem.limit_in_bytes (容器内核使用memory limit值)</span>
</span></span><span class="line"><span class="cl"><span class="m">9223372036854771712</span>
</span></span><span class="line"><span class="cl"><span class="o">[</span>root@node01 docker-57ba1991ab4ba50a9b2eaf5bf90e2c20073198d767653becf77d55ee25e1a6f9.scope<span class="o">]</span><span class="c1"># </span>
</span></span><span class="line"><span class="cl"><span class="o">[</span>root@node01 docker-57ba1991ab4ba50a9b2eaf5bf90e2c20073198d767653becf77d55ee25e1a6f9.scope<span class="o">]</span><span class="c1"># cat memory.soft_limit_in_bytes</span>
</span></span><span class="line"><span class="cl"><span class="m">9223372036854771712</span>
</span></span><span class="line"><span class="cl"><span class="o">[</span>docker-57ba1991ab4ba50a9b2eaf5bf90e2c20073198d767653becf77d55ee25e1a6f9.scope<span class="o">]</span><span class="c1"># cat notify_on_release</span>
</span></span><span class="line"><span class="cl"><span class="m">0</span>
</span></span><span class="line"><span class="cl"><span class="o">[</span>docker-57ba1991ab4ba50a9b2eaf5bf90e2c20073198d767653becf77d55ee25e1a6f9.scope<span class="o">]</span><span class="c1"># cat memory.oom_control </span>
</span></span><span class="line"><span class="cl">oom_kill_disable <span class="m">0</span>
</span></span><span class="line"><span class="cl">under_oom <span class="m">0</span>
</span></span><span class="line"><span class="cl">oom_kill <span class="m">0</span>
</span></span><span class="line"><span class="cl"><span class="o">[</span>docker-57ba1991ab4ba50a9b2eaf5bf90e2c20073198d767653becf77d55ee25e1a6f9.scope<span class="o">]</span><span class="c1"># cat memory.usage_in_bytes </span>
</span></span><span class="line"><span class="cl"><span class="m">2265088</span>
</span></span><span class="line"><span class="cl"><span class="o">[</span>docker-57ba1991ab4ba50a9b2eaf5bf90e2c20073198d767653becf77d55ee25e1a6f9.scope<span class="o">]</span><span class="c1"># cat memory.kmem.usage_in_bytes </span>
</span></span><span class="line"><span class="cl"><span class="m">901120</span>
</span></span><span class="line"><span class="cl"><span class="o">[</span>docker-57ba1991ab4ba50a9b2eaf5bf90e2c20073198d767653becf77d55ee25e1a6f9.scope<span class="o">]</span><span class="c1"># cat memory.stat </span>
</span></span><span class="line"><span class="cl">cache <span class="m">12288</span>
</span></span><span class="line"><span class="cl">rss <span class="m">1351680</span>
</span></span><span class="line"><span class="cl">rss_huge <span class="m">0</span>
</span></span><span class="line"><span class="cl">shmem <span class="m">4096</span>
</span></span><span class="line"><span class="cl">mapped_file <span class="m">4096</span>
</span></span><span class="line"><span class="cl">dirty <span class="m">0</span>
</span></span><span class="line"><span class="cl">writeback <span class="m">0</span>
</span></span><span class="line"><span class="cl">swap <span class="m">0</span>
</span></span><span class="line"><span class="cl">pgpgin <span class="m">4544</span>
</span></span><span class="line"><span class="cl">pgpgout <span class="m">4211</span>
</span></span><span class="line"><span class="cl">pgfault <span class="m">1948</span>
</span></span><span class="line"><span class="cl">pgmajfault <span class="m">0</span>
</span></span><span class="line"><span class="cl">inactive_anon <span class="m">4096</span>
</span></span><span class="line"><span class="cl">active_anon <span class="m">1351680</span>
</span></span><span class="line"><span class="cl">inactive_file <span class="m">8192</span>
</span></span><span class="line"><span class="cl">active_file <span class="m">0</span>
</span></span><span class="line"><span class="cl">unevictable <span class="m">0</span>
</span></span><span class="line"><span class="cl">hierarchical_memory_limit <span class="m">1073741824</span>
</span></span><span class="line"><span class="cl">hierarchical_memsw_limit <span class="m">1073741824</span>
</span></span><span class="line"><span class="cl">total_cache <span class="m">12288</span>
</span></span><span class="line"><span class="cl">total_rss <span class="m">1351680</span>
</span></span><span class="line"><span class="cl">total_rss_huge <span class="m">0</span>
</span></span><span class="line"><span class="cl">total_shmem <span class="m">4096</span>
</span></span><span class="line"><span class="cl">total_mapped_file <span class="m">4096</span>
</span></span><span class="line"><span class="cl">total_dirty <span class="m">0</span>
</span></span><span class="line"><span class="cl">total_writeback <span class="m">0</span>
</span></span><span class="line"><span class="cl">total_swap <span class="m">0</span>
</span></span><span class="line"><span class="cl">total_pgpgin <span class="m">4544</span>
</span></span><span class="line"><span class="cl">total_pgpgout <span class="m">4211</span>
</span></span><span class="line"><span class="cl">total_pgfault <span class="m">1948</span>
</span></span><span class="line"><span class="cl">total_pgmajfault <span class="m">0</span>
</span></span><span class="line"><span class="cl">total_inactive_anon <span class="m">4096</span>
</span></span><span class="line"><span class="cl">total_active_anon <span class="m">1351680</span>
</span></span><span class="line"><span class="cl">total_inactive_file <span class="m">8192</span>
</span></span><span class="line"><span class="cl">total_active_file <span class="m">0</span>
</span></span><span class="line"><span class="cl">total_unevictable <span class="m">0</span>
</span></span></code></pre></div><p>根据memory可得到如下关系：
<code>memory.usage_in_bytes = memory.kmem.usage_in_bytes + rss + cache</code>
即2265088=901120+1351680+12288</p>
<p>那么容器的真实内存即：
<code>memory.usage=memory.usage_in_bytes-cache</code>
即<code>rss+kmem_usage</code></p>
<p>通过<code>docker stat</code>查看，与公式相符合</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">CONTAINER ID        NAME                                                                                     CPU %               MEM USAGE / LIMIT   MEM %               NET I/O             BLOCK I/O           PIDS
</span></span><span class="line"><span class="cl">57ba1991ab4b        k8s...default_21a55da5-f9f8-11e9-b051-fa163e7e981a_0   0.00%               2.148MiB / 1GiB     0.21%               12MB / 68.8MB       0B / 0B             2
</span></span></code></pre></div><h2 id="结论" class="headerLink">
    <a href="#%e7%bb%93%e8%ae%ba" class="header-mark"></a>结论</h2><p>实际环境中，docker与k8s两种内存表示方式不同，一般<code>docker stat</code>总体值会小于<code>kubectl top</code></p>
<ul>
<li>docker中内存表示为：
<code>memory.usage = memory.usage_in_bytes - cache</code></li>
<li>k8s中：
<code>memory.usage = working_set = memory.usage_in_bytes - total_inactive_file (&gt;=0)</code>
根据cgroup memory关系有：
<code>memory.usage_in_bytes = memory.kmem.usage_in_bytes + rss + cache</code></li>
</ul>
<p>真实环境中两种表示相差不大，但更推荐使用<code>working_set</code>作为容器内存真实使用量(kubelt判断OOM的依据)，
则容器内存使用率可表示为：
<code>container_memory_working_set_bytes / memory.limit_in_bytes</code></p>
<h2 id="参考" class="headerLink">
    <a href="#%e5%8f%82%e8%80%83" class="header-mark"></a>参考</h2><ol>
<li><a href="https://www.kernel.org/doc/Documentation/cgroup-v1/memory.txt" target="_blank" rel="noopener noreffer">https://www.kernel.org/doc/Documentation/cgroup-v1/memory.txt</a></li>
<li><a href="https://medium.com/@zhimin.wen/memory-limit-of-pod-and-oom-killer-891ee1f1cad8" target="_blank" rel="noopener noreffer">https://medium.com/@zhimin.wen/memory-limit-of-pod-and-oom-killer-891ee1f1cad8</a></li>
</ol>
]]></description>
</item><item>
    <title>k8s与docker组件堆栈及Debug</title>
    <link>https://qingwave.github.io/k8s-docker-stack/</link>
    <pubDate>Sat, 11 May 2019 23:11:45 &#43;0000</pubDate><author>
        <name>qinng</name>
    </author><guid>https://qingwave.github.io/k8s-docker-stack/</guid>
    <description><![CDATA[<p>k8s组件日志级别热更新</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl"><span class="c1"># 调整日志级别到3</span>
</span></span><span class="line"><span class="cl">curl -X PUT http://127.0.0.1:8081/debug/flags/v -d <span class="s2">&#34;3&#34;</span>
</span></span></code></pre></div><p>controller manager</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">wget http://localhost:10252/debug/pprof/profile
</span></span><span class="line"><span class="cl">wget http://localhost:10252/debug/pprof/heap
</span></span><span class="line"><span class="cl">curl http://127.0.0.1:10252/debug/pprof/goroutine?debug<span class="o">=</span><span class="m">1</span> &gt;&gt; debug1
</span></span><span class="line"><span class="cl">curl http://127.0.0.1:10252/debug/pprof/goroutine?debug<span class="o">=</span><span class="m">2</span> &gt;&gt; debug2
</span></span></code></pre></div><p>scheduler</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl"><span class="nb">kill</span> -12 <span class="si">${</span><span class="nv">SCHED_PID</span><span class="si">}</span>
</span></span><span class="line"><span class="cl">获取scheduler cache信息，输出到日志
</span></span></code></pre></div><p>kubelet 堆栈信息</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">wget http://localhost:10250/debug/pprof/profile
</span></span><span class="line"><span class="cl">wget http://localhost:10250/debug/pprof/heap
</span></span><span class="line"><span class="cl">curl http://127.0.0.1:10250/debug/pprof/goroutine?debug<span class="o">=</span><span class="m">1</span> &gt;&gt; debug1
</span></span><span class="line"><span class="cl">curl http://127.0.0.1:10250/debug/pprof/goroutine?debug<span class="o">=</span><span class="m">2</span> &gt;&gt; debug2
</span></span></code></pre></div><p>docker 堆栈信息</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">curl --unix-socket /var/run/docker.sock -X GET http://v1.2/debug/pprof/profile
</span></span><span class="line"><span class="cl">curl --unix-socket /var/run/docker.sock -X GET http://v1.2/debug/pprof/
</span></span><span class="line"><span class="cl">curl --unix-socket /var/run/docker.sock -X GET http://v1.2/debug/pprof/
</span></span><span class="line"><span class="cl"> 
</span></span><span class="line"><span class="cl">sudo <span class="nb">kill</span> -SIGUSR1 <span class="k">$(</span>pidof dockerd<span class="k">)</span>
</span></span><span class="line"><span class="cl">/var/run/docker/
</span></span><span class="line"><span class="cl"> 
</span></span><span class="line"><span class="cl">curl --unix-socket /var/run/docker.sock -X GET http://v1.2/debug/pprof/profile &gt;&gt;docker.profile
</span></span><span class="line"><span class="cl">curl --unix-socket /var/run/docker.sock -X GET http://v1.2/debug/pprof/goroutine &gt;&gt; docker.goroutine
</span></span><span class="line"><span class="cl">curl --unix-socket /var/run/docker.sock -X GET http://v1.2/debug/pprof/goroutine?debug<span class="o">=</span><span class="m">2</span> &gt;&gt;docker.gorouting_debug_2
</span></span><span class="line"><span class="cl">curl --unix-socket /var/run/docker.sock -X GET http://v1.2/debug/pprof/heap?debug<span class="o">=</span><span class="m">2</span> &gt;&gt;docker.heap
</span></span></code></pre></div><p>docker-registry 堆栈信息</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl"><span class="c1">#先登入机器,然后执行</span>
</span></span><span class="line"><span class="cl">wget localhost:5002/debug/pprof/profile <span class="c1">#这个是cpu占用时间的采样结果，要先等30s</span>
</span></span><span class="line"><span class="cl">wget localhost:5002/debug/pprof/heap <span class="c1">#内存的使用情况</span>
</span></span><span class="line"><span class="cl">wget localhost:5002/debug/pprof/goroutine?debug<span class="o">=</span><span class="m">2</span> <span class="c1">#调用栈的全部信息</span>
</span></span><span class="line"><span class="cl">wget localhost:5002/debug/pprof/goroutine
</span></span><span class="line"><span class="cl">其他可用的profile:
</span></span><span class="line"><span class="cl">allocs block goroutine cmdline mutex threadcreate trace，替换上面命令pprof/后面的词即可
</span></span></code></pre></div>]]></description>
</item><item>
    <title>深入理解K8s资源限制</title>
    <link>https://qingwave.github.io/understanding-resource-limits-in-kubernetes/</link>
    <pubDate>Wed, 09 Jan 2019 16:34:33 &#43;0000</pubDate><author>
        <name>Qingwave</name>
    </author><guid>https://qingwave.github.io/understanding-resource-limits-in-kubernetes/</guid>
    <description><![CDATA[<h2 id="写在前面" class="headerLink">
    <a href="#%e5%86%99%e5%9c%a8%e5%89%8d%e9%9d%a2" class="header-mark"></a>写在前面</h2><p>当我开始大范围使用Kubernetes的时候，我开始考虑一个我做实验时没有遇到的问题：当集群里的节点没有足够资源的时候，Pod会卡在Pending状态。你是没有办法给节点增加CPU或者内存的，那么你该怎么做才能将这个Pod从这个节点拿走？最简单的办法是添加另一个节点，我承认我总是这么干。最终这个策略无法发挥出Kubernetes最重要的一个能力：即它优化计算资源使用的能力。这些场景里面实际的问题并不是节点太小，而是我们没有仔细为Pod计算过资源限制。</p>
<p>资源限制是我们可以向Kubernetes提供的诸多配置之一，它意味着两点：工作负载运行需要哪些资源；最多允许消费多少资源。第一点对于调度器而言十分重要，因为它要以此选择合适的节点。第二点对于Kubelet非常重要，每个节点上的守护进程Kubelet负责Pod的运行健康状态。大多数本文的读者可能对资源限制有一定的了解，实际上这里面有很多有趣的细节。在这个系列的两篇文章中我会先仔细分析内存资源限制，然后第二篇文章中分析CPU资源限制。</p>
<h2 id="资源限制" class="headerLink">
    <a href="#%e8%b5%84%e6%ba%90%e9%99%90%e5%88%b6" class="header-mark"></a>资源限制</h2><p>资源限制是通过每个容器containerSpec的resources字段进行设置的，它是v1版本的ResourceRequirements类型的API对象。每个指定了&quot;limits&quot;和&quot;requests&quot;的对象都可以控制对应的资源。目前只有CPU和内存两种资源。第三种资源类型，持久化存储仍然是beta版本，我会在以后的博客里进行分析。大多数情况下，deployment、statefulset、daemonset的定义里都包含了podSpec和多个containerSpec。这里有个完整的v1资源对象的yaml格式配置：</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-yaml" data-lang="yaml"><span class="line"><span class="cl"><span class="nt">resources</span><span class="p">:</span><span class="w">  
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">requests</span><span class="p">:</span><span class="w">    
</span></span></span><span class="line"><span class="cl"><span class="w">        </span><span class="nt">cpu</span><span class="p">:</span><span class="w"> </span><span class="l">50m</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">        </span><span class="nt">memory</span><span class="p">:</span><span class="w"> </span><span class="l">50Mi</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">limits</span><span class="p">:</span><span class="w">    
</span></span></span><span class="line"><span class="cl"><span class="w">        </span><span class="nt">cpu</span><span class="p">:</span><span class="w"> </span><span class="l">100m</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">        </span><span class="nt">memory</span><span class="p">:</span><span class="w"> </span><span class="l">100Mi</span><span class="w">
</span></span></span></code></pre></div><p>这个对象可以这么理解：这个容器通常情况下，需要5%的CPU时间和50MiB的内存（requests），同时最多允许它使用10%的CPU时间和100MiB的内存（limits）。我会对requests和limits的区别做进一步讲解，但是一般来说，在调度的时候requests比较重要，在运行时limits比较重要。尽管资源限制配置在每个容器上，你可以认为Pod的资源限制就是它里面容器的资源限制之和，我们可以从系统的视角观察到这种关系。</p>
<h3 id="内存限制" class="headerLink">
    <a href="#%e5%86%85%e5%ad%98%e9%99%90%e5%88%b6" class="header-mark"></a>内存限制</h3><p>通常情况下分析内存要比分析CPU简单一些，所以我从这里开始着手。我的一个目标是给大家展示内存在系统中是如何实现的，也就是Kubernetes对容器运行时（docker/containerd）所做的工作，容器运行时对Linux内核所做的工作。从分析内存资源限制开始也为后面分析CPU打好了基础。首先，让我们回顾一下前面的例子：</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-yaml" data-lang="yaml"><span class="line"><span class="cl"><span class="nt">resources</span><span class="p">:</span><span class="w">  
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">requests</span><span class="p">:</span><span class="w">    
</span></span></span><span class="line"><span class="cl"><span class="w">        </span><span class="nt">memory</span><span class="p">:</span><span class="w"> </span><span class="l">50Mi</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">limits</span><span class="p">:</span><span class="w">    
</span></span></span><span class="line"><span class="cl"><span class="w">        </span><span class="nt">memory</span><span class="p">:</span><span class="w"> </span><span class="l">100Mi</span><span class="w">
</span></span></span></code></pre></div><p>单位后缀Mi表示的是MiB，所以这个资源对象定义了这个容器需要50MiB并且最多能使用100MiB的内存。当然还有其他单位可以进行表示。为了了解如何用这些值是来控制容器进程，我们首先创建一个没有配置内存限制的Pod:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">$ kubectl run limit-test --image<span class="o">=</span>busybox --command -- /bin/sh -c <span class="s2">&#34;while true; do sleep 2; done&#34;</span>
</span></span><span class="line"><span class="cl">deployment.apps <span class="s2">&#34;limit-test&#34;</span> created
</span></span></code></pre></div><p>用Kubectl命令我们可以验证这个Pod是没有资源限制的：</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">$ kubectl get pods limit-test-7cff9996fc-zpjps -o<span class="o">=</span><span class="nv">jsonpath</span><span class="o">=</span><span class="s1">&#39;{.spec.containers[0].resources}&#39;</span>
</span></span><span class="line"><span class="cl">map<span class="o">[]</span>
</span></span></code></pre></div><p>Kubernetes最酷的一点是你可以跳到系统以外的角度来观察每个构成部分，所以我们登录到运行Pod的节点，看看Docker是如何运行这个容器的：</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">$ docker ps <span class="p">|</span> grep busy <span class="p">|</span> cut -d<span class="s1">&#39; &#39;</span> -f1
</span></span><span class="line"><span class="cl">5c3af3101afb
</span></span><span class="line"><span class="cl">$ docker inspect 5c3af3101afb -f <span class="s2">&#34;{{.HostConfig.Memory}}&#34;</span>
</span></span><span class="line"><span class="cl"><span class="m">0</span>
</span></span></code></pre></div><p>这个容器的<code>.HostConfig.Memory</code>域对应了docker run时的<code>--memory</code>参数，0值表示未设定。Docker会对这个值做什么？为了控制容器进程能够访问的内存数量，Docker配置了一组control group，或者叫cgroup。Cgroup在2008年1月时合并到Linux 2.6.24版本的内核。它是一个很重要的话题。我们说cgroup是容器的一组用来控制内核如何运行进程的相关属性集合。针对内存、CPU和各种设备都有对应的cgroup。Cgroup是具有层级的，这意味着每个cgroup拥有一个它可以继承属性的父亲，往上一直直到系统启动时创建的root cgroup。</p>
<p>Cgroup可以通过/proc和/sys伪文件系统轻松查看到，所以检查容器如何配置内存的cgroup就很简单了。在容器的Pid namespace里，根进程的pid为1，但是namespace以外它呈现的是系统级pid，我们可以用来查找它的cgroups：</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">$ ps ax <span class="p">|</span> grep /bin/sh
</span></span><span class="line"><span class="cl">   <span class="m">9513</span> ?        Ss     0:00 /bin/sh -c <span class="k">while</span> true<span class="p">;</span> <span class="k">do</span> sleep 2<span class="p">;</span> <span class="k">done</span>
</span></span><span class="line"><span class="cl">$ sudo cat /proc/9513/cgroup
</span></span><span class="line"><span class="cl">...
</span></span><span class="line"><span class="cl">6:memory:/kubepods/burstable/podfbc202d3-da21-11e8-ab5e-42010a80014b/0a1b22ec1361a97c3511db37a4bae932d41b22264e5b97611748f8b662312574
</span></span></code></pre></div><p>我列出了内存cgroup，这正是我们所关注的。你在路径里可以看到前面提到的cgroup层级。一些比较重要的点是：首先，这个路径是以kubepods开始的cgroup，所以我们的进程继承了这个group的每个属性，还有burstable的属性（Kubernetes将Pod设置为burstable QoS类别）和一组用于审计的Pod表示。最后一段路径是我们进程实际使用的cgroup。我们可以把它追加到<code>/sys/fs/cgroups/memory</code>后面查看更多信息：</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">$ ls -l /sys/fs/cgroup/memory/kubepods/burstable/podfbc202d3-da21-11e8-ab5e-42010a80014b/0a1b22ec1361a97c3511db37a4bae932d41b22264e5b97611748f8b662312574
</span></span><span class="line"><span class="cl">...
</span></span><span class="line"><span class="cl">-rw-r--r-- <span class="m">1</span> root root <span class="m">0</span> Oct <span class="m">27</span> 19:53 memory.limit_in_bytes
</span></span><span class="line"><span class="cl">-rw-r--r-- <span class="m">1</span> root root <span class="m">0</span> Oct <span class="m">27</span> 19:53 memory.soft_limit_in_bytes
</span></span></code></pre></div><p>再一次，我只列出了我们所关心的记录。我们暂时不关注<code>memory.soft_limit_in_bytes</code>，而将重点转移到<code>memory.limit_in_bytes</code>属性，它设置了内存限制。它等价于Docker命令中的<code>--memory</code>参数，也就是Kubernetes里的内存资源限制。我们看看：</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">$ sudo cat /sys/fs/cgroup/memory/kubepods/burstable/podfbc202d3-da21-11e8-ab5e-42010a80014b/0a1b22ec1361a97c3511db37a4bae932d41b22264e5b97611748f8b662312574/memory.limit_in_bytes
</span></span><span class="line"><span class="cl"><span class="m">9223372036854771712</span>
</span></span></code></pre></div><p>这是没有设置资源限制时我的节点上显示的情况。这里有对它的一个简单的解释(<a href="https://unix.stackexchange.com/questions/420906/what-is-the-value-for-the-cgroups-limit-in-bytes-if-the-memory-is-not-restricte" target="_blank" rel="noopener noreffer">https://unix.stackexchange.com/questions/420906/what-is-the-value-for-the-cgroups-limit-in-bytes-if-the-memory-is-not-restricte</a>)。 所以我们看到如果没有在Kubernetes里设置内存限制的话，会导致Docker设置<code>HostConfig.Memory</code>值为0，并进一步导致容器进程被放置在默认值为&quot;no limit&quot;的<code>memory.limit_in_bytes</code>内存cgroup下。我们现在创建使用100MiB内存限制的Pod：</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">$ kubectl run limit-test --image<span class="o">=</span>busybox --limits <span class="s2">&#34;memory=100Mi&#34;</span> --command -- /bin/sh -c <span class="s2">&#34;while true; do sleep 2; done&#34;</span>
</span></span><span class="line"><span class="cl">deployment.apps <span class="s2">&#34;limit-test&#34;</span> created
</span></span></code></pre></div><p>我们再一次使用kubectl验证我们的资源配置：</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">$ kubectl get pods limit-test-5f5c7dc87d-8qtdx -o<span class="o">=</span><span class="nv">jsonpath</span><span class="o">=</span><span class="s1">&#39;{.spec.containers[0].resources}&#39;</span>
</span></span><span class="line"><span class="cl">map<span class="o">[</span>limits:map<span class="o">[</span>memory:100Mi<span class="o">]</span> requests:map<span class="o">[</span>memory:100Mi<span class="o">]]</span>
</span></span></code></pre></div><p>你会注意到除了我们设置的limits外，Pod还增加了requests。当你设置limits而没有设置requests时，Kubernetes默认让requests等于limits。如果你从调度器的角度看这是非常有意义的。我会在下面进一步讨论requests。当这个Pod启动后，我们可以看到Docker如何配置的容器以及这个进程的内存cgroup：</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">$ docker ps <span class="p">|</span> grep busy <span class="p">|</span> cut -d<span class="s1">&#39; &#39;</span> -f1
</span></span><span class="line"><span class="cl">8fec6c7b6119
</span></span><span class="line"><span class="cl">$ docker inspect 8fec6c7b6119 --format <span class="s1">&#39;{{.HostConfig.Memory}}&#39;</span>
</span></span><span class="line"><span class="cl"><span class="m">104857600</span>
</span></span><span class="line"><span class="cl">$ ps ax <span class="p">|</span> grep /bin/sh
</span></span><span class="line"><span class="cl">   <span class="m">29532</span> ?      Ss     0:00 /bin/sh -c <span class="k">while</span> true<span class="p">;</span> <span class="k">do</span> sleep 2<span class="p">;</span> <span class="k">done</span>
</span></span><span class="line"><span class="cl">$ sudo cat /proc/29532/cgroup
</span></span><span class="line"><span class="cl">...
</span></span><span class="line"><span class="cl">6:memory:/kubepods/burstable/pod88f89108-daf7-11e8-b1e1-42010a800070/8fec6c7b61190e74cd9f88286181dd5fa3bbf9cf33c947574eb61462bc254d11
</span></span><span class="line"><span class="cl">$ sudo cat /sys/fs/cgroup/memory/kubepods/burstable/pod88f89108-daf7-11e8-b1e1-42010a800070/8fec6c7b61190e74cd9f88286181dd5fa3bbf9cf33c947574eb61462bc254d11/memory.limit_in_bytes
</span></span><span class="line"><span class="cl"><span class="m">104857600</span>
</span></span></code></pre></div><p>正如你所见，Docker基于我们的containerSpec正确地设置了这个进程的内存cgroup。但是这对于运行时意味着什么？Linux内存管理是一个复杂的话题，Kubernetes工程师需要知道的是：当一个宿主机遇到了内存资源压力时，内核可能会有选择性地杀死进程。如果一个使用了多于限制内存的进程会有更高几率被杀死。因为Kubernetes的任务是尽可能多地向这些节点上安排Pod，这会导致节点内存压力异常。如果你的容器使用了过多内存，那么它很可能会被oom-killed。如果Docker收到了内核的通知，Kubernetes会找到这个容器并依据设置尝试重启这个Pod。</p>
<p>所以Kubernetes默认创建的内存requests是什么？拥有一个100MiB的内存请求会影响到cgroup？可能它设置了我们之前看到的<code>memory.soft_limit_in_bytes</code>？让我们看看：</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">$ sudo cat /sys/fs/cgroup/memory/kubepods/burstable/pod88f89108-daf7-11e8-b1e1-42010a800070/8fec6c7b61190e74cd9f88286181dd5fa3bbf9cf33c947574eb61462bc254d11/memory.soft_limit_in_bytes
</span></span><span class="line"><span class="cl"><span class="m">9223372036854771712</span>
</span></span></code></pre></div><p>你可以看到软限制仍然被设置为默认值“no limit”。即使Docker支持通过参数<code>--memory-reservation</code>进行设置，但Kubernetes并不支持这个参数。这是否意味着为你的容器指定内存requests并不重要？不，不是的。requests要比limits更重要。limits告诉Linux内核什么时候你的进程可以为了清理空间而被杀死。requests帮助Kubernetes调度找到合适的节点运行Pod。如果不设置它们，或者设置得非常低，那么可能会有不好的影响。</p>
<p>例如，假设你没有配置内存requests来运行Pod，而配置了一个较高的limits。正如我们所知道的Kubernetes默认会把requests的值指向limits，如果没有合适的资源的节点的话，Pod可能会调度失败，即使它实际需要的资源并没有那么多。另一方面，如果你运行了一个配置了较低requests值的Pod，你其实是在鼓励内核oom-kill掉它。为什么？假设你的Pod通常使用100MiB内存，你却只为它配置了50MiB内存requests。如果你有一个拥有75MiB内存空间的节点，那么这个Pod会被调度到这个节点。当Pod内存消耗扩大到100MiB时，会让这个节点压力变大，这个时候内核可能会选择杀掉你的进程。所以我们要正确配置Pod的内存requests和limits。</p>
<p>希望这篇文章能够帮助说明Kubernetes容器内存限制是如何设置和实现的，以及为什么你需要正确设置这些值。如果你为Kubernetes提供了它所需要的足够信息，它可以智能地调度你的任务并最大化使用你的云计算资源。</p>
<h3 id="cpu限制" class="headerLink">
    <a href="#cpu%e9%99%90%e5%88%b6" class="header-mark"></a>CPU限制</h3><p>CPU 资源限制比内存资源限制更复杂，原因将在下文详述。幸运的是 CPU 资源限制和内存资源限制一样都是由 cgroup 控制的，上文中提到的思路和工具在这里同样适用，我们只需要关注他们的不同点就行了。首先，让我们将 CPU 资源限制添加到之前示例中的 yaml：</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-yaml" data-lang="yaml"><span class="line"><span class="cl"><span class="nt">resources</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">requests</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">memory</span><span class="p">:</span><span class="w"> </span><span class="l">50Mi</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">cpu</span><span class="p">:</span><span class="w"> </span><span class="l">50m</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">limits</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">memory</span><span class="p">:</span><span class="w"> </span><span class="l">100Mi</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">cpu</span><span class="p">:</span><span class="w"> </span><span class="l">100m</span><span class="w">
</span></span></span></code></pre></div><p>单位后缀 m 表示千分之一核，也就是说 1 Core = 1000m。因此该资源对象指定容器进程需要 50/1000 核（5%）才能被调度，并且允许最多使用 100/1000 核（10%）。同样，2000m 表示两个完整的 CPU 核心，你也可以写成 2 或者 2.0。为了了解 Docker 和 cgroup 如何使用这些值来控制容器，我们首先创建一个只配置了 CPU requests 的 Pod：</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">$ kubectl run limit-test --image<span class="o">=</span>busybox --requests <span class="s2">&#34;cpu=50m&#34;</span> --command -- /bin/sh -c <span class="s2">&#34;while true; do sleep 2; done&#34;</span>
</span></span><span class="line"><span class="cl">deployment.apps <span class="s2">&#34;limit-test&#34;</span> created
</span></span></code></pre></div><p>通过 kubectl 命令我们可以验证这个 Pod 配置了 50m 的 CPU requests：</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">$ kubectl get pods limit-test-5b4c495556-p2xkr -o<span class="o">=</span><span class="nv">jsonpath</span><span class="o">=</span><span class="s1">&#39;{.spec.containers[0].resources}&#39;</span>
</span></span><span class="line"><span class="cl">map<span class="o">[</span>requests:map<span class="o">[</span>cpu:50m<span class="o">]]</span>
</span></span></code></pre></div><p>我们还可以看到 Docker 为容器配置了相同的资源限制：</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">$ docker ps <span class="p">|</span> grep busy <span class="p">|</span> cut -d<span class="s1">&#39; &#39;</span> -f1
</span></span><span class="line"><span class="cl">f2321226620e
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">$ docker inspect f2321226620e --format <span class="s1">&#39;{{.HostConfig.CpuShares}}&#39;</span>
</span></span><span class="line"><span class="cl"><span class="m">51</span>
</span></span></code></pre></div><p>这里显示的为什么是 51，而不是 50？这是因为 Linux cgroup 和 Docker 都将 CPU 核心数分成了 1024 个时间片（shares），而 Kubernetes 将它分成了 1000 个 shares。
shares 用来设置 CPU 的相对值，并且是针对所有的 CPU（内核），默认值是 1024，假如系统中有两个 cgroup，分别是 A 和 B，A 的 shares 值是 1024，B 的 shares 值是 512，那么 A 将获得 1024/(1204+512)=66% 的 CPU 资源，而 B 将获得 33% 的 CPU 资源。</p>
<p>shares 有两个特点：</p>
<ol>
<li>如果 A 不忙，没有使用到 66% 的 CPU 时间，那么剩余的 CPU 时间将会被系统分配给 B，即 B 的 CPU 使用率可以超过 33%。</li>
<li>如果添加了一个新的 cgroup C，且它的 shares 值是 1024，那么 A 的限额变成了 1024/(1204+512+1024)=40%，B 的变成了 20%。</li>
</ol>
<p>从上面两个特点可以看出：</p>
<p>在闲的时候，shares 基本上不起作用，只有在 CPU 忙的时候起作用，这是一个优点。
由于 shares 是一个绝对值，需要和其它 cgroup 的值进行比较才能得到自己的相对限额，而在一个部署很多容器的机器上，cgroup 的数量是变化的，所以这个限额也是变化的，自己设置了一个高的值，但别人可能设置了一个更高的值，所以这个功能没法精确的控制 CPU 使用率。</p>
<p>与配置内存资源限制时 Docker 配置容器进程的内存 cgroup 的方式相同，设置 CPU 资源限制时 Docker 会配置容器进程的 cpu,cpuacct cgroup：</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">$ ps ax <span class="p">|</span> grep /bin/sh
</span></span><span class="line"><span class="cl">   <span class="m">60554</span> ?      Ss     0:00 /bin/sh -c <span class="k">while</span> true<span class="p">;</span> <span class="k">do</span> sleep 2<span class="p">;</span> <span class="k">done</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">$ sudo cat /proc/60554/cgroup
</span></span><span class="line"><span class="cl">...
</span></span><span class="line"><span class="cl">4:cpu,cpuacct:/kubepods/burstable/pode12b33b1-db07-11e8-b1e1-42010a800070/3be263e7a8372b12d2f8f8f9b4251f110b79c2a3bb9e6857b2f1473e640e8e75
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">$ ls -l /sys/fs/cgroup/cpu,cpuacct/kubepods/burstable/pode12b33b1-db07-11e8-b1e1-42010a800070/3be263e7a8372b12d2f8f8f9b4251f110b79c2a3bb9e6857b2f1473e640e8e75
</span></span><span class="line"><span class="cl">total <span class="m">0</span>
</span></span><span class="line"><span class="cl">drwxr-xr-x <span class="m">2</span> root root <span class="m">0</span> Oct <span class="m">28</span> 23:19 .
</span></span><span class="line"><span class="cl">drwxr-xr-x <span class="m">4</span> root root <span class="m">0</span> Oct <span class="m">28</span> 23:19 ..
</span></span><span class="line"><span class="cl">...
</span></span><span class="line"><span class="cl">-rw-r--r-- <span class="m">1</span> root root <span class="m">0</span> Oct <span class="m">28</span> 23:19 cpu.shares
</span></span></code></pre></div><p>Docker 容器的 HostConfig.CpuShares 属性映射到 cgroup 的 cpu.shares 属性，可以验证一下：</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">$ sudo cat /sys/fs/cgroup/cpu,cpuacct/kubepods/burstable/podb5c03ddf-db10-11e8-b1e1-42010a800070/64b5f1b636dafe6635ddd321c5b36854a8add51931c7117025a694281fb11444/cpu.shares
</span></span><span class="line"><span class="cl"><span class="m">51</span>
</span></span></code></pre></div><p>你可能会很惊讶，设置了 CPU requests 竟然会把值传播到 cgroup，而在上一篇文章中我们设置内存 requests 时并没有将值传播到 cgroup。这是因为内存的 soft limit 内核特性对 Kubernetes 不起作用，而设置了 cpu.shares 却对 Kubernetes 很有用。后面我会详细讨论为什么会这样。现在让我们先看看设置 CPU limits 时会发生什么：</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">$ kubectl run limit-test --image<span class="o">=</span>busybox --requests <span class="s2">&#34;cpu=50m&#34;</span> --limits <span class="s2">&#34;cpu=100m&#34;</span> --command -- /bin/sh -c <span class="s2">&#34;while true; do
</span></span></span><span class="line"><span class="cl"><span class="s2">sleep 2; done&#34;</span>
</span></span><span class="line"><span class="cl">deployment.apps <span class="s2">&#34;limit-test&#34;</span> created
</span></span></code></pre></div><p>再一次使用 kubectl 验证我们的资源配置：</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">$ kubectl get pods limit-test-5b4fb64549-qpd4n -o<span class="o">=</span><span class="nv">jsonpath</span><span class="o">=</span><span class="s1">&#39;{.spec.containers[0].resources}&#39;</span>
</span></span><span class="line"><span class="cl">map<span class="o">[</span>limits:map<span class="o">[</span>cpu:100m<span class="o">]</span> requests:map<span class="o">[</span>cpu:50m<span class="o">]]</span>
</span></span></code></pre></div><p>查看对应的 Docker 容器的配置：</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">$ docker ps <span class="p">|</span> grep busy <span class="p">|</span> cut -d<span class="s1">&#39; &#39;</span> -f1
</span></span><span class="line"><span class="cl">f2321226620e
</span></span><span class="line"><span class="cl">$ docker inspect 472abbce32a5 --format <span class="s1">&#39;{{.HostConfig.CpuShares}} {{.HostConfig.CpuQuota}} {{.HostConfig.CpuPeriod}}&#39;</span>
</span></span><span class="line"><span class="cl"><span class="m">51</span> <span class="m">10000</span> <span class="m">100000</span>
</span></span></code></pre></div><p>可以明显看出，CPU requests 对应于 Docker 容器的 HostConfig.CpuShares 属性。而 CPU limits 就不太明显了，它由两个属性控制：HostConfig.CpuPeriod 和 HostConfig.CpuQuota。Docker 容器中的这两个属性又会映射到进程的 cpu,couacct cgroup 的另外两个属性：cpu.cfs_period_us 和 cpu.cfs_quota_us。我们来看一下：</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">$ sudo cat /sys/fs/cgroup/cpu,cpuacct/kubepods/burstable/pod2f1b50b6-db13-11e8-b1e1-42010a800070/f0845c65c3073e0b7b0b95ce0c1eb27f69d12b1fe2382b50096c4b59e78cdf71/cpu.cfs_period_us
</span></span><span class="line"><span class="cl"><span class="m">100000</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">$ sudo cat /sys/fs/cgroup/cpu,cpuacct/kubepods/burstable/pod2f1b50b6-db13-11e8-b1e1-42010a800070/f0845c65c3073e0b7b0b95ce0c1eb27f69d12b1fe2382b50096c4b59e78cdf71/cpu.cfs_quota_us
</span></span><span class="line"><span class="cl"><span class="m">10000</span>
</span></span></code></pre></div><p>如我所说，这些值与容器配置中指定的值相同。但是这两个属性的值是如何从我们在 Pod 中设置的 100m cpu limits 得出的呢，他们是如何实现该 limits 的呢？这是因为 cpu requests 和 cpu limits 是使用两个独立的控制系统来实现的。Requests 使用的是 cpu shares 系统，cpu shares 将每个 CPU 核心划分为 1024 个时间片，并保证每个进程将获得固定比例份额的时间片。如果总共有 1024 个时间片，并且两个进程中的每一个都将 cpu.shares 设置为 512，那么它们将分别获得大约一半的 CPU 可用时间。但 cpu shares 系统无法精确控制 CPU 使用率的上限，如果一个进程没有设置 shares，则另一个进程可用自由使用 CPU 资源。</p>
<p>大约在 2010 年左右，谷歌团队和其他一部分人注意到了这个问题。为了解决这个问题，后来在 linux 内核中增加了第二个功能更强大的控制系统：CPU 带宽控制组。带宽控制组定义了一个 周期，通常为 1/10 秒（即 100000 微秒）。还定义了一个 配额，表示允许进程在设置的周期长度内所能使用的 CPU 时间数，两个文件配合起来设置CPU的使用上限。两个文件的单位都是微秒（us），cfs_period_us 的取值范围为 1 毫秒（ms）到 1 秒（s），cfs_quota_us 的取值大于 1ms 即可，如果 cfs_quota_us 的值为 -1（默认值），表示不受 CPU 时间的限制。</p>
<p>下面是几个例子：</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl"><span class="c1"># 1.限制只能使用1个CPU（每250ms能使用250ms的CPU时间）</span>
</span></span><span class="line"><span class="cl">$ <span class="nb">echo</span> <span class="m">250000</span> &gt; cpu.cfs_quota_us /* <span class="nv">quota</span> <span class="o">=</span> 250ms */
</span></span><span class="line"><span class="cl">$ <span class="nb">echo</span> <span class="m">250000</span> &gt; cpu.cfs_period_us /* <span class="nv">period</span> <span class="o">=</span> 250ms */
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># 2.限制使用2个CPU（内核）（每500ms能使用1000ms的CPU时间，即使用两个内核）</span>
</span></span><span class="line"><span class="cl">$ <span class="nb">echo</span> <span class="m">1000000</span> &gt; cpu.cfs_quota_us /* <span class="nv">quota</span> <span class="o">=</span> 1000ms */
</span></span><span class="line"><span class="cl">$ <span class="nb">echo</span> <span class="m">500000</span> &gt; cpu.cfs_period_us /* <span class="nv">period</span> <span class="o">=</span> 500ms */
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># 3.限制使用1个CPU的20%（每50ms能使用10ms的CPU时间，即使用一个CPU核心的20%）</span>
</span></span><span class="line"><span class="cl">$ <span class="nb">echo</span> <span class="m">10000</span> &gt; cpu.cfs_quota_us /* <span class="nv">quota</span> <span class="o">=</span> 10ms */
</span></span><span class="line"><span class="cl">$ <span class="nb">echo</span> <span class="m">50000</span> &gt; cpu.cfs_period_us /* <span class="nv">period</span> <span class="o">=</span> 50ms */
</span></span></code></pre></div><p>在本例中我们将 Pod 的 cpu limits 设置为 100m，这表示 100/1000 个 CPU 核心，即 100000 微秒的 CPU 时间周期中的 10000。所以该 limits 翻译到 cpu,cpuacct cgroup 中被设置为 cpu.cfs_period_us=100000 和 cpu.cfs_quota_us=10000。顺便说一下，其中的 cfs 代表 Completely Fair Scheduler（绝对公平调度），这是 Linux 系统中默认的 CPU 调度算法。还有一个实时调度算法，它也有自己相应的配额值。</p>
<p>现在让我们来总结一下：</p>
<ul>
<li>在 Kubernetes 中设置的 cpu requests 最终会被 cgroup 设置为 cpu.shares 属性的值， cpu limits 会被带宽控制组设置为 cpu.cfs_period_us 和 cpu.cfs_quota_us 属性的值。与内存一样，cpu requests 主要用于在调度时通知调度器节点上至少需要多少个 cpu shares 才可以被调度。</li>
<li>与 内存 requests 不同，设置了 cpu requests 会在 cgroup 中设置一个属性，以确保内核会将该数量的 shares 分配给进程。</li>
<li>cpu limits 与 内存 limits 也有所不同。如果容器进程使用的内存资源超过了内存使用限制，那么该进程将会成为 oom-killing 的候选者。但是容器进程基本上永远不能超过设置的 CPU 配额，所以容器永远不会因为尝试使用比分配的更多的 CPU 时间而被驱逐。系统会在调度程序中强制进行 CPU 资源限制，以确保进程不会超过这个限制。</li>
</ul>
<p>如果你没有在容器中设置这些属性，或将他们设置为不准确的值，会发生什么呢？与内存一样，如果只设置了 limits 而没有设置 requests，Kubernetes 会将 CPU 的 requests 设置为 与 limits 的值一样。如果你对你的工作负载所需要的 CPU 时间了如指掌，那再好不过了。如果只设置了 CPU requests 却没有设置 CPU limits 会怎么样呢？这种情况下，Kubernetes 会确保该 Pod 被调度到合适的节点，并且该节点的内核会确保节点上的可用 cpu shares 大于 Pod 请求的 cpu shares，但是你的进程不会被阻止使用超过所请求的 CPU 数量。既不设置 requests 也不设置 limits 是最糟糕的情况：调度程序不知道容器需要什么，并且进程对 cpu shares 的使用是无限制的，这可能会对 node 产生一些负面影响。</p>
<p>最后我还想告诉你们的是：为每个 pod 都手动配置这些参数是挺麻烦的事情，kubernetes 提供了 LimitRange 资源，可以让我们配置某个 namespace 默认的 request 和 limit 值。</p>
<h2 id="默认限制" class="headerLink">
    <a href="#%e9%bb%98%e8%ae%a4%e9%99%90%e5%88%b6" class="header-mark"></a>默认限制</h2><p>通过上文的讨论大家已经知道了忽略资源限制会对 Pod 产生负面影响，因此你可能会想，如果能够配置某个 namespace 默认的 request 和 limit 值就好了，这样每次创建新 Pod 都会默认加上这些限制。Kubernetes 允许我们通过 LimitRange 资源对每个命名空间设置资源限制。要创建默认的资源限制，需要在对应的命名空间中创建一个 LimitRange 资源。下面是一个例子：</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-yaml" data-lang="yaml"><span class="line"><span class="cl"><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l">v1</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l">LimitRange</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">metadata</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">default-limit</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">spec</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">limits</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span>- <span class="nt">default</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">      </span><span class="nt">memory</span><span class="p">:</span><span class="w"> </span><span class="l">100Mi</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">      </span><span class="nt">cpu</span><span class="p">:</span><span class="w"> </span><span class="l">100m</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">defaultRequest</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">      </span><span class="nt">memory</span><span class="p">:</span><span class="w"> </span><span class="l">50Mi</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">      </span><span class="nt">cpu</span><span class="p">:</span><span class="w"> </span><span class="l">50m</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span>- <span class="nt">max</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">      </span><span class="nt">memory</span><span class="p">:</span><span class="w"> </span><span class="l">512Mi</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">      </span><span class="nt">cpu</span><span class="p">:</span><span class="w"> </span><span class="l">500m</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span>- <span class="nt">min</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">      </span><span class="nt">memory</span><span class="p">:</span><span class="w"> </span><span class="l">50Mi</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">      </span><span class="nt">cpu</span><span class="p">:</span><span class="w"> </span><span class="l">50m</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">type</span><span class="p">:</span><span class="w"> </span><span class="l">Container</span><span class="w">
</span></span></span></code></pre></div><p>这里的几个字段可能会让你们有些困惑，我拆开来给你们分析一下。</p>
<ul>
<li>limits 字段下面的 default 字段表示每个 Pod 的默认的 limits 配置，所以任何没有分配资源的 limits 的 Pod 都会被自动分配 100Mi limits 的内存和 100m limits 的 CPU。</li>
<li>defaultRequest 字段表示每个 Pod 的默认 requests 配置，所以任何没有分配资源的 requests 的 Pod 都会被自动分配 50Mi requests 的内存和 50m requests 的 CPU。</li>
<li>max 和 min 字段比较特殊，如果设置了这两个字段，那么只要这个命名空间中的 Pod 设置的 limits 和 requests 超过了这个上限和下限，就不会允许这个 Pod 被创建。我暂时还没有发现这两个字段的用途，如果你知道，欢迎在留言告诉我。</li>
<li>LimitRange 中设定的默认值最后由 Kubernetes 中的准入控制器 LimitRanger 插件来实现。准入控制器由一系列插件组成，它会在 API 接收对象之后创建 Pod 之前对 Pod 的 Spec - 字段进行修改。对于 LimitRanger 插件来说，它会检查每个 Pod 是否设置了 limits 和 requests，如果没有设置，就给它配置 LimitRange 中设定的默认值。通过检查 Pod 中的 annotations 注释，你可以看到 LimitRanger 插件已经在你的 Pod 中设置了默认值。例如：</li>
</ul>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-yaml" data-lang="yaml"><span class="line"><span class="cl"><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l">v1</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l">Pod</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">metadata</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">annotations</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">kubernetes.io/limit-ranger: &#39;LimitRanger plugin set</span><span class="p">:</span><span class="w"> </span><span class="l">cpu request for container</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">      </span><span class="l">limit-test&#39;</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">limit-test-859d78bc65-g6657</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">namespace</span><span class="p">:</span><span class="w"> </span><span class="l">default</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">spec</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">containers</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span>- <span class="nt">args</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span>- <span class="l">/bin/sh</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span>- -<span class="l">c</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span>- <span class="l">while true; do sleep 2; done</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">image</span><span class="p">:</span><span class="w"> </span><span class="l">busybox</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">imagePullPolicy</span><span class="p">:</span><span class="w"> </span><span class="l">Always</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">limit-test</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">resources</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">      </span><span class="nt">requests</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">        </span><span class="nt">cpu</span><span class="p">:</span><span class="w"> </span><span class="l">100m</span><span class="w">
</span></span></span></code></pre></div><p>以上就是我对 Kubernetes 资源限制的全部见解，希望能对你有所帮助。如果你想了解更多关于 Kubernetes 中资源的 limits 和 requests、以及 linux cgroup 和内存管理的更多详细信息，可以查看我在文末提供的参考链接。</p>
<h2 id="参考文档" class="headerLink">
    <a href="#%e5%8f%82%e8%80%83%e6%96%87%e6%a1%a3" class="header-mark"></a>参考文档</h2><ul>
<li><a href="https://medium.com/@betz.mark/understanding-resource-limits-in-kubernetes-cpu-time-9eff74d3161b" target="_blank" rel="noopener noreffer">https://medium.com/@betz.mark/understanding-resource-limits-in-kubernetes-cpu-time-9eff74d3161b</a></li>
<li><a href="https://medium.com/@betz.mark/understanding-resource-limits-in-kubernetes-memory-6b41e9a955f9" target="_blank" rel="noopener noreffer">https://medium.com/@betz.mark/understanding-resource-limits-in-kubernetes-memory-6b41e9a955f9</a></li>
</ul>
]]></description>
</item><item>
    <title>Pod memory usage in k8s</title>
    <link>https://qingwave.github.io/pod-memory-usage-in-k8s/</link>
    <pubDate>Thu, 15 Nov 2018 12:42:15 &#43;0000</pubDate><author>
        <name>Qingwave</name>
    </author><guid>https://qingwave.github.io/pod-memory-usage-in-k8s/</guid>
    <description><![CDATA[<h2 id="cadvisor内存使用率指标" class="headerLink">
    <a href="#cadvisor%e5%86%85%e5%ad%98%e4%bd%bf%e7%94%a8%e7%8e%87%e6%8c%87%e6%a0%87" class="header-mark"></a>Cadvisor内存使用率指标</h2><h3 id="cadvisor中有关pod内存使用率的指标" class="headerLink">
    <a href="#cadvisor%e4%b8%ad%e6%9c%89%e5%85%b3pod%e5%86%85%e5%ad%98%e4%bd%bf%e7%94%a8%e7%8e%87%e7%9a%84%e6%8c%87%e6%a0%87" class="header-mark"></a>Cadvisor中有关pod内存使用率的指标</h3><table>
<thead>
<tr>
<th>指标</th>
<th>说明</th>
</tr>
</thead>
<tbody>
<tr>
<td>container_memory_cache</td>
<td>Number of bytes of page cache memory.</td>
</tr>
<tr>
<td>container_memory_rss</td>
<td>Size of RSS in bytes.(包括匿名映射页和交换区缓存)</td>
</tr>
<tr>
<td>container_memory_swap</td>
<td>Container swap usage in bytes.</td>
</tr>
<tr>
<td>container_memory_usage_bytes</td>
<td>Current memory usage in bytes,including all memory regardless ofwhen it was accessed. (包括 cache, rss, swap等)</td>
</tr>
<tr>
<td>container_memory_max_usage_bytes</td>
<td>Maximum memory usage recorded in bytes.</td>
</tr>
<tr>
<td>container_memory_working_set_bytes</td>
<td>Current working set in bytes. （工作区内存使用量=活跃的匿名与和缓存,以及file-baked页 &lt;=container_memory_usage_bytes）</td>
</tr>
<tr>
<td>container_memory_failcnt</td>
<td>Number of memory usage hits limits.</td>
</tr>
<tr>
<td>container_memory_failures_total</td>
<td>Cumulative count of memory allocation failures.</td>
</tr>
<tr>
<td>其中</td>
<td></td>
</tr>
<tr>
<td><code>container_memory_max_usage_bytes &gt; container_memory_usage_bytes &gt;= container_memory_working_set_bytes &gt; container_memory_rss</code></td>
<td></td>
</tr>
</tbody>
</table>
<h3 id="cadvisor中相关定义" class="headerLink">
    <a href="#cadvisor%e4%b8%ad%e7%9b%b8%e5%85%b3%e5%ae%9a%e4%b9%89" class="header-mark"></a>Cadvisor中相关定义</h3><div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">type MemoryStats struct { // Current memory usage, this includes all memory regardless of when it was // accessed. // Units: Bytes. Usage uint64 json:&#34;usage&#34;
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">// Maximum memory usage recorded.
</span></span><span class="line"><span class="cl">	// Units: Bytes.
</span></span><span class="line"><span class="cl">	MaxUsage uint64 `json:&#34;max_usage&#34;`
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">	// Number of bytes of page cache memory.
</span></span><span class="line"><span class="cl">	// Units: Bytes.
</span></span><span class="line"><span class="cl">	Cache uint64 `json:&#34;cache&#34;`
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">	// The amount of anonymous and swap cache memory (includes transparent
</span></span><span class="line"><span class="cl">	// hugepages).
</span></span><span class="line"><span class="cl">	// Units: Bytes.
</span></span><span class="line"><span class="cl">	RSS uint64 `json:&#34;rss&#34;`
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">	// The amount of swap currently used by the processes in this cgroup
</span></span><span class="line"><span class="cl">	// Units: Bytes.
</span></span><span class="line"><span class="cl">	Swap uint64 `json:&#34;swap&#34;`
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">	// The amount of working set memory, this includes recently accessed memory,
</span></span><span class="line"><span class="cl">	// dirty memory, and kernel memory. Working set is &lt;= &#34;usage&#34;.
</span></span><span class="line"><span class="cl">	// Units: Bytes.
</span></span><span class="line"><span class="cl">	WorkingSet uint64 `json:&#34;working_set&#34;`
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">	Failcnt uint64 `json:&#34;failcnt&#34;`
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">	ContainerData    MemoryStatsMemoryData `json:&#34;container_data,omitempty&#34;`
</span></span><span class="line"><span class="cl">	HierarchicalData MemoryStatsMemoryData `json:&#34;hierarchical_data,omitempty&#34;`
</span></span><span class="line"><span class="cl">}
</span></span></code></pre></div><blockquote>
<p>You might think that memory utilization is easily tracked with container_memory_usage_bytes, however, this metric also includes cached (think filesystem cache) items that can be evicted under memory pressure. The better metric is container_memory_working_set_bytes as this is what the OOM killer is watching for.
To calculate container memory utilization we use: sum(container_memory_working_set_bytes{name!~&ldquo;POD&rdquo;}) by (name)</p>
</blockquote>
<p>kubelet 通过 watch container_memory_working_set_bytes 来判断是否OOM， 所以用 working set来评价容器内存使用量更科学</p>
<h2 id="cgroup中关于mem指标" class="headerLink">
    <a href="#cgroup%e4%b8%ad%e5%85%b3%e4%ba%8emem%e6%8c%87%e6%a0%87" class="header-mark"></a>Cgroup中关于mem指标</h2><p>cgroup目录相关文件</p>
<table>
<thead>
<tr>
<th>文件名</th>
<th>说明</th>
<th>cadvisor中对应指标</th>
</tr>
</thead>
<tbody>
<tr>
<td>memory.usage_in_bytes</td>
<td>已使用的内存量(包含cache和buffer)(字节)，相当于linux的used_meme</td>
<td>container_memory_usage_bytes</td>
</tr>
<tr>
<td>memory.limit_in_bytes</td>
<td>限制的内存总量(字节)，相当于linux的total_mem</td>
<td></td>
</tr>
<tr>
<td>memory.failcnt</td>
<td>申请内存失败次数计数</td>
<td></td>
</tr>
<tr>
<td>memory.memsw.usage_in_bytes</td>
<td>已使用的内存和swap(字节)</td>
<td></td>
</tr>
<tr>
<td>memory.memsw.limit_in_bytes</td>
<td>限制的内存和swap容量(字节)</td>
<td></td>
</tr>
<tr>
<td>memory.memsw.failcnt</td>
<td>申请内存和swap失败次数计数</td>
<td></td>
</tr>
<tr>
<td>memory.stat</td>
<td>内存相关状态</td>
<td></td>
</tr>
</tbody>
</table>
<p>memory.stat中包含有的内存信息</p>
<table>
<thead>
<tr>
<th>统计</th>
<th>描述</th>
<th>cadvisor中对应指标</th>
</tr>
</thead>
<tbody>
<tr>
<td>cache</td>
<td>页缓存，包括 tmpfs（shmem），单位为字节</td>
<td>container_memory_cache</td>
</tr>
<tr>
<td>rss</td>
<td>匿名和 swap 缓存，不包括 tmpfs（shmem），单位为字节</td>
<td>container_memory_rss</td>
</tr>
<tr>
<td>mapped_file</td>
<td>memory-mapped 映射的文件大小，包括 tmpfs（shmem），单位为字节</td>
<td></td>
</tr>
<tr>
<td>pgpgin</td>
<td>存入内存中的页数</td>
<td></td>
</tr>
<tr>
<td>pgpgout</td>
<td>从内存中读出的页数</td>
<td></td>
</tr>
<tr>
<td>swap</td>
<td>swap 用量，单位为字节</td>
<td>container_memory_swap</td>
</tr>
<tr>
<td>active_anon</td>
<td>在活跃的最近最少使用（least-recently-used，LRU）列表中的匿名和 swap 缓存，包括 tmpfs（shmem），单位为字节</td>
<td></td>
</tr>
<tr>
<td>inactive_anon</td>
<td>不活跃的 LRU 列表中的匿名和 swap 缓存，包括 tmpfs（shmem），单位为字节</td>
<td></td>
</tr>
<tr>
<td>active_file</td>
<td>活跃 LRU 列表中的 file-backed 内存，以字节为单位</td>
<td></td>
</tr>
<tr>
<td>inactive_file</td>
<td>不活跃 LRU 列表中的 file-backed 内存，以字节为单位</td>
<td></td>
</tr>
<tr>
<td>unevictable</td>
<td>无法再生的内存，以字节为单位</td>
<td></td>
</tr>
<tr>
<td>hierarchical_memory_limit</td>
<td>包含 memory cgroup 的层级的内存限制，单位为字节</td>
<td></td>
</tr>
<tr>
<td>hierarchical_memsw_limit</td>
<td>包含 memory cgroup 的层级的内存加 swap 限制，单位为字节</td>
<td></td>
</tr>
</tbody>
</table>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">active_anon + inactive_anon = anonymous memory + file cache for tmpfs + swap cache = rss + file cache for tmpfs 
</span></span><span class="line"><span class="cl">active_file + inactive_file = cache - size of tmpfs
</span></span><span class="line"><span class="cl">working set = usage - total_inactive(k8s根据workingset 来判断是否驱逐pod)
</span></span></code></pre></div><p>mstat看到的active/inactive memory就分别是active list和inactive list中的内存大小。如果inactive list很大，表明在必要时可以回收的页面很多；而如果inactive list很小，说明可以回收的页面不多。
Active/inactive memory是针对用户进程所占用的内存而言的，内核占用的内存（包括slab）不在其中。
至于在源代码中看到的ACTIVE_ANON和ACTIVE_FILE，分别表示anonymous pages和file-backed pages。用户进程的内存页分为两种：与文件关联的内存（比如程序文件、数据文件所对应的内存页）和与文件无关的内存（比如进程的堆栈，用malloc申请的内存），前者称为file-backed pages，后者称为anonymous pages。File-backed pages在发生换页(page-in或page-out)时，是从它对应的文件读入或写出；anonymous pages在发生换页时，是对交换区进行读/写操作。</p>
<h2 id="参考" class="headerLink">
    <a href="#%e5%8f%82%e8%80%83" class="header-mark"></a>参考</h2><ul>
<li><a href="https://blog.freshtracks.io/a-deep-dive-into-kubernetes-metrics-part-3-container-resource-metrics-361c5ee46e66" target="_blank" rel="noopener noreffer">https://blog.freshtracks.io/a-deep-dive-into-kubernetes-metrics-part-3-container-resource-metrics-361c5ee46e66</a></li>
<li><a href="https://github.com/google/cadvisor/blob/08f0c2397cbca790a4db0f1212cb592cc88f6e26/info/v1/container.go#L338:6" target="_blank" rel="noopener noreffer">https://github.com/google/cadvisor/blob/08f0c2397cbca790a4db0f1212cb592cc88f6e26/info/v1/container.go#L338:6</a></li>
</ul>
]]></description>
</item><item>
    <title>Pod中时区设置</title>
    <link>https://qingwave.github.io/pod-timezone/</link>
    <pubDate>Sat, 13 Oct 2018 14:18:43 &#43;0000</pubDate><author>
        <name>Qingwave</name>
    </author><guid>https://qingwave.github.io/pod-timezone/</guid>
    <description><![CDATA[<h2 id="pod设置本地时区的两种方法" class="headerLink">
    <a href="#pod%e8%ae%be%e7%bd%ae%e6%9c%ac%e5%9c%b0%e6%97%b6%e5%8c%ba%e7%9a%84%e4%b8%a4%e7%a7%8d%e6%96%b9%e6%b3%95" class="header-mark"></a>Pod设置本地时区的两种方法</h2><p>我们下载的很多容器内的时区都是格林尼治时间，与北京时间差8小时，这将导致容器内的日志和文件创建时间与实际时区不符，有两种方式解决这个问题：</p>
<ul>
<li>修改镜像中的时区配置文件</li>
<li>将宿主机的时区配置文件/etc/localtime使用volume方式挂载到容器中</li>
</ul>
<h3 id="修改dockfile" class="headerLink">
    <a href="#%e4%bf%ae%e6%94%b9dockfile" class="header-mark"></a>修改Dockfile</h3><p>修改前</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">$ docker run -d nginx:latest
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">$ docker ps
</span></span><span class="line"><span class="cl">CONTAINER ID        IMAGE               COMMAND                  CREATED              STATUS              PORTS               NAMES
</span></span><span class="line"><span class="cl">ca7aacad1493        nginx               <span class="s2">&#34;nginx -g &#39;daemon of…&#34;</span>   About a minute ago   Up About a minute   80/tcp              inspiring_elbakyan
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">$ docker <span class="nb">exec</span> -it inspiring_elbakyan date
</span></span><span class="line"><span class="cl">Wed Feb <span class="m">13</span> 06:51:41 UTC <span class="m">2019</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">date
</span></span><span class="line"><span class="cl">Wed Feb <span class="m">13</span> 14:51:45 CST <span class="m">2019</span>
</span></span></code></pre></div><p>创建timezone-dockerfile</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-dockerfile" data-lang="dockerfile"><span class="line"><span class="cl"><span class="k">FROM</span><span class="s"> nginx</span><span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span><span class="k">RUN</span> /bin/cp /usr/share/zoneinfo/Asia/Shanghai /etc/localtime <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>     <span class="o">&amp;&amp;</span> <span class="nb">echo</span> <span class="s1">&#39;Asia/Shanghai&#39;</span> &gt;/etc/timezone<span class="err">
</span></span></span></code></pre></div><div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">$ docker build -t timezone -f timezone-dockerfile .
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">$ docker run -d timezone
</span></span><span class="line"><span class="cl">af39a27d8c8b48b80fb9b052144bd682d75d994dba2e03a02101514304f363d0
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">$ docker <span class="nb">exec</span> -it af39a27d8c8b date
</span></span><span class="line"><span class="cl">Wed Feb <span class="m">13</span> 15:05:14 CST <span class="m">2019</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">$ date
</span></span><span class="line"><span class="cl">Wed Feb <span class="m">13</span> 15:05:16 CST <span class="m">2019</span>
</span></span></code></pre></div><h3 id="挂载localtime文件" class="headerLink">
    <a href="#%e6%8c%82%e8%bd%bdlocaltime%e6%96%87%e4%bb%b6" class="header-mark"></a>挂载localtime文件</h3><p>第二种方式实现更简单，不需要更改镜像，只需要配置yaml文件，步骤如下：</p>
<p>创建测试pod，busybox-pod.yaml</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-yaml" data-lang="yaml"><span class="line"><span class="cl"><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l">v1</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l">Pod</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">metadata</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">busybox</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">namespace</span><span class="p">:</span><span class="w"> </span><span class="l">default</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">spec</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">containers</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span>- <span class="nt">image</span><span class="p">:</span><span class="w"> </span><span class="l">busybox:1.28.3</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">command</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">      </span>- <span class="l">sleep</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">      </span>- <span class="s2">&#34;3600&#34;</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">imagePullPolicy</span><span class="p">:</span><span class="w"> </span><span class="l">IfNotPresent</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">busybox</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">volumeMounts</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">      </span>- <span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">host-time</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">        </span><span class="nt">mountPath</span><span class="p">:</span><span class="w"> </span><span class="l">/etc/localtime</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">        </span><span class="nt">readOnly</span><span class="p">:</span><span class="w"> </span><span class="kc">true</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">volumes</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span>- <span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">host-time</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">      </span><span class="nt">hostPath</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">        </span><span class="nt">path</span><span class="p">:</span><span class="w"> </span><span class="l">/etc/localtime</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">restartPolicy</span><span class="p">:</span><span class="w"> </span><span class="l">Always</span><span class="w">
</span></span></span></code></pre></div><p>测试时间</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">$ kubectl apply -f busybox-pod.yaml
</span></span><span class="line"><span class="cl">pod/busybox created
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">$ kubectl <span class="nb">exec</span> -it busybox date
</span></span><span class="line"><span class="cl">Wed Feb <span class="m">13</span> 06:16:35 UTC <span class="m">2019</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">$ date
</span></span><span class="line"><span class="cl">Wed Feb <span class="m">13</span> 14:16:39 CST <span class="m">2019</span>
</span></span></code></pre></div><p>将/etc/localtime挂载到pod中，配置如下:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-yaml" data-lang="yaml"><span class="line"><span class="cl"><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l">v1</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l">Pod</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">metadata</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">busybox</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">namespace</span><span class="p">:</span><span class="w"> </span><span class="l">default</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">spec</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">containers</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span>- <span class="nt">image</span><span class="p">:</span><span class="w"> </span><span class="l">busybox:1.28.3</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">command</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">      </span>- <span class="l">sleep</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">      </span>- <span class="s2">&#34;3600&#34;</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">imagePullPolicy</span><span class="p">:</span><span class="w"> </span><span class="l">IfNotPresent</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">busybox</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">volumeMounts</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">      </span>- <span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">host-time</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">        </span><span class="nt">mountPath</span><span class="p">:</span><span class="w"> </span><span class="l">/etc/localtime</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">        </span><span class="nt">readOnly</span><span class="p">:</span><span class="w"> </span><span class="kc">true</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">volumes</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span>- <span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">host-time</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">      </span><span class="nt">hostPath</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">        </span><span class="nt">path</span><span class="p">:</span><span class="w"> </span><span class="l">/etc/localtime</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">restartPolicy</span><span class="p">:</span><span class="w"> </span><span class="l">Always</span><span class="w">
</span></span></span></code></pre></div><p>测试时间</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">$ kubectl apply -f busybox-pod.yaml
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">$ kubectl <span class="nb">exec</span> -it busybox date
</span></span><span class="line"><span class="cl">Wed Feb <span class="m">13</span> 14:17:50 CST <span class="m">2019</span> <span class="c1">#与当前时间一致</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">$ date
</span></span><span class="line"><span class="cl">Wed Feb <span class="m">13</span> 14:17:52 CST <span class="m">2019</span>
</span></span></code></pre></div>]]></description>
</item><item>
    <title>apline镜像添加时区与字符设置</title>
    <link>https://qingwave.github.io/apline-timezone/</link>
    <pubDate>Sat, 18 Aug 2018 16:40:42 &#43;0000</pubDate><author>
        <name>Qingwave</name>
    </author><guid>https://qingwave.github.io/apline-timezone/</guid>
    <description><![CDATA[<ol>
<li>
<p>添加时区
设置<code>TZ</code>与安装tzdata</p>
</li>
<li>
<p>添加work用户
<code>addgroup -S work &amp;&amp; adduser -S -G work work -s /bin/sh</code></p>
</li>
<li>
<p>设置字符格式
设置环境变量<code>LANG</code>与<code>LC_ALL</code></p>
</li>
</ol>
<p>Dockerfile如下：</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">FROM alpine
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">ENV TZ=Asia/Shanghai \
</span></span><span class="line"><span class="cl">    LANG=en_US.UTF-8  \
</span></span><span class="line"><span class="cl">    LC_ALL=en_US.UTF8
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl">RUN apk update &amp;&amp; \
</span></span><span class="line"><span class="cl">    apk add --no-cache tzdata &amp;&amp; \
</span></span><span class="line"><span class="cl">    addgroup -S work &amp;&amp; adduser -S -G work work -s /bin/bash &amp;&amp; \
</span></span><span class="line"><span class="cl">    rm -rf /var/cache/apk/* /tmp/* /var/tmp/* $HOME/.cache
</span></span></code></pre></div>]]></description>
</item><item>
    <title>Docker多阶段构建</title>
    <link>https://qingwave.github.io/multi-stage-dockerfile/</link>
    <pubDate>Thu, 16 Aug 2018 17:56:37 &#43;0000</pubDate><author>
        <name>Qingwave</name>
    </author><guid>https://qingwave.github.io/multi-stage-dockerfile/</guid>
    <description><![CDATA[<h2 id="之前的做法" class="headerLink">
    <a href="#%e4%b9%8b%e5%89%8d%e7%9a%84%e5%81%9a%e6%b3%95" class="header-mark"></a>之前的做法</h2><p>在 Docker 17.05 版本之前，我们构建 Docker 镜像时，通常会采用两种方式：</p>
<h3 id="全部放入一个-dockerfile" class="headerLink">
    <a href="#%e5%85%a8%e9%83%a8%e6%94%be%e5%85%a5%e4%b8%80%e4%b8%aa-dockerfile" class="header-mark"></a>全部放入一个 Dockerfile</h3><p>一种方式是将所有的构建过程编包含在一个  Dockerfile  中，包括项目及其依赖库的编译、测试、打包等流程，这里可能会带来的一些问题：</p>
<ul>
<li>Dockerfile  特别长，可维护性降低</li>
<li>镜像层次多，镜像体积较大，部署时间变长</li>
<li>源代码存在泄露的风险</li>
</ul>
<p>例如编写  app.go  文件，该程序输出  Hello World!</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-golang" data-lang="golang"><span class="line"><span class="cl"><span class="kn">package</span> <span class="nx">main</span>  
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="s">&#34;fmt&#34;</span>  
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="kd">func</span> <span class="nf">main</span><span class="p">(){</span>  
</span></span><span class="line"><span class="cl">    <span class="nx">fmt</span><span class="p">.</span><span class="nf">Printf</span><span class="p">(</span><span class="s">&#34;Hello World!&#34;</span><span class="p">);</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span></code></pre></div><p>编写  Dockerfile.one  文件</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">FROM golang:1.9-alpine
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">RUN apk --no-cache add git ca-certificates
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">WORKDIR /go/src/github.com/go/helloworld/
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">COPY app.go .
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">RUN go get -d -v github.com/go-sql-driver/mysql \
</span></span><span class="line"><span class="cl">  &amp;&amp; CGO_ENABLED=0 GOOS=linux go build -a -installsuffix cgo -o app . \
</span></span><span class="line"><span class="cl">  &amp;&amp; cp /go/src/github.com/go/helloworld/app /root
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">WORKDIR /root/
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">CMD [&#34;./app&#34;]
</span></span></code></pre></div><p>构建镜像</p>
<p><code>$ docker build -t go/helloworld:1 -f Dockerfile.one .</code></p>
<h3 id="分散到多个-dockerfile" class="headerLink">
    <a href="#%e5%88%86%e6%95%a3%e5%88%b0%e5%a4%9a%e4%b8%aa-dockerfile" class="header-mark"></a>分散到多个 Dockerfile</h3><p>另一种方式，就是我们事先在一个  Dockerfile  将项目及其依赖库编译测试打包好后，再将其拷贝到运行环境中，这种方式需要我们编写两个  Dockerfile  和一些编译脚本才能将其两个阶段自动整合起来，这种方式虽然可以很好地规避第一种方式存在的风险，但明显部署过程较复杂。</p>
<p>例如，编写  Dockerfile.build  文件</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">FROM golang:1.9-alpine
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">RUN apk --no-cache add git
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">WORKDIR /go/src/github.com/go/helloworld
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">COPY app.go .
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">RUN go get -d -v github.com/go-sql-driver/mysql \
</span></span><span class="line"><span class="cl">  &amp;&amp; CGO_ENABLED=0 GOOS=linux go build -a -installsuffix cgo -o app .
</span></span></code></pre></div><p>编写  Dockerfile.copy  文件</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">FROM alpine:latest
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">RUN apk --no-cache add ca-certificates
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">WORKDIR /root/
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">COPY app .
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">CMD [&#34;./app&#34;]
</span></span></code></pre></div><p>新建  build.sh</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl"><span class="cp">#!/bin/sh
</span></span></span><span class="line"><span class="cl"><span class="cp"></span><span class="nb">echo</span> Building go/helloworld:build
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">docker build -t go/helloworld:build . -f Dockerfile.build
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">docker create --name extract go/helloworld:build
</span></span><span class="line"><span class="cl">docker cp extract:/go/src/github.com/go/helloworld/app ./app
</span></span><span class="line"><span class="cl">docker rm -f extract
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="nb">echo</span> Building go/helloworld:2
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">docker build --no-cache -t go/helloworld:2 . -f Dockerfile.copy
</span></span><span class="line"><span class="cl">rm ./app
</span></span></code></pre></div><p>现在运行脚本即可构建镜像</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">$ chmod +x build.sh
</span></span><span class="line"><span class="cl">$ ./build.sh
</span></span></code></pre></div><p>对比两种方式生成的镜像大小</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">$ docker image ls
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">REPOSITORY      TAG    IMAGE ID        CREATED         SIZE
</span></span><span class="line"><span class="cl">go/helloworld   <span class="m">2</span>      f7cf3465432c    <span class="m">22</span> seconds ago  6.47MB
</span></span><span class="line"><span class="cl">go/helloworld   <span class="m">1</span>      f55d3e16affc    <span class="m">2</span> minutes ago   295MB
</span></span></code></pre></div><h2 id="使用多阶段构建" class="headerLink">
    <a href="#%e4%bd%bf%e7%94%a8%e5%a4%9a%e9%98%b6%e6%ae%b5%e6%9e%84%e5%bb%ba" class="header-mark"></a>使用多阶段构建</h2><p>为解决以上问题，Docker v17.05 开始支持多阶段构建 ( multistage builds )。使用多阶段构建我们就可以很容易解决前面提到的问题，并且只需要编写一个  Dockerfile ：</p>
<p>例如，编写  Dockerfile  文件</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">FROM golang:1.9-alpine as builder
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">RUN apk --no-cache add git
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">WORKDIR /go/src/github.com/go/helloworld/
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">RUN go get -d -v github.com/go-sql-driver/mysql
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">COPY app.go .
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">RUN CGO_ENABLED=0 GOOS=linux go build -a -installsuffix cgo -o app .
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">FROM alpine:latest as prod
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">RUN apk --no-cache add ca-certificates
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">WORKDIR /root/
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">COPY --from=0 /go/src/github.com/go/helloworld/app .
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">CMD [&#34;./app&#34;]
</span></span></code></pre></div><p>构建镜像</p>
<p><code>$ docker build -t go/helloworld:3 .</code></p>
<p>对比三个镜像大小</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">$ docker image ls
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">REPOSITORY        TAG   IMAGE ID         CREATED            SIZE
</span></span><span class="line"><span class="cl">go/helloworld     <span class="m">3</span>     d6911ed9c846     <span class="m">7</span> seconds ago      6.47MB
</span></span><span class="line"><span class="cl">go/helloworld     <span class="m">2</span>     f7cf3465432c     <span class="m">22</span> seconds ago     6.47MB
</span></span><span class="line"><span class="cl">go/helloworld     <span class="m">1</span>     f55d3e16affc     <span class="m">2</span> minutes ago      295MB
</span></span></code></pre></div><p>很明显使用多阶段构建的镜像体积小，同时也完美解决了上边提到的问题。</p>
<p>只构建某一阶段的镜像
我们可以使用  as  来为某一阶段命名，例如</p>
<p><code>FROM golang:1.9-alpine as builder</code></p>
<p>例如当我们只想构建  builder  阶段的镜像时，我们可以在使用  docker build  命令时加上  &ndash;target  参数即可</p>
<p><code>$ docker build --target builder -t username/imagename:tag .</code></p>
<p>构建时从其他镜像复制文件
上面例子中我们使用  COPY &ndash;from=0 /go/src/github.com/go/helloworld/app .  从上一阶段的镜像中复制文件，我们也可以复制任意镜像中的文件。</p>
<p><code>$ COPY --from=nginx:latest /etc/nginx/nginx.conf /nginx.conf</code></p>
]]></description>
</item></channel>
</rss>
