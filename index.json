[{"categories":["杂记"],"content":"惊蛰一过，人也跟着躁动起来，心里的嫩芽和这花儿啊虫儿啊一样冒了出来，想出去看看花、踏踏青、爬爬山，总之是要亲近这春，赏春景、吃春食、醉春风。 回来西安已快两年，时常想起北京的春，玉渊潭的樱花、元大都的海棠、景山的牡丹、S2上漫山遍野的山桃花，最喜欢的还是颐和园的玉兰，可以坐在树下待上好久，嗅着香味儿，看着大爷大妈们长枪短炮的拍花。每个地方的春都是不一样的，或许是参杂了当时的心境，春天一到，就想去出去体验体验这片土地的春意。 过完年，环城公园的迎春花最先绽放，先是星星点点，而后连成一片，青灰色的城墙下开始明朗起来。城墙外的大明宫，最近每年都会去放风筝，有时赶上腊梅，狠狠地在枝头吸上一口，沁人心脾；有时梅花正盛，白的、红的、黄的，总能在花下碰到穿汉服的小姑娘，人面红花，相应成趣；再晚些时候，只能在太液池看着鸭子游来游去。到了三月，天气突然热上几天，紫叶李趁着人们不注意一下子就开满了街角巷边，一阵风来，白色的小花瓣纷纷落下，像是起舞的碎花裙。 三月的中下旬，樱花徐徐开了起来，相比于青龙寺，更喜欢交大的樱花，想起之前在学校，早起去中楼上课穿过樱花东路，团团簇簇的樱花伸出手来，满眼都是繁花。交大的樱花据说是彭康校长组织种下的，大部分是关山樱，三两株绿色的是御衣黄。关山樱是重瓣花，看起来厚重饱满，刚开始是白色的，随着花期慢慢变成粉色，真有些千朵万朵压枝低。清明过后，几场雨下来，花褪残红，落在地上成了花径，又是一番风景。然后，兴庆宫的牡丹和郁金香开始形成规模。再然后，春天也跟着柳絮飘走了。 东坡说花不看开人易老，毕业以后，越发觉得时间过得飞过，一日一日，一年一年好像没有什么区别，或许平淡的日子里也有很多细小的花瓣吧，等待着去观赏去发现。人生又看得几清明，早已没了什么远大的目标，认真生活已是不易，开不成牡丹，开一朵阿拉伯婆婆纳也好。 ","date":"Mar 10, 2023","objectID":"/xian-flower-blooming/:0:0","series":null,"tags":["随笔"],"title":"长安花事","uri":"/xian-flower-blooming/#"},{"categories":["code"],"content":"内存数据库经我们经常用到，例如Redis，那么如何从零实现一个内存数据库呢，本文旨在介绍如何使用Golang编写一个KV内存数据库MossDB。 ","date":"Mar 01, 2023","objectID":"/golang-in-memory-database/:0:0","series":null,"tags":["golang","database","etcd"],"title":"Golang实现一个事务型内存数据库","uri":"/golang-in-memory-database/#"},{"categories":["code"],"content":" 特性MossDB是一个纯Golang编写、可嵌入的、键值型内存数据库，包含以下特性 可持久化，类似Redis AOF(Append only Log) 支持事务 支持近实时的TTL(Time to Live), 可以实现毫秒级的过期删除 前缀搜索 Watch接口，可以监听某个键值的内容变化，类似etcd的Watch 多后端存储，目前支持HashMap和RadixTree ","date":"Mar 01, 2023","objectID":"/golang-in-memory-database/:1:0","series":null,"tags":["golang","database","etcd"],"title":"Golang实现一个事务型内存数据库","uri":"/golang-in-memory-database/#特性"},{"categories":["code"],"content":" 命名由来Moss有苔、苔花的含义，MossDB的名字来源于清代袁牧的一句诗: 苔花如米小，也学牡丹开 MossDB虽小，但五脏俱全，也支持了很多重要功能。另外，巧合的是《流浪地球2》中的超级计算机550W名字就是Moss。 ","date":"Mar 01, 2023","objectID":"/golang-in-memory-database/:2:0","series":null,"tags":["golang","database","etcd"],"title":"Golang实现一个事务型内存数据库","uri":"/golang-in-memory-database/#命名由来"},{"categories":["code"],"content":" 架构内存数据库虽然使用简单，实现起来却有很多细节，Golang目前也存在不少优秀的开源内存数据库，比如buntdb、go-memdb，在编写MossDB过程中也借鉴了一些它们的特性。 MossDB的架构如图： 自上往下分为： 接口层，提供API接受用户请求 核心层，实现事务、过期删除、Watch等功能 存储层，提供KV的后端存储以及增删改查 持久化层，使用AOL持久化即每次修改操作都会持久化到磁盘Log中 ","date":"Mar 01, 2023","objectID":"/golang-in-memory-database/:3:0","series":null,"tags":["golang","database","etcd"],"title":"Golang实现一个事务型内存数据库","uri":"/golang-in-memory-database/#架构"},{"categories":["code"],"content":" 快速开始MossDB可嵌入到Go程序中，可以通过go get获取 go get github.com/qingwave/mossdb MossDB提供了易用的API，可以方便地进行数据处理，下面的示例代码展示了如何使用MossDB： package main import ( \"log\" \"github.com/qingwave/mossdb\" ) func main() { // create db instance db, err := mossdb.New(\u0026mossdb.Config{}) if err != nil { log.Fatal(err) } // set, get data db.Set(\"key1\", []byte(\"val1\")) log.Printf(\"get key1: %s\", db.Get(\"key1\")) // transaction db.Tx(func(tx *mossdb.Tx) error { val1 := tx.Get(\"key1\") tx.Set(\"key2\", val1) return nil }) } 更多示例见源码 ","date":"Mar 01, 2023","objectID":"/golang-in-memory-database/:3:1","series":null,"tags":["golang","database","etcd"],"title":"Golang实现一个事务型内存数据库","uri":"/golang-in-memory-database/#快速开始"},{"categories":["code"],"content":" 具体实现从下往上分别介绍下MossDB如何设计与实现，以及相关的细节。 ","date":"Mar 01, 2023","objectID":"/golang-in-memory-database/:4:0","series":null,"tags":["golang","database","etcd"],"title":"Golang实现一个事务型内存数据库","uri":"/golang-in-memory-database/#具体实现"},{"categories":["code"],"content":" AOF持久化AOF源于Redis提供两种持久化技术，另外一种是RDB，AOF是指在每次写操作后，将该操作序列化后追加到文件中，重启时重放文件中的对应操作，从而达到持久化的目的。其实现简单，用在MossDB是一个不错的选择，但需要注意的是AOF缺点同样明显，如果文件较大，每次重启会花费较多时间。 Redis的AOF是一种后写式日志，先写内存直接返回给用户，再写磁盘文件持久化，可以保证其高性能，但如果中途宕机会丢失数据。MossDB中的AOF采用了WAL(预写式日志)实现，先写Log再写内存，用来保证数据不会丢失，从而可以进一步实现事务。 那么采用WAL会不会影响其性能？每次必须等到落盘后才进行其他操作，WAL的每次写入会先写到内核缓冲区，这个调用很快就返回了，内核再将数据落盘。我们也可以使用fsync调用强制内核执行直接将数据写入磁盘。在MossDB中普通写操作之会不会直接调用fsync，事务写中强制开启fsync，从而平衡数据一致性与性能。 WAL的实现引用了tiwall/wal，其中封装了对Log文件的操作，可以支持批量写入。由于WAL是二进制的，必须将数据进行编码，通过varint编码实现，将数据长度插入到数据本体之前，读取时可以读取定长的数据长度，然后按长度读取数据本体。MossDB中数据格式如下： type Record struct { Op uint16 KeySize uint32 ValSize uint32 Timestamp uint64 TTL uint64 Key []byte Val []byte } 对应编码后的二进制格式为: |------------------------------------------------------------| | Op | KeySize | ValSize | Timestamp | TTL | Key | Val | |------------------------------------------------------------| | 2 | 4 | 4 | 8 | 8 | []byte | []byte | |------------------------------------------------------------| 使用binary.BigEndian.PutUint16进行编码，解码时通过binary.BigEndian.Uint16，从而依次取得生成完整的数据。 ","date":"Mar 01, 2023","objectID":"/golang-in-memory-database/:4:1","series":null,"tags":["golang","database","etcd"],"title":"Golang实现一个事务型内存数据库","uri":"/golang-in-memory-database/#aof持久化"},{"categories":["code"],"content":" 存储引擎MossDB提供了存储接口，只要实现了此接口都可以作为其后端存储 type Store interface { Get(key string) ([]byte, bool) Set(key string, val []byte) Delete(key string) Prefix(key string) map[string][]byte Dump() map[string][]byte Len() int } 内置提供了HashMap与RadixTree两种方式，HashMap实现简单通过简单封装map可以快速进行查询与插入，但范围搜索性能差。RadixTree即前缀树，查询插入的时间复杂度只与Key的长度相关，而且支持范围搜索，MossDB采用Adaptive Radix Tree可以避免原生的前准树空间浪费。 由于RadixTree的特性，MossDB可以方便的进行前缀搜索，目前支持List与Watch操作。 ","date":"Mar 01, 2023","objectID":"/golang-in-memory-database/:4:2","series":null,"tags":["golang","database","etcd"],"title":"Golang实现一个事务型内存数据库","uri":"/golang-in-memory-database/#存储引擎"},{"categories":["code"],"content":" 事务实现要实现事务必须要保证其ACID特性，MossDB的事务定义如下： type Tx struct { db *DB // DB实例 commits []*Record // 对于写操作生成 Record, 用来做持久化 undos []*Record // 对于写操作生成 Undo Record，用于回滚 } MossDB中一次事务的流程主要包含以下几个步骤： 首先加锁，保证其数据一致性 对于写操作，生成Commits和Undo Records，然后写入内存；读操作则直接执行 提交阶段，将Commits持久化到WAL中；若写入失败，则删除已写入数据；成功则设置数据的其他属性(TTL, Watch等) 若中间发生错误，执行回滚操作，将Undo Records的记录执行 事务完成，释放锁 ","date":"Mar 01, 2023","objectID":"/golang-in-memory-database/:4:3","series":null,"tags":["golang","database","etcd"],"title":"Golang实现一个事务型内存数据库","uri":"/golang-in-memory-database/#事务实现"},{"categories":["code"],"content":" Watch由于工作中经常使用Kubernetes，对于其Watch接口印象深刻，通过Watch来充当其事件总线，保证其声明式对象的管理。Kubernetes的Watch底层由etcd实现，MossDB也实现了类似的功能。 Watch的定义如下： type Watcher struct { mu sync.RWMutex // 锁 watchers map[string]*SubWatcher // watchId与具体Watcher直接的映射 keys map[string]containerx.Set[string] // Watch单个key ranges *art.Tree // 前缀Watch queue workqueue.WorkQueue // 工作队列，存放所有事件 stop chan struct{} // 是否中止 } 通过工作队列模式，任何写操作都会同步追加到队列中，如果存在单个key的监听者，则通过watchers map获取到对应列表，依次发送事件。对于前缀Watch，我们不可能记录此前缀的所有Key，这里借鉴了etcd，通过RadixTree保存前缀Key，当有新事件时，匹配Key所在的路径，如果有监听者，则进行事件通知。 调用Watch会返回一个Channel，用户只需要监听Channel即可 func Watch(db *mossdb.DB) { key := \"watch-key\" ctx, cancel := context.WithTimeout(context.Background(), 1000*time.Millisecond) defer cancel() // start watch key ch := db.Watch(ctx, key) log.Printf(\"start watch key %s\", key) go func() { // 模拟发送event db.Set(key, mossdb.Val(\"val1\")) db.Set(key, mossdb.Val(\"val2\")) db.Delete(key) time.Sleep(100 * time.Second) db.Set(key, mossdb.Val(\"val3\")) }() for { select { case \u003c-ctx.Done(): log.Printf(\"context done\") return case resp, ok := \u003c-ch: if !ok { log.Printf(\"watch done\") return } log.Printf(\"receive event: %s, key: %s, new val: %s\", resp.Event.Op, resp.Event.Key, resp.Event.Val) } } // Output // 2023/02/23 09:48:50 start watch key watch-key // 2023/02/23 09:34:19 receive event: MODIFY, key: watch-key, new val: val1 // 2023/02/23 09:34:19 receive event: MODIFY, key: watch-key, new val: val2 // 2023/02/23 09:34:19 receive event: DELETE, key: watch-key, new val: // 2023/02/23 09:34:19 context done } ","date":"Mar 01, 2023","objectID":"/golang-in-memory-database/:4:4","series":null,"tags":["golang","database","etcd"],"title":"Golang实现一个事务型内存数据库","uri":"/golang-in-memory-database/#watch"},{"categories":["code"],"content":" TTL过期删除再很多场景很有用，比如验证码过期、订单未支付关闭等。MossDB采用时间堆来实现精确的Key过期策略，具体原理可以参考之前的文章Golang分布式应用之定时任务，在查询操作时也会检查Key是否过期，如果过期则直接返回空数据。配合Watch操作可以精确管理数据的生命周期。 ","date":"Mar 01, 2023","objectID":"/golang-in-memory-database/:4:5","series":null,"tags":["golang","database","etcd"],"title":"Golang实现一个事务型内存数据库","uri":"/golang-in-memory-database/#ttl"},{"categories":["code"],"content":" 总结至此，MossDB的实现细节已经分析完成，支持了事务、持久化、Watch与过期删除等特性，后续可能会支持HTTP API、存储快照等功能。 所有代码见https://github.com/qingwave/mossdb，欢迎批评指正以及Star。 Explore more in https://qingwave.github.io ","date":"Mar 01, 2023","objectID":"/golang-in-memory-database/:5:0","series":null,"tags":["golang","database","etcd"],"title":"Golang实现一个事务型内存数据库","uri":"/golang-in-memory-database/#总结"},{"categories":["code"],"content":"这两年Rust火的一塌糊涂，甚至都烧到了前端，再不学习怕是要落伍了。最近翻了翻文档，写了个简单的Ping应用练练手，被所有权折腾的够呛，相比起Golang上手难度大很多，现将开发中的一些问题总结如下，所有源码见ring。 ","date":"Nov 24, 2022","objectID":"/rust-ping/:0:0","series":null,"tags":["rust"],"title":"Rust初探: 实现一个Ping","uri":"/rust-ping/#"},{"categories":["code"],"content":" 目标实现一个Ping，功能包含： 命令行解析 实现ICMP协议，pnet包中已经包含了ICMP包定义，可以使用socket2库发送 周期性发送Ping，通过多线程发送，再汇总结果 监听退出信号 ","date":"Nov 24, 2022","objectID":"/rust-ping/:1:0","series":null,"tags":["rust"],"title":"Rust初探: 实现一个Ping","uri":"/rust-ping/#目标"},{"categories":["code"],"content":" 命令行解析系统库std::env::args可以解析命令行参数，但对于一些复杂的参数使用起来比较繁琐，更推荐clap。利用clap的注解，通过结构体定义命令行参数 /// ping but with rust, rust + ping -\u003e ring #[derive(Parser, Debug, Clone)] // Parser生成clap命令行解析方法 #[command(author, version, about, long_about = None)] pub struct Args { /// Count of ping times #[arg(short, default_value_t = 4)] // short表示开启短命名，默认为第一个字母，可以指定；default_value_t设置默认值 count: u16, /// Ping packet size #[arg(short = 's', default_value_t = 64)] packet_size: usize, /// Ping ttl #[arg(short = 't', default_value_t = 64)] ttl: u32, /// Ping timeout seconds #[arg(short = 'w', default_value_t = 1)] timeout: u64, /// Ping interval duration milliseconds #[arg(short = 'i', default_value_t = 1000)] interval: u64, /// Ping destination, ip or domain #[arg(value_parser=Address::parse)] // 自定义解析 destination: Address, } clap可以方便的指定参数命名、默认值、解析方法等，运行结果如下 ➜ ring git:(main) cargo run -- -h Compiling ring v0.1.0 (/home/i551329/work/ring) Finished dev [unoptimized + debuginfo] target(s) in 1.72s Running `target/debug/ring -h` ping but with rust, rust + ping -\u003e ring Usage: ring [OPTIONS] \u003cDESTINATION\u003e Arguments: \u003cDESTINATION\u003e Ping destination, ip or domain Options: -c \u003cCOUNT\u003e Count of ping times [default: 4] -s \u003cPACKET_SIZE\u003e Ping packet size [default: 64] -t \u003cTTL\u003e Ping ttl [default: 64] -w \u003cTIMEOUT\u003e Ping timeout seconds [default: 1] -i \u003cINTERVAL\u003e Ping interval duration milliseconds [default: 1000] -h, --help Print help information -V, --version Print version information ","date":"Nov 24, 2022","objectID":"/rust-ping/:1:1","series":null,"tags":["rust"],"title":"Rust初探: 实现一个Ping","uri":"/rust-ping/#命令行解析"},{"categories":["code"],"content":" 实现Pingpnet中提供了ICMP包的定义，socket2可以将定义好的ICMP包发送给目标IP，另一种实现是通过pnet_transport::transport_channel发送原始数据包，但需要过滤结果而且权限要求较高。 首先定义ICMP包 let mut buf = vec![0; self.config.packet_size]; let mut icmp = MutableEchoRequestPacket::new(\u0026mut buf[..]).ok_or(RingError::InvalidBufferSize)?; icmp.set_icmp_type(IcmpTypes::EchoRequest); // 设置为EchoRequest类型 icmp.set_icmp_code(IcmpCodes::NoCode); icmp.set_sequence_number(self.config.sequence + seq_offset); // 序列号 icmp.set_identifier(self.config.id); icmp.set_checksum(util::checksum(icmp.packet(), 1)); // 校验函数 通过socket2发送请求 let socket = Socket::new(Domain::IPV4, Type::DGRAM, Some(Protocol::ICMPV4))?; let src = SocketAddr::new(net::IpAddr::V4(Ipv4Addr::UNSPECIFIED), 0); socket.bind(\u0026src.into())?; // 绑定源地址 socket.set_ttl(config.ttl)?; socket.set_read_timeout(Some(Duration::from_secs(config.timeout)))?; // 超时配置 socket.set_write_timeout(Some(Duration::from_secs(config.timeout)))?; // 发送 socket.send_to(icmp.packet_mut(), \u0026self.dest.into())?; 最后处理相应，转换成pnet中的EchoReplyPacket let mut mem_buf = unsafe { \u0026mut *(buf.as_mut_slice() as *mut [u8] as *mut [std::mem::MaybeUninit\u003cu8\u003e]) }; let (size, _) = self.socket.recv_from(\u0026mut mem_buf)?; // 转换成EchoReply let reply = EchoReplyPacket::new(\u0026buf).ok_or(RingError::InvalidPacket)?; 至此，一次Ping请求完成。 ","date":"Nov 24, 2022","objectID":"/rust-ping/:1:2","series":null,"tags":["rust"],"title":"Rust初探: 实现一个Ping","uri":"/rust-ping/#实现ping"},{"categories":["code"],"content":" 周期性发送Ping需要周期性的发送请求，比如秒秒请求一次，如果直接通过循环实现，一次请求卡住将影响主流程，必须通过多线程来保证固定周期的发送。 发送请求 let send = Arc::new(AtomicU64::new(0)); // 统计发送次数 let _send = send.clone(); let this = Arc::new(self.clone()); let (sx, rx) = bounded(this.config.count as usize); // channel接受线程handler thread::spawn(move || { for i in 0..this.config.count { let _this = this.clone(); sx.send(thread::spawn(move || _this.ping(i))).unwrap(); // 线程中运行ping，并将handler发送到channel中 _send.fetch_add(1, Ordering::SeqCst); // 发送一次，send加1 if i \u003c this.config.count - 1 { thread::sleep(Duration::from_millis(this.config.interval)); } } drop(sx); // 发送完成关闭channel }); thread::spawn可以快速创建线程，但需要注意所有权的转移，如果在线程内部调用方法获取变量，需要通过Arc原子引用计数 send变量用来统计发送数，原子类型，并且用Arc包裹；this是当前类的Arc克隆，会转移到线程中 第一个线程内周期性调用ping()，并且其在单独线程中运行 通过bounded来定义channel(类似Golang中的chan)，用来处理结果，发送完成关闭 处理结果 let success = Arc::new(AtomicU64::new(0)); // 定义请求成功的请求 let _success = success.clone(); let (summary_s, summary_r) = bounded(1); // channel来判断是否处理完成 thread::spawn(move || { for handle in rx.iter() { if let Some(res) = handle.join().ok() { if res.is_ok() { _success.fetch_add(1, Ordering::SeqCst); // 如果handler结果正常，success加1 } } } summary_s.send(()).unwrap(); // 处理完成 }); 第二个线程用来统计结果，channel通道取出ping线程的handler，如果返回正常则加1 处理信号 let stop = signal_notify()?; // 监听退出信号 select!( recv(stop) -\u003e sig =\u003e { if let Some(s) = sig.ok() { // 收到退出信号 println!(\"Receive signal {:?}\", s); } }, recv(summary_r) -\u003e summary =\u003e { // 任务完成 if let Some(e) = summary.err() { println!(\"Error on summary: {}\", e); } }, ); 通过select来处理信号(类似Golang中的select)，到收到退出信号或者任务完成时继续往下执行。 ","date":"Nov 24, 2022","objectID":"/rust-ping/:1:3","series":null,"tags":["rust"],"title":"Rust初探: 实现一个Ping","uri":"/rust-ping/#周期性发送"},{"categories":["code"],"content":" 信号处理Golang中可以很方便的处理信号，但在Rust中官方库没有提供类似功能，可以通过signal_hook与crossbeam_channel实现监听退出信号 fn signal_notify() -\u003e std::io::Result\u003cReceiver\u003ci32\u003e\u003e { let (s, r) = bounded(1); // 定义channel，用来异步接受退出信号 let mut signals = signal_hook::iterator::Signals::new(\u0026[SIGINT, SIGTERM])?; // 创建信号 thread::spawn(move || { for signal in signals.forever() { // 如果结果到信号发送到channel中 s.send(signal).unwrap(); break; } }); Ok(r) // 返回接受channel } ","date":"Nov 24, 2022","objectID":"/rust-ping/:1:4","series":null,"tags":["rust"],"title":"Rust初探: 实现一个Ping","uri":"/rust-ping/#信号处理"},{"categories":["code"],"content":" 其他很多吐槽人Golang的错误处理，Rust也不遑多让，不过提供了?语法糖，也可以配合anyhow与thiserror来简化错误处理。 ","date":"Nov 24, 2022","objectID":"/rust-ping/:1:5","series":null,"tags":["rust"],"title":"Rust初探: 实现一个Ping","uri":"/rust-ping/#其他"},{"categories":["code"],"content":" 验证Ping域名/IP ring git:(main) cargo run -- www.baidu.com PING www.baidu.com(103.235.46.40) 64 bytes from 103.235.46.40: icmp_seq=1 ttl=64 time=255.85ms 64 bytes from 103.235.46.40: icmp_seq=2 ttl=64 time=254.17ms 64 bytes from 103.235.46.40: icmp_seq=3 ttl=64 time=255.41ms 64 bytes from 103.235.46.40: icmp_seq=4 ttl=64 time=256.50ms --- www.baidu.com ping statistics --- 4 packets transmitted, 4 received, 0% packet loss, time 3257.921ms 测试退出信息，运行中通过Ctrl+C中止 cargo run 8.8.8.8 -c 10 PING 8.8.8.8(8.8.8.8) 64 bytes from 8.8.8.8: icmp_seq=1 ttl=64 time=4.32ms 64 bytes from 8.8.8.8: icmp_seq=2 ttl=64 time=3.02ms 64 bytes from 8.8.8.8: icmp_seq=3 ttl=64 time=3.24ms ^CReceive signal 2 --- 8.8.8.8 ping statistics --- 3 packets transmitted, 3 received, 0% packet loss, time 2365.104ms ","date":"Nov 24, 2022","objectID":"/rust-ping/:2:0","series":null,"tags":["rust"],"title":"Rust初探: 实现一个Ping","uri":"/rust-ping/#验证"},{"categories":["code"],"content":" 总结Rust为了安全高效，通过引入所有权来解决GC问题，也带来了许多不便，编程时必须要考虑到变量的声明周期、借用等问题，所有语言都是在方便、性能、安全之间做权衡，要么程序员不方便，要么编译器多做点功。换一个角度来说Bug总是不可避免的，在编译阶段出现总好过运行阶段。 所有源码见: https://github.com/qingwave/ring Explore more in https://qingwave.github.io ","date":"Nov 24, 2022","objectID":"/rust-ping/:3:0","series":null,"tags":["rust"],"title":"Rust初探: 实现一个Ping","uri":"/rust-ping/#总结"},{"categories":["code"],"content":"对于高可用的服务，为了保证服务可用性，更新配置时必然不能直接停止服务，可以使用配置热加载来避免服务暂停，不需要重启服务。 配置的热加载可以分为两个场景，手动更新与自动更新。 ","date":"Sep 30, 2022","objectID":"/config-reload/:0:0","series":null,"tags":["golang","etcd","linux","分布式"],"title":"如何实现零宕机的配置热加载","uri":"/config-reload/#"},{"categories":["code"],"content":" 手动更新对于一些临时调试，服务数量不多的情况下，可以进行手动更新配置。需要实现两点，如何触发更新，以及接受到更新后如何操作。 触发更新的手段很多，常见的有 通过命令行，例如nginx -s reload 通过信号，通常是SIGHUP，比如sshd、Prometheus等，其实Nginx的热加载内部也是调用SIGHUP信号 HTTP接口，例如Prometheus也支持HTTP的方式通过curl -X POST :9090/-/reload可以重新加载配置 RPC接口，类似HTTP 接受到配置更新通知后，需要程序内部来重新加载配置，类似初始化过程，但要注意运行时可以要加锁来保证线程安全。 ","date":"Sep 30, 2022","objectID":"/config-reload/:1:0","series":null,"tags":["golang","etcd","linux","分布式"],"title":"如何实现零宕机的配置热加载","uri":"/config-reload/#手动更新"},{"categories":["code"],"content":" 自动更新自动更新是建立手动更新的基础上，首先服务要提供手动更新的方法，其次可以通过服务本身或者外部进程来自动调用配置更新接口，外部程序可以使用SideCar的形式与服务绑定。 自动加载配置的关键是如何感知配置变化，要考虑到单机环境与分布式环境。 ","date":"Sep 30, 2022","objectID":"/config-reload/:2:0","series":null,"tags":["golang","etcd","linux","分布式"],"title":"如何实现零宕机的配置热加载","uri":"/config-reload/#自动更新"},{"categories":["code"],"content":" 单机环境Linux提供了inotify接口，可以用来监听文件或者目录的增上改查事件。我们可以使用inotify来监听配置变化，如果有更新则调用更新接口来实现热加载。其他平台也提供了类似的接口。 在Golang中fsnotify提供了跨平台的文件监听接口，可以方便的监听文件，使用方式如下： watcher, _ := fsnotify.NewWatcher() defer watcher.Close() // 监听目录或者文件 watcher.Add(\"/tmp\") go func() { for { // 获取监听事件 select { case event, ok := \u003c-watcher.Events: if !ok { return } log.Println(\"event:\", event) if event.Has(fsnotify.Write) { log.Println(\"modified file:\", event.Name) // 进行更新操作 } case err, ok := \u003c-watcher.Errors: if !ok { return } log.Println(\"error:\", err) } } }() ","date":"Sep 30, 2022","objectID":"/config-reload/:2:1","series":null,"tags":["golang","etcd","linux","分布式"],"title":"如何实现零宕机的配置热加载","uri":"/config-reload/#单机环境"},{"categories":["code"],"content":" 分布式环境在分布式环境中实现配置热更新，需要能够感知配置（本地或者远端），对于本地配置需要平台配合将远端配置同步到本地（比如kubernetes会同步ConfigMap到Pod中），然后按照单机环境的方式来监听文件变化。 对于远端配置，需要依赖额外的分布式配置中心，比如Apollo、etcd、ZooKeeper等。以etcd为例，etcd提供了watch接口，可以监听对应配置的变化 // 获取watch Channel ch := client.Watch(d.watchContext, d.Prefix, clientv3.WithPrefix()) // 处理事件 for { select { case wr, ok := \u003c-ch: if !ok { return fmt.Errorf(\"watch closed\") } if wr.Err() != nil { return wr.Err() } for _, ev := range wr.Events { key, val := string(ev.Kv.Key), string(ev.Kv.Value) switch ev.Type { case mvccpb.PUT: // 更新处理逻辑 // 1. 对比配置是否变化 // 2. 变化了更新内存中的配置 case mvccpb.DELETE: // 删除处理逻辑 } } } } 为了实现配置更新通知，通常有两种方式，Pull与Push。 Pull就是客户端轮询，定期查询配置是否更新，这种方式实现简单，对服务器压力小，但时效性低 Push由服务端实现，通过维护一个长连接，实时推送数据，这种方式时效性高，但逻辑更复杂，连接过多会影响服务端性能。目前etcd v3版本是通过HTTP2来实现实时数据推送 ","date":"Sep 30, 2022","objectID":"/config-reload/:2:2","series":null,"tags":["golang","etcd","linux","分布式"],"title":"如何实现零宕机的配置热加载","uri":"/config-reload/#分布式环境"},{"categories":["code"],"content":" 总结本文主要总结实现配置热更新的多种方式，手动更新可以通过Socket、信号等进程间通信手段来通知服务，自动更新可以通过inotify来感知配置变化，在分布式环境中就需要配合分布式配置中心来进行热更新。 Explore more in https://qingwave.github.io ","date":"Sep 30, 2022","objectID":"/config-reload/:3:0","series":null,"tags":["golang","etcd","linux","分布式"],"title":"如何实现零宕机的配置热加载","uri":"/config-reload/#总结"},{"categories":["code"],"content":"借助一些设计模式、流式编程、函数编程的方法可以让我们的Golang代码更清晰优雅，本文中描述了在错误处理、可选配置、并发控制等方面的优化手段。 ","date":"Sep 12, 2022","objectID":"/golang-programming-pattern/:0:0","series":null,"tags":["golang"],"title":"Golang优雅之道","uri":"/golang-programming-pattern/#"},{"categories":["code"],"content":" 链式错误处理很多人不喜欢Go的错误处理，需要写大量if err != nil的代码，特别是在一些复杂步骤场景中，每一步都要判断结果是否出错。在这种情况中，可以通过类似链式调用将错误封装在其中。 比如在对象中附带一个error属性，在每一步调用中如果error不为空直接返回 type Handler struct { props interface err error } func (h *Handler) Err() error { return h.err } func (h *Handler) Step1() *Handler { if h.err != nil { return h } // do something for step2 return h } func (h *Handler) Step2() *Handler { if h.err != nil { return h } // do something fot step2 return h } // ... StepN() 调用时直接通过链式调用即可，最后再判断错误 h := \u0026Handler{} if err := h.Step1().Step2().StepN().Err(); err != nil { // handle error } 这种方式在一些数据库包中有大量使用，比如etcd、gorm。 ","date":"Sep 12, 2022","objectID":"/golang-programming-pattern/:1:0","series":null,"tags":["golang"],"title":"Golang优雅之道","uri":"/golang-programming-pattern/#链式错误处理"},{"categories":["code"],"content":" 可选配置在创建对象时，如果可配置的属性很多，通常会引入一个配置文件 type Config struct { Port string Host string Timeout time.Time // ... } func NewServer(conf *Config) *Server { } 通常这些配置都有默认值，config也不是必须的，通过建造者模式可以轻松解决此类问题 builder := \u0026Builder{} server := builder.WithPort(\"8080\").WithHost(\"0.0.0.0\").WithTimeOut(10*time.Second).Complete() 但建造者需要写一个建造类，配置对应的属性设置方法 type Builder struct { server Server } func (b *Builder) WithPort(port string) *Builder { b.server.port = port return b } func (b *Builder) WithHost(host string) *Builder { b.server.host = host return b } 除了建造者模式，还可以通过可选配置，对调用者更友好，将配置项封装成Option，需要的时候注入对应的Option即可 type Option func(*Server) type WithPort(port int) Option { return func(s *Server) { s.port = port } } type WithHost(host int) Option { return func(s *Server) { s.host = host } } type NewServer(opts ...Option) *Server { s := defaultServer() // 默认配置 for _, opt := range opts { opt(s) // 添加可选配置 } return s } 调用时，只需在NewServer配置对应的Option即可 // 默认配置 s := NewServer() // 可选配置 s := NewServer(WithPort(\"8080\"), WithHost(\"127.0.0.1\")) 可选配置相比直接使用配置和建造者模式，更加清晰，也非常容易扩展和维护，在kuberentes、etcd库中都有非常多的应用。 ","date":"Sep 12, 2022","objectID":"/golang-programming-pattern/:2:0","series":null,"tags":["golang"],"title":"Golang优雅之道","uri":"/golang-programming-pattern/#可选配置"},{"categories":["code"],"content":" 并发控制Golang基础库中已经提供不少并发控制工具，比如Channel、WaitGroup、各种锁等等。 ","date":"Sep 12, 2022","objectID":"/golang-programming-pattern/:3:0","series":null,"tags":["golang"],"title":"Golang优雅之道","uri":"/golang-programming-pattern/#并发控制"},{"categories":["code"],"content":" ErrGroupWaitGroup可以等待多个Goroutine执行结束，但很多时候并发执行多个任务，如果其中一个任务出错那么整体失败，需要直接返回，这种情况下我们可以使用ErrGroup ErrGroup借助封装了WaitGroup、Once以及Context，调用Wait时如果一个任务失败取消Context直接返回，核心逻辑如下 type ErrGroup struct { ctx context.Context cancel func() wg sync.WaitGroup errOnce sync.Once err error } func (g *ErrGroup) Wait() error { g.wg.Wait() if g.cancel != nil { g.cancel() } return g.err } func (g *ErrGroup) Go(f func(ctx context.Context) error) { g.wg.Add(1) go func() { defer g.wg.Done() if err := f(g.ctx); err != nil { // 执行失败则运行cancel g.errOnce.Do(func() { g.err = err if g.cancel != nil { g.cancel() } }) } }() } ","date":"Sep 12, 2022","objectID":"/golang-programming-pattern/:3:1","series":null,"tags":["golang"],"title":"Golang优雅之道","uri":"/golang-programming-pattern/#errgroup"},{"categories":["code"],"content":" 控制并发数借助有缓冲的Channel，可以实现控制Goroutine并发数，逻辑如下： func NewCtrlGroup(number int) *CtrlGroup { return \u0026CtrlGroup{ ch: make(chan struct{}, number), } } type CtrlGroup struct { ch chan struct{} wg sync.WaitGroup } func (g *CtrlGroup) Enter() { g.ch \u003c- struct{}{} } func (g *CtrlGroup) Leave() { \u003c-g.ch } func (g *CtrlGroup) Go(f func()) { g.Enter() // 接收到新任务，发送到Channel，如果Channel满需要等待 g.wg.Add(1) go func() { defer g.Leave() // 任务结束，取出一个元素 defer g.wg.Done() f() }() } func (g *CtrlGroup) Wait() { g.wg.Wait() } ","date":"Sep 12, 2022","objectID":"/golang-programming-pattern/:3:2","series":null,"tags":["golang"],"title":"Golang优雅之道","uri":"/golang-programming-pattern/#控制并发数"},{"categories":["code"],"content":" MapReduce除了WaitGroup、ErrGroup处理一些简单的并发任务，有时候我们需要执行类似MapReduce的操作，通过Map对数据源并行处理，然后通过Reduce合并结果。在Java、Python中提供了类似功能。 比如实现一个实现一组数据的平方和，利用MapReduce在Golang中实现如下： num := 1000000 res, err := mapreduce.New(mapreduce.WithWorkers(16)). From(func(r mapreduce.Writer) error { // 产生数据源 for i := 1; i \u003c num; i++ { r.Write(i) } return nil }). Map(func(item any) (any, error) { // 处理数据 v, ok := item.(int) if !ok { return nil, fmt.Errorf(\"invaild type\") } resp := v * v return resp, nil }). Reduce(func(r mapreduce.Reader) (any, error) { // 合并结果 sum := 0 for { item, ok := r.Read() if !ok { break } v, ok := item.(int) if !ok { return nil, fmt.Errorf(\"invaild type\") } sum += v } return sum, nil }). Do() 主要逻辑是利用Channel（或者线程安全的队列）将源数据发送到Map的执行Worker中，处理完后再转发到Reduce Goroutine中，通过ErrGroup等待所有Worker执行完成。源码见mapreduce.go。 类似的也可以实现Kubernetes中Controller模式，通过队列或者Channel将生产者与消费者解耦，并行处理提高运行速度。 ","date":"Sep 12, 2022","objectID":"/golang-programming-pattern/:3:3","series":null,"tags":["golang"],"title":"Golang优雅之道","uri":"/golang-programming-pattern/#mapreduce"},{"categories":["code"],"content":" 总结本文总结了Golang的一些有趣的编程模式，例如链式调用、可选配置、并发控制等，通过这些技巧或者手段，可以提高编码的质量，所有代码见gocorex。 Explore more in https://qingwave.github.io ","date":"Sep 12, 2022","objectID":"/golang-programming-pattern/:4:0","series":null,"tags":["golang"],"title":"Golang优雅之道","uri":"/golang-programming-pattern/#总结"},{"categories":["code","前端"],"content":"婚礼将近，作为一个有能耐好折腾的程序员怎么能不趁机展示下，着手开发个婚礼邀请函微信小程序。 ","date":"Sep 06, 2022","objectID":"/wedding-invitation/:0:0","series":null,"tags":["小程序"],"title":"程序员的浪漫: 婚礼邀请函小程序","uri":"/wedding-invitation/#"},{"categories":["code","前端"],"content":" 总体设计选用微信小程序，传播方便，相对公众号定制性也更强。原本打算Github找一个改改，无奈不是太繁杂、就是审美不过关，还是自己开头开始吧。 主要功能： 长页展示，不花里胡哨 照片展示，需要各种排版，避免单调 婚礼信息展示，日历、地点等 背景音乐，没有音乐就好比吃面不就蒜，总是少点味道 支持转发、分享 其它锦上添花的功能，比如点赞，评论，需要有数据库的支持，看自己需求了。 小程序地址： 效果如下： ","date":"Sep 06, 2022","objectID":"/wedding-invitation/:1:0","series":null,"tags":["小程序"],"title":"程序员的浪漫: 婚礼邀请函小程序","uri":"/wedding-invitation/#总体设计"},{"categories":["code","前端"],"content":" 开发过程首先是大体过下开发文档，熟悉前端的应该都比较好上手，一些用法和Vue比较相似，就是容易写混，经常把wx:if写成v-if之类的。 ","date":"Sep 06, 2022","objectID":"/wedding-invitation/:2:0","series":null,"tags":["小程序"],"title":"程序员的浪漫: 婚礼邀请函小程序","uri":"/wedding-invitation/#开发过程"},{"categories":["code","前端"],"content":" 背景音乐通过BackgroundAudioManager实现背景音乐，暂停、续播都比较方便。 获取实例后，设置对应的标题、音乐链接即可直接播放 const bgm = wx.getBackgroundAudioManager(); bgm.title = conf.BASE.bgmName; bgm.coverImgUrl = conf.BASE.share; bgm.src = conf.BASE.bgm; 暂停与播放可以绑定到对应的音乐图标上了，点击切换，主要逻辑如下： var t = this; bgm.onStop(function () { t.setData({ playing: false }); }) bgm.onEnded(function () { t.setData({ playing: false }); }) bgm.onPause(function () { t.setData({ playing: false }); }) bgm.onPlay(function () { t.setData({ playing: true }); }) 将音乐图标与事件绑定，当播放时展示rotate动画，暂停时停止动画animation-play-state: paused \u003cimage class=\"player-img {{playing ? '': 'player-stop'}}\" lazyLoad=\"false\" mode=\"aspectFit\" src=\"{{static}}\"\u003e\u003c/image\u003e ","date":"Sep 06, 2022","objectID":"/wedding-invitation/:2:1","series":null,"tags":["小程序"],"title":"程序员的浪漫: 婚礼邀请函小程序","uri":"/wedding-invitation/#背景音乐"},{"categories":["code","前端"],"content":" 图片展示图片主要是要考虑到各种排版，避免审美疲劳，可以参考一些婚礼应用的排版设计，比如婚礼乎、婚礼纪之类的，这里大量参考了小程序我的婚礼邀请的设计。 圆形图片 展示新郎新娘名称时可以用到，通过设置border-radius: 50%;来实现 排版 横版照片可以直接填充，竖版照片填充过大，可以一行两张或三张，如果直接对齐太严肃，可以通过margin-top来设置落差，下面设置为三等分的图片设置 .triple-img { border-radius: 10rpx; height: 300rpx; width: 30%; } .img-1 { margin-top: -100rpx; } .img-2: { margin-top: 0; } .img-3 { margin-top: 100rpx; } 设置相框 { border: 6rpx solid #cbd5e1; } 照片周围装饰线，可通过伪元素设置 img::before { border: 4rpx solid #cbd5e1; border-bottom: none; border-right: none; } 然后就是组合这些排列，添加对应的文字标题 图片预览 微信提供了图片预览的API，可以直接使用，将方法绑定到对应图片或图片组上 function viewImg() { wx.previewImage({ urls: imgs, // 预览的图片列表 current: src, // 初始预览的图片url success: function (res) { }, fail: function (res) { }, complete: function (res) { }, }) } 图片开发可以先使用本地图片，开发完成后可以将图片压缩后（我使用的是图压）上传到对象存储或者云开发的存储中。 ","date":"Sep 06, 2022","objectID":"/wedding-invitation/:2:2","series":null,"tags":["小程序"],"title":"程序员的浪漫: 婚礼邀请函小程序","uri":"/wedding-invitation/#图片展示"},{"categories":["code","前端"],"content":" 地图展示小程序提供了原生组件map，在腾讯地图上选取所在酒店的经纬度，填充到markers中 \u003cmap bindtap=\"openMap\" style=\"width: 100%; height: 400rpx;\" data-info=\"{{item}}\" enablePoi=\"true\" scale=\"16\" enableRotate=\"true\" latitude=\"{{item.latitude}}\" longitude=\"{{item.longitude}}\" markers=\"{{item.markers}}\"\u003e\u003c/map\u003e 其中openMap用来打开地图 function(e) { let info = e.target.dataset.info; wx.openLocation({ // 填充对应的信息 name: info.address, address: info.address, latitude: info.latitude, longitude: info.longitude, fail: function(res) { console.log(\"failed to open location\", res) } }); } ","date":"Sep 06, 2022","objectID":"/wedding-invitation/:2:3","series":null,"tags":["小程序"],"title":"程序员的浪漫: 婚礼邀请函小程序","uri":"/wedding-invitation/#地图展示"},{"categories":["code","前端"],"content":" 锦上添花通过上面的步骤已经完成了邀请函，如果需要添加一些交互功能，就需要使用到服务器，或者直接使用云开发更简单点。 点赞实现 点赞很简单，数据库中设置一个likes字段，当用户点击时加1，如果点赞过再点击减1，可以通过云开发提供的原子操作实现 function () { var num = 0 var likes = this.data.likes if (!this.data.liked) { num = 1 // 未点赞，加1 likes++ } else { num = -1 // 已点赞，减1 likes-- } const _ = this.data.db.command this.setData({ liked: !this.data.liked, likes: likes }) this.data.db.collection('wedding').doc('config').update({ data: { likes: _.inc(num) // 原子操作，更新点赞值 }, fail: function (err) { console.log(\"set failed\", err) } }) } 如果需要记录点赞的用户，首先需要用户登录，相对不太友好，点赞后可以记录用户OpenID到对应表。 发送通知 首先要申请消息模板，在小程序管理界面可申请，记录模板id和内容key值。 这里通过云函数实现发送婚礼邀请的通知，只是当用户点击时，实时出发。如果需要延时触发（比如婚礼一天前提醒），则需要服务器支持，通过延时任务或者定期轮询来实现。 云函数实现通知 const cloud = require('wx-server-sdk') cloud.init() // 云函数入口函数 exports.main = async (event, context) =\u003e { const wxContext = cloud.getWXContext() const result = await cloud.openapi.subscribeMessage.send({ touser: wxContext.OPENID, // 获取用户id page: event.page, data: event.data, // 添加对应的数据，值要与模板中的对应 templateId: event.templateId // 模板id }) return result } 在小程序中调用 function(e) { if (!conf.BASE.cloudEnable) { return } let info = e.target.dataset.info; wx.requestSubscribeMessage({ tmplIds: [conf.BASE.msgId], success: function(res) { wx.cloud.callFunction({ name: \"sendMsg\", // 云函数名称 data: { page: indexPage, templateId: conf.BASE.msgId, // 模板id data: { // 对应数据 \"time2\": { \"value\": `${info.year}年${info.month}月${info.day}日 12:00` }, \"thing5\": { \"value\": `${conf.BASE.msgTitle}` }, \"thing6\": { \"value\": `${info.city}${info.address}` }, \"thing7\": { \"value\": info.room } } } }) } }) } ","date":"Sep 06, 2022","objectID":"/wedding-invitation/:2:4","series":null,"tags":["小程序"],"title":"程序员的浪漫: 婚礼邀请函小程序","uri":"/wedding-invitation/#锦上添花"},{"categories":["code","前端"],"content":" 一些坑 小程序的双向绑定，必须通过this.setData来设置，否则页面不会更新 部分功能在IOS与安卓上表现不一致，需要真机测试下 云开发的权限问题，会造成小程序的操作失败 分享到朋友圈中的小程序，直接打开会进入到单页模式，一些功能会受限比如更新云数据库，需要配置云开发权限设置 ","date":"Sep 06, 2022","objectID":"/wedding-invitation/:3:0","series":null,"tags":["小程序"],"title":"程序员的浪漫: 婚礼邀请函小程序","uri":"/wedding-invitation/#一些坑"},{"categories":["code","前端"],"content":" 后记前前后后小一周时间，算是搞定了，效果也符合预期。不过终究怎么展现只是个形式，内容更重要。 Explore more in https://qingwave.github.io ","date":"Sep 06, 2022","objectID":"/wedding-invitation/:4:0","series":null,"tags":["小程序"],"title":"程序员的浪漫: 婚礼邀请函小程序","uri":"/wedding-invitation/#后记"},{"categories":["code"],"content":"ZooKeeper是Apache下一个开源项目，提供分布式配置、同步服务以及命名注册等，是一个高可靠的分布式协调系统。 其应用场景与etcd类似，可以使用在 服务发现 分布式锁 选主 分布式队列 分布式系统协调 负载均衡 如在Hadooop、Kafka中将ZooKeeper作为核心组件。本文结合Golang来编写对应的中间件，所有代码见https://github.com/qingwave/gocorex ","date":"Aug 08, 2022","objectID":"/golang-distributed-system-x-zk/:0:0","series":null,"tags":["zk","golang","分布式"],"title":"Golang分布式应用之ZooKeeper","uri":"/golang-distributed-system-x-zk/#"},{"categories":["code"],"content":" 服务注册服务注册主要细节在etcd中已提及，主要来解决分布式环境中服务注册注销与状态感知，包括： 服务注册、注销 服务宕机或异常时，自动注销 感知服务端点变化 借助zk实现服务发现: 可以通过将端点写同一个目录(相同前缀，如/services/job/endpoint1, /services/job/endpoint2)，写入临时节点，如果服务宕机，Session过期对应端点会自动删除 通过Watch API可以监听端点变化 核心代码如下： // 注册，1表示临时节点 func (d *ZkDiscovery) Register(ctx context.Context) error { _, err := d.conn.Create(d.myKey, []byte(d.Val), 1, d.ACL) if err == zk.ErrNodeExists { return nil } return err } // 注销，直接删除对应Key即可 func (d *ZkDiscovery) UnRegister(ctx context.Context) error { err := d.conn.Delete(d.myKey, -1) if err == zk.ErrNoNode { return nil } return err } 服务监听通过zk Watch接口 func (d *ZkDiscovery) Watch(ctx context.Context) error { d.watchContext, d.watchCancel = context.WithCancel(ctx) // 获取最新列表 if err := d.refreshServices(); err != nil { return err } if d.Callbacks.OnStartedDiscovering != nil { d.Callbacks.OnStartedDiscovering(d.ListServices()) } defer d.watchCancel() defer func() { if d.Callbacks.OnStoppedDiscovering != nil { d.Callbacks.OnStoppedDiscovering() } }() loop: // 添加节点变化 children, _, ch, err := d.conn.ChildrenW(d.Path) if err != nil { return err } d.setServices(containerx.NewSet(children...)) for { select { case \u003c-d.watchContext.Done(): return nil case e, ok := \u003c-ch: // zk 是一个一次性触发器，收到事件后需要重新watch if !ok { goto loop } if e.Err != nil { return e.Err } // 当子节点变化时，获取最新服务列表 switch e.Type { case zk.EventNodeCreated, zk.EventNodeChildrenChanged: d.refreshServices() } switch e.State { case zk.StateExpired: return fmt.Errorf(\"node [%s] expired\", d.myKey) case zk.StateDisconnected: return nil } if d.Callbacks.OnServiceChanged != nil { d.Callbacks.OnServiceChanged(d.ListServices()) } } } } 通过worker模拟不同的端点，测试代码如下： func main() { ctx, cancel := context.WithCancel(context.Background()) worker := func(i int, run bool) { id := fmt.Sprintf(\"10.0.0.%d\", i) val := fmt.Sprintf(\"10.0.0.%d\", i) sd, err := zkdiscovery.New(zkdiscovery.ZkDiscoveryConfig{ Endpoints: []string{\"127.0.0.1\"}, Path: \"/zk/services\", SessionTimeout: 2 * time.Second, Key: id, Val: val, Callbacks: zkdiscovery.DiscoveryCallbacks{ OnStartedDiscovering: func(services []zkdiscovery.Service) { log.Printf(\"[%s] onstarted, services: %v\", id, services) }, OnStoppedDiscovering: func() { log.Printf(\"[%s] onstoped\", id) }, OnServiceChanged: func(services []zkdiscovery.Service) { log.Printf(\"[%s] onchanged, services: %v\", id, services) }, }, }) if err != nil { log.Fatalf(\"failed to create service discovery: %v\", err) } defer sd.Close() if !run { if sd.UnRegister(context.Background()); err != nil { log.Fatalf(\"failed to unregister service [%s]: %v\", id, err) } return } if err := sd.Register(context.Background()); err != nil { log.Fatalf(\"failed to register service [%s]: %v\", id, err) } if err := sd.Watch(ctx); err != nil { log.Printf(\"[%s] failed to watch service: %v\", id, err) } } wg := group.NewGroup() for i := 0; i \u003c 3; i++ { id := i wg.Go(func() { worker(id, true) }) } go func() { time.Sleep(2 * time.Second) worker(3, true) }() // unregister go func() { time.Sleep(4 * time.Second) worker(1, false) }() // wg.Wait() time.Sleep(5 * time.Second) cancel() time.Sleep(1 * time.Second) } 通过结果可以看到服务能够正常注册注销，而且可以监听到节点变化 2022/08/09 03:01:29 connected to 127.0.0.1:2181 2022/08/09 03:01:29 connected to 127.0.0.1:2181 2022/08/09 03:01:29 connected to 127.0.0.1:2181 2022/08/09 03:01:29 authenticated: id=72787622169739423, timeout=4000 2022/08/09 03:01:29 re-submitting `0` credentials after reconnect 2022/08/09 03:01:29 authenticated: id=72787622169739424, timeout=4000 2022/08/09 03:01:29 authenticated: id=72787622169739425, timeout=4000 2022/08/09 03:01:29 re-submitting `0` credentials after reconnect 2022/08/09 03:01:29 re-submitting `0` credentials after reconnect 2022/08/09 03:01:29 [10.0.0.2] onstarted, services: [{10.0.0.1 } {10.0.0.0 } {10.0.0.2 }] 2022/08/09 03:01:29 [10.0.0.0] onstarted, services: [{10.0.0.0 } {10.0.0.2 } {10.0.0.1 }] 2022/08/09 03:01:29 [10.0.0.1] onstarted, services: [{10.0.0.0 } {10.0.0.2 } {10.0.0.1 }] 2022/08/09 03:01:31 connected to 127.0.0.1:2181","date":"Aug 08, 2022","objectID":"/golang-distributed-system-x-zk/:1:0","series":null,"tags":["zk","golang","分布式"],"title":"Golang分布式应用之ZooKeeper","uri":"/golang-distributed-system-x-zk/#服务注册"},{"categories":["code"],"content":" 分布式锁在包github.com/go-zookeeper/zk中已经实现了分布式锁，主要借助了ZooKeeper的临时节点的功能 加锁时，创建临时节点（client与zk server会保持长链接，链接中断则创建的临时数据会被删除） 解锁时，直接删除节点即可 主要来看加锁过程 func (l *Lock) LockWithData(data []byte) error { if l.lockPath != \"\" { return ErrDeadlock } prefix := fmt.Sprintf(\"%s/lock-\", l.path) path := \"\" var err error // 重试3次 for i := 0; i \u003c 3; i++ { // 创建临时顺序节点，同名节点会加序列号 path, err = l.c.CreateProtectedEphemeralSequential(prefix, data, l.acl) if err == ErrNoNode { // Create parent node. parts := strings.Split(l.path, \"/\") pth := \"\" for _, p := range parts[1:] { var exists bool pth += \"/\" + p // 父路径不存在，创建父节点 exists, _, err = l.c.Exists(pth) if err != nil { return err } if exists == true { continue } _, err = l.c.Create(pth, []byte{}, 0, l.acl) if err != nil \u0026\u0026 err != ErrNodeExists { return err } } } else if err == nil { break } else { return err } } if err != nil { return err } // 解析序列号 seq, err := parseSeq(path) if err != nil { return err } // 获取lock下所有子节点，根据序列号判断是否获得锁 for { children, _, err := l.c.Children(l.path) if err != nil { return err } lowestSeq := seq prevSeq := -1 prevSeqPath := \"\" for _, p := range children { s, err := parseSeq(p) if err != nil { return err } if s \u003c lowestSeq { lowestSeq = s } // 获取此节点前一个序列号 if s \u003c seq \u0026\u0026 s \u003e prevSeq { prevSeq = s prevSeqPath = p } } // 如果当前节点序列号最低，则获取到锁 if seq == lowestSeq { // Acquired the lock break } // 否则等待节点删除 _, _, ch, err := l.c.GetW(l.path + \"/\" + prevSeqPath) if err != nil \u0026\u0026 err != ErrNoNode { return err } else if err != nil \u0026\u0026 err == ErrNoNode { // try again continue } ev := \u003c-ch if ev.Err != nil { return ev.Err } } l.seq = seq l.lockPath = path return nil } 主要逻辑如下： 创建临时顺序节点 如果父节点不存在，则创建父节点 获取lock下所有子节点序列号 如果当前节点序列号最小，则获得锁 否则，等待前一个删除，直到获取锁 对比etcd的实现，大体思路基本一致，主要差异点在于 TTL实现：etcd通过Lease的实现TTL，获取锁后不断刷新Lease; zk通过Session来实现TTL，Session中止会自动清楚临时节点 顺序获取锁：etcd通过Revision来实现；zk则通过临时顺序节点 ","date":"Aug 08, 2022","objectID":"/golang-distributed-system-x-zk/:2:0","series":null,"tags":["zk","golang","分布式"],"title":"Golang分布式应用之ZooKeeper","uri":"/golang-distributed-system-x-zk/#分布式锁"},{"categories":["code"],"content":" 对比etcdZooKeeper与etcd的使用场景高度重合，可以项目替代，主要区别有以下几点 对比项 ZooKeeper etcd 一致性协议 zab raft 健康检查 基于Session 心跳，Lease刷新 Watch 一次性触发器、只能添加子节点创建、删除，事件不包含数据 可以添加前缀、Range、子节点变化 多版本控制 不支持 支持，所有Key含有Revision etcd作为后期之秀，在功能上更丰富，新项目可以优先尝试使用etcd作为其分布式协调引擎。 ","date":"Aug 08, 2022","objectID":"/golang-distributed-system-x-zk/:3:0","series":null,"tags":["zk","golang","分布式"],"title":"Golang分布式应用之ZooKeeper","uri":"/golang-distributed-system-x-zk/#对比etcd"},{"categories":["code"],"content":" 总结本文分析了ZooKeeper在分布式锁、服务发现等场景上的实现方式，并对比了与etcd的差异点。 本文所有代码见https://github.com/qingwave/gocorex，欢迎批评指正。 Explore more in https://qingwave.github.io ","date":"Aug 08, 2022","objectID":"/golang-distributed-system-x-zk/:4:0","series":null,"tags":["zk","golang","分布式"],"title":"Golang分布式应用之ZooKeeper","uri":"/golang-distributed-system-x-zk/#总结"},{"categories":["code"],"content":"etcd是一个可靠的分布式KV存储数据库，由CoreOS开源。Kuberentes使用etcd作为其存储引擎，随着云原生的火热，etcd也逐渐广泛应用起来。 etcd除了作为普通的KV存储、配置存储，还可以用在以下分布式场景中： 服务发现 分布式锁 选主 分布式队列 分布式系统协调 负载均衡 本文结合Golang来编写对应的中间件，所有代码见https://github.com/qingwave/gocorex ","date":"Aug 07, 2022","objectID":"/golang-distributed-system-x-etcd/:0:0","series":null,"tags":["etcd","golang","分布式"],"title":"Golang分布式应用之etcd","uri":"/golang-distributed-system-x-etcd/#"},{"categories":["code"],"content":" 服务发现在分布式系统中，如何能找到所需要访问的服务即服务发现。服务较少时可以直接访问其IP，但随着业务规模的扩大，维护其地址越来越复杂，如果服务频繁的扩缩容，必须能够实时感应服务的断点变化。 通常有多种方式可以解决 系统级别，如LVS、DNS、Kubernetes中的Service、Istio等 微服务注册中心，如Spring Cloud中的Enruka，Dubbo等 借助分布式协调系统etcd、ZK、Consul等 服务发现提供的功能包括： 服务注册、注销 服务宕机或异常时，自动注销 感知服务端点变化 借助etcd实现服务发现 可以通过将端点写同一个目录(相同前缀，如/services/job/endpoint1, /services/job/endpoint2)，并通过Lease设置一个过期时间，不断刷新Lease，如果服务宕机，Lease过期对应端点会自动删除 通过Watch API可以监听端点变化 主要代码如下 func New(config EtcdDiscoveryConfig) (*EtcdDiscovery, error) { // 创建session，session会自动续约 session, err := concurrency.NewSession(config.Client, concurrency.WithTTL(config.TTLSeconds)) if err != nil { return nil, err } config.Prefix = strings.TrimSuffix(config.Prefix, \"/\") + \"/\" return \u0026EtcdDiscovery{ EtcdDiscoveryConfig: config, session: session, myKey: config.Prefix + config.Key, services: make(map[string]string), }, nil } func (d *EtcdDiscovery) Register(ctx context.Context) error { lease := d.session.Lease() // 注册服务 _, err := d.Client.Put(ctx, d.myKey, d.Val, clientv3.WithLease(lease)) return err } func (d *EtcdDiscovery) UnRegister(ctx context.Context) error { // 注销服务 _, err := d.Client.Delete(ctx, d.myKey) return err } // 监听端点变化 func (d *EtcdDiscovery) Watch(ctx context.Context) error { // context用来停止监听 d.watchContext, d.watchCancel = context.WithCancel(ctx) // 首先获取所有端点 resp, err := d.Client.Get(d.watchContext, d.Prefix, clientv3.WithPrefix()) services := make(map[string]string) for _, kv := range resp.Kvs { services[string(kv.Key)] = string(kv.Value) } d.setServices(services) // 回调点，用户可自定义 if d.Callbacks.OnStartedDiscovering != nil { d.Callbacks.OnStartedDiscovering(d.ListServices()) } defer func() { if d.Callbacks.OnStoppedDiscovering != nil { d.Callbacks.OnStoppedDiscovering() } }() defer d.watchCancel() // 监听目录，通过WithPrefix可以添加子目录变化 ch := d.Client.Watch(d.watchContext, d.Prefix, clientv3.WithPrefix()) for { select { case \u003c-d.watchContext.Done(): return nil case wr, ok := \u003c-ch: if !ok { return fmt.Errorf(\"watch closed\") } if wr.Err() != nil { return wr.Err() } // 将添加事件同步到本地端点列表 for _, ev := range wr.Events { key, val := string(ev.Kv.Key), string(ev.Kv.Value) switch ev.Type { case mvccpb.PUT: d.addService(key, val) case mvccpb.DELETE: d.delService(key) } if d.Callbacks.OnServiceChanged != nil { event := DiscoveryEvent{Type: mvccpb.Event_EventType_name[int32(ev.Type)], Service: d.serviceFromKv(key, val)} d.Callbacks.OnServiceChanged(d.ListServices(), event) } } } } } 主要实现逻辑如下： 创建Session， Session中Lease会自动续约 服务注册时，在目录下创建对应的子目录，并附带Lease 通过Watch接口监听目录变化，同步到本地 简单测试下，通过worker模拟不同的端点 func main() { client, err := clientv3.New(clientv3.Config{ Endpoints: []string{\"localhost:2379\"}, DialTimeout: 3 * time.Second, }) if err != nil { log.Fatalf(\"failed to create etcd lock: %v\", err) } defer client.Close() worker := func(i int, run bool) { id := fmt.Sprintf(\"worker-%d\", i) val := fmt.Sprintf(\"10.0.0.%d\", i) sd, err := etcdiscovery.New(etcdiscovery.EtcdDiscoveryConfig{ Client: client, Prefix: \"/services\", Key: id, Val: val, TTLSeconds: 2, Callbacks: etcdiscovery.DiscoveryCallbacks{ OnStartedDiscovering: func(services []etcdiscovery.Service) { log.Printf(\"[%s], onstarted, services: %v\", id, services) }, OnStoppedDiscovering: func() { log.Printf(\"[%s], onstoped\", id) }, OnServiceChanged: func(services []etcdiscovery.Service, event etcdiscovery.DiscoveryEvent) { log.Printf(\"[%s], onchanged, services: %v, event: %v\", id, services, event) }, }, }) if err != nil { log.Fatalf(\"failed to create service etcdiscovery: %v\", err) } defer sd.Close() if !run { if sd.UnRegister(context.Background()); err != nil { log.Fatalf(\"failed to unregister service [%s]: %v\", id, err) } return } if err := sd.Register(context.Background()); err != nil { log.Fatalf(\"failed to register service [%s]: %v\", id, err) } if err := sd.Watch(context.Background()); err != nil { log.Fatalf(\"failed to watch service: %v\", err) } } wg := group.NewGroup() for i := 0; i \u003c 3; i++ { id := i wg.Go(func() { worker(id, true) }) } go func() { time.Sleep(2 * time.Second) worker(","date":"Aug 07, 2022","objectID":"/golang-distributed-system-x-etcd/:1:0","series":null,"tags":["etcd","golang","分布式"],"title":"Golang分布式应用之etcd","uri":"/golang-distributed-system-x-etcd/#服务发现"},{"categories":["code"],"content":" 分布式锁在ECTD官方库go.etcd.io/etcd/client/v3/concurrency中，已经支持分布式锁。 主要原理与之前通过Redis实现的分布式锁类似，如果目录创建成功则加锁成功，解锁直接删除即可。 etcd锁的使用 // 创建session并不断刷新 session, err := concurrency.NewSession(client, concurrency.WithTTL(2*time.Second)) if err != nil { return nil, err } mutex := concurrency.NewMutex(session, config.Prefix) mutex.Lock() defer mutext.UnLock() do().... 加锁的核心逻辑如下 func (m *Mutex) tryAcquire(ctx context.Context) (*v3.TxnResponse, error) { s := m.s client := m.s.Client() m.myKey = fmt.Sprintf(\"%s%x\", m.pfx, s.Lease()) cmp := v3.Compare(v3.CreateRevision(m.myKey), \"=\", 0) // put self in lock waiters via myKey; oldest waiter holds lock put := v3.OpPut(m.myKey, \"\", v3.WithLease(s.Lease())) // reuse key in case this session already holds the lock get := v3.OpGet(m.myKey) // fetch current holder to complete uncontended path with only one RPC getOwner := v3.OpGet(m.pfx, v3.WithFirstCreate()...) resp, err := client.Txn(ctx).If(cmp).Then(put, getOwner).Else(get, getOwner).Commit() if err != nil { return nil, err } m.myRev = resp.Header.Revision if !resp.Succeeded { m.myRev = resp.Responses[0].GetResponseRange().Kvs[0].CreateRevision } return resp, nil } tryAcquire通过事务来执行加锁逻辑: 判断当前Key是否为空，即代码中Revision为0 如果为空，使用Put设置并附加Lease 如果不为空，获取当前锁的所有者，即最先加锁的对象，避免惊群效应 func (m *Mutex) Lock(ctx context.Context) error { resp, err := m.tryAcquire(ctx) if err != nil { return err } // if no key on prefix / the minimum rev is key, already hold the lock ownerKey := resp.Responses[1].GetResponseRange().Kvs if len(ownerKey) == 0 || ownerKey[0].CreateRevision == m.myRev { m.hdr = resp.Header return nil } client := m.s.Client() _, werr := waitDeletes(ctx, client, m.pfx, m.myRev-1) // release lock key if wait failed if werr != nil { m.Unlock(client.Ctx()) return werr } // make sure the session is not expired, and the owner key still exists. gresp, werr := client.Get(ctx, m.myKey) return nil } Lock方法会一直阻塞，直到获取锁返回执行出错: 调用tryAcquire 如果已经加锁成功，或者已经加过锁（可重入），则直接返回 调用waitDeletes方法，等待所有小于当前Revsion的Key删除 ","date":"Aug 07, 2022","objectID":"/golang-distributed-system-x-etcd/:2:0","series":null,"tags":["etcd","golang","分布式"],"title":"Golang分布式应用之etcd","uri":"/golang-distributed-system-x-etcd/#分布式锁"},{"categories":["code"],"content":" 分布式选主对于有状态的服务，为了提供其服务水平SLA减少宕机时间，通过会有多个副本，当主节点宕机时，副本节点可以快速切换。 通过etcd可以实现选主服务，与分布式比较类似 选主成功，不断上报心跳 通过Watch接口，当节点失效时，去竞争主(类似加锁过程) 在ECTD官方库go.etcd.io/etcd/client/v3/concurrency中，已经支持了分布式选主。 选主核心逻辑如下 func (e *Election) Campaign(ctx context.Context, val string) error { s := e.session client := e.session.Client() k := fmt.Sprintf(\"%s%x\", e.keyPrefix, s.Lease()) txn := client.Txn(ctx).If(v3.Compare(v3.CreateRevision(k), \"=\", 0)) txn = txn.Then(v3.OpPut(k, val, v3.WithLease(s.Lease()))) txn = txn.Else(v3.OpGet(k)) resp, err := txn.Commit() if err != nil { return err } e.leaderKey, e.leaderRev, e.leaderSession = k, resp.Header.Revision, s if !resp.Succeeded { kv := resp.Responses[0].GetResponseRange().Kvs[0] e.leaderRev = kv.CreateRevision if string(kv.Value) != val { if err = e.Proclaim(ctx, val); err != nil { e.Resign(ctx) return err } } } _, err = waitDeletes(ctx, client, e.keyPrefix, e.leaderRev-1) if err != nil { // clean up in case of context cancel select { case \u003c-ctx.Done(): e.Resign(client.Ctx()) default: e.leaderSession = nil } return err } e.hdr = resp.Header return nil } 以上逻辑与ECTD锁中的实现非常相似 开启事务，首先判断当前服务Key是否存在 不存在，通过Put设置对应值 存在获得当前目录最小Revision的值，即当前主节点 通过waitDeletes，直到当前进程的Revision 简单封装下，支持回调，参考了Kubernetes的选主实现 func New(config LeaderElectionConfig) (*EctdLeaderElection, error) { session, err := concurrency.NewSession(config.Client, concurrency.WithTTL(config.LeaseSeconds)) if err != nil { return nil, err } election := concurrency.NewElection(session, config.Prefix) return \u0026EctdLeaderElection{ LeaderElectionConfig: config, session: session, election: election, }, nil } // 运行选主 func (le *EctdLeaderElection) Run(ctx context.Context) error { defer func() { le.Callbacks.OnStoppedLeading() }() ctx, cancel := context.WithCancel(ctx) defer cancel() // 添加选主变化 go le.observe(ctx) // 开始选主 if err := le.election.Campaign(ctx, le.Identity); err != nil { return err } // 选主完成，运行OnStarted，运行结束则退出选主 le.Callbacks.OnStartedLeading(ctx) return nil } // 监听Key变化，执行回调 func (le *EctdLeaderElection) observe(ctx context.Context) { if le.Callbacks.OnNewLeader == nil { return } ch := le.election.Observe(ctx) for { select { case \u003c-ctx.Done(): return case resp, ok := \u003c-ch: if !ok { return } if len(resp.Kvs) == 0 { continue } leader := string(resp.Kvs[0].Value) if leader != le.Identity { go le.Callbacks.OnNewLeader(leader) } } } } func (le *EctdLeaderElection) Close() error { return le.session.Close() } 测试选主服务 func main() { client, err := clientv3.New(clientv3.Config{ Endpoints: []string{\"localhost:2379\"}, DialTimeout: 3 * time.Second, }) if err != nil { log.Fatalf(\"failed to create etcd lock: %v\", err) } defer client.Close() prefix := \"/worker/election\" worker := func(i int) { id := fmt.Sprintf(\"worker-%d\", i) le, err := leaderelection.New(leaderelection.LeaderElectionConfig{ Client: client, LeaseSeconds: 15, Prefix: prefix, Identity: id, Callbacks: leaderelection.LeaderCallbacks{ OnStartedLeading: func(ctx context.Context) { log.Printf(\"OnStarted[%s]: acquire new leader\", id) time.Sleep(3 * time.Second) log.Printf(\"OnStarted[%s]: worker done\", id) }, OnStoppedLeading: func() { log.Printf(\"OnStopped[%s]: exit\", id) }, OnNewLeader: func(identity string) { log.Printf(\"OnNewLeader[%s]: new leader %s\", id, identity) }, }, }) if err != nil { log.Fatalf(\"failed to create leader election: %v\", err) } defer le.Close() le.Run(context.Background()) } wg := sync.WaitGroup{} for i := 1; i \u003c= 3; i++ { wg.Add(1) id := i go func() { defer wg.Done() worker(id) }() } wg.Wait() } 运行结果 2022/08/08 09:33:32 OnNewLeader[worker-2]: new leader worker-3 2022/08/08 09:33:32 OnNewLeader[worker-1]: new leader worker-3 2022/08/08 09:33:32 OnStarted[worker-3]: acquire new leader 2022/08/08 09:34:02 OnStarted[worker-3]: worker done 2022/08/08 09:34:02 OnStopped[worker-3]: exit 2022/08/08 09:34:02 OnStarted[worker-2]: acquire new leader 2022/08/08 09:34:02 OnNewLeader[worker-1]: new leader worker-2 2022/08/08 09:34:32 OnStarted[worker-2]: worker done 2022/08/08 09:34:32 OnStopped[worke","date":"Aug 07, 2022","objectID":"/golang-distributed-system-x-etcd/:3:0","series":null,"tags":["etcd","golang","分布式"],"title":"Golang分布式应用之etcd","uri":"/golang-distributed-system-x-etcd/#分布式选主"},{"categories":["code"],"content":" 发布订阅借助etcd的前缀查找、Watch的功能，可以实现发布订阅功能，主要逻辑如下 // 发布时，直接通过Put将对象设置在对应Topic路径下，并可以设置Lease，自动删除过时消息 func (ps *EtcdPubSub) Publish(ctx context.Context, topic string, msg Msg) error { le, err := ps.Client.Lease.Grant(ctx, int64(ps.TTLSeconds)) if err != nil { return err } _, err = ps.Client.Put(ctx, ps.Prefix+topic+\"/\"+msg.Name, msg.Val, clientv3.WithLease(le.ID)) return err } // 订阅时，通过Watch来监听Topic是否有Put事件，这里忽略Delete事件 // Revision为0时，从当前时间点开始监听 // Revision为1时，监听Topic创建后的所有事件 func (ps *EtcdPubSub) SubscribeFromRev(ctx context.Context, topic string, rev int64) (\u003c-chan Msg, error) { wch := ps.Client.Watch(ctx, ps.Prefix+topic, clientv3.WithPrefix(), clientv3.WithFilterDelete(), clientv3.WithRev(rev)) msg := make(chan Msg) go func() { defer close(msg) for { wc, ok := \u003c-wch if !ok { return } for _, ev := range wc.Events { if ev.Type != mvccpb.PUT { break } name := strings.TrimPrefix(string(ev.Kv.Key), ps.Prefix+topic+\"/\") msg \u003c- Msg{Name: name, Val: string(ev.Kv.Value)} } } }() return msg, nil } 发布时，直接通过PUT操作在Topic路径下设置消息； 订阅时，通过Watch来捕获消息，通过Revision来配置不同的监听行为 Revision为0时，从当前时间点开始监听 Revision为1时，监听Topic创建后的所有事件 ","date":"Aug 07, 2022","objectID":"/golang-distributed-system-x-etcd/:4:0","series":null,"tags":["etcd","golang","分布式"],"title":"Golang分布式应用之etcd","uri":"/golang-distributed-system-x-etcd/#发布订阅"},{"categories":["code"],"content":" 总结本文主要结合Golang总结了etcd中服务发现、分布式锁、选主等实现方式，另外etcd还可以应用在发布订阅、负载均衡等方面。 本文所有代码见https://github.com/qingwave/gocorex，欢迎批评指正。 Explore more in https://qingwave.github.io ","date":"Aug 07, 2022","objectID":"/golang-distributed-system-x-etcd/:5:0","series":null,"tags":["etcd","golang","分布式"],"title":"Golang分布式应用之etcd","uri":"/golang-distributed-system-x-etcd/#总结"},{"categories":["code"],"content":"在系统开发中，有一类任务不是立即执行，而是在未来某个时间点或者按照一定间隔去执行，比如日志定期压缩、报表制作、过期数据清理等，这就是定时任务。 在单机中，定时任务通常需要实现一个类似crontab的系统，一般有两种方式： 最小堆，按照任务执行时间建堆，每次取最近的任务执行 时间轮，将任务放到时间轮列表中，每次转动取对应的任务列表执行 ","date":"Jul 28, 2022","objectID":"/golang-distributed-system-x-cron/:0:0","series":null,"tags":["cron","golang","分布式"],"title":"Golang分布式应用之定时任务","uri":"/golang-distributed-system-x-cron/#"},{"categories":["code"],"content":" 最小堆最小堆是一种特殊的完全二叉树，任意非叶子节点的值不大于其子节点，如图 通过最小堆，根据任务最近执行时间键堆，每次取堆顶元素即最近需要执行的任务，设置timer定时器，到期后触发任务执行。由于堆的特性每次调整的时间复杂度为O(lgN)，相较于普通队列性能更快。 在container/heap中已经实现操作堆的相关函数，我们只需要实现定期任务核心逻辑即可。 // 运行 func (c *Cron) Run() error { // 设置cron已启动，atomic.Bool来保证并发安全 c.started.Store(true) // 主循环 for { // 如果停止则退出 if !c.started.Load() { break } c.runTask() } return nil } // 核心逻辑 func (c *Cron) runTask() { now := time.Now() duration := infTime // 获取堆顶元素 task, ok := c.tasks.Peek() if ok { // 如果已删除则弹出 if !c.set.Has(task.Name()) { c.tasks.Pop() return } // 计算于当前时间查找，设置定时器 if task.next.After(now) { duration = task.next.Sub(now) } else { duration = 0 } } timer := time.NewTimer(duration) defer timer.Stop() // 当有新元素插入直接返回，防止新元素执行时间小于当前堆顶元素 select { case \u003c-c.new: return case \u003c-timer.C: } // 弹出任务，执行 go task.Exec() // 计算下次执行时间，如果为0说明任务已结束，否则重新入堆 task.next = task.Next(time.Now()) if task.next.IsZero() { c.set.Delete(task.Name()) } else { c.tasks.Push(task) } } 主要逻辑可总结为: 将任务按照下次执行时间建最小堆 每次取堆顶任务，设置定时器 如果中间有新加入任务，转入步骤2 定时器到期后执行任务 再次取下个任务，转入步骤2，依次执行 ","date":"Jul 28, 2022","objectID":"/golang-distributed-system-x-cron/:1:0","series":null,"tags":["cron","golang","分布式"],"title":"Golang分布式应用之定时任务","uri":"/golang-distributed-system-x-cron/#最小堆"},{"categories":["code"],"content":" 时间轮另一种实现Cron的方式是时间轮，时间轮通过一个环形队列，每个插槽放入需要到期执行的任务，按照固定间隔转动时间轮，取插槽中任务列表执行，如图所示: 时间轮可看作一个表盘，如图中时间间隔为1秒，总共60个格子，如果任务在3秒后执行则放为插槽3，每秒转动次取插槽上所有任务执行。 如果执行时间超过最大插槽，比如有个任务需要63秒后执行（超过了最大格子刻度），一般可以通过多层时间轮，或者设置一个额外变量圈数，只执行圈数为0的任务。 时间轮插入的时间复杂度为O(1)，获取任务列表复杂度为O(1)，执行列表最差为O(n)。对比最小堆，时间轮插入删除元素更快。 核心代码如下: // 定义 type TimeWheel struct { interval time.Duration // 触发间隔 slots int // 总插槽数 currentSlot int // 当前插槽数 tasks []*list.List // 环形列表，每个元素为对应插槽的任务列表 set containerx.Set[string] // 记录所有任务key值，用来检查任务是否被删除 tricker *time.Ticker // 定时触发器 logger logr.Logger } func (tw *TimeWheel) Run() error { tw.tricker = time.NewTicker(tw.interval) for { // 通过定时器模拟时间轮转动 now, ok := \u003c-tw.tricker.C if !ok { break } // 转动一次，执行任务列表 tw.RunTask(now, tw.currentSlot) tw.currentSlot = (tw.currentSlot + 1) % tw.slots } return nil } func (tw *TimeWheel) RunTask(now time.Time, slot int) { // 一次执行任务列表 for item := taskList.Front(); item != nil; { task, ok := item.Value.(*TimeWheelTask) // 任务圈数大于0，不需要执行，将圈数减一 if task.circle \u003e 0 { task.circle-- item = item.Next() continue } // 运行任务 go task.Exec() // 计算任务下次运行时间 next := item.Next() taskList.Remove(item) item = next task.next = task.Next(now) if !task.next.IsZero() { tw.add(now, task) } else { tw.Remove(task.Name()) } } } // 添加任务，计算下一次任务执行的插槽与圈数 func (tw *TimeWheel) add(now time.Time, task *TimeWheelTask) { if !task.initialized { task.next = task.Next(now) task.initialized = true } duration := task.next.Sub(now) if duration \u003c= 0 { task.slot = tw.currentSlot + 1 task.circle = 0 } else { mult := int(duration / tw.interval) task.slot = (tw.currentSlot + mult) % tw.slots task.circle = mult / tw.slots } tw.tasks[task.slot].PushBack(task) tw.set.Insert(task.Name()) } 时间轮的主要逻辑如下： 将任务存在对应插槽的时间 通过定时间模拟时间轮转动 每次到期后遍历当前插槽的任务列表，若任务圈数为0则执行 如果任务未结束，计算下次执行的插槽与圈数 转入步骤2，依次执行 ","date":"Jul 28, 2022","objectID":"/golang-distributed-system-x-cron/:2:0","series":null,"tags":["cron","golang","分布式"],"title":"Golang分布式应用之定时任务","uri":"/golang-distributed-system-x-cron/#时间轮"},{"categories":["code"],"content":" 总结本文主要总结了定时任务的两种实现方式，最小堆与时间轮，并分析其核心实现逻辑。 对于执行分布式定时任务，可以借助延时消息队列或者直接使用Kubernetes的CronJob。 自己开发的话可以借助Etcd： 中心节点Coordinator将任务按照一定算法(Hash、轮询、或者更复杂的分配算法)将任务与工作节点Worker绑定 每个Worker添加到有绑定到自己的任务则取出放到本地的Cron中 如果Worker挂掉，执行将其上任务重新绑定即可 本文所有代码见https://github.com/qingwave/gocorex/tree/main/cron Explore more in https://qingwave.github.io ","date":"Jul 28, 2022","objectID":"/golang-distributed-system-x-cron/:3:0","series":null,"tags":["cron","golang","分布式"],"title":"Golang分布式应用之定时任务","uri":"/golang-distributed-system-x-cron/#总结"},{"categories":["code"],"content":"Redis是一个高性能的内存数据库，常被应用于分布式系统中，除了作为分布式缓存或简单的内存数据库还有一些特殊的应用场景，本文结合Golang来编写对应的中间件。 本文所有代码见https://github.com/qingwave/gocorex ","date":"Jul 22, 2022","objectID":"/golang-distributed-system-x-redis/:0:0","series":null,"tags":["redis","golang","分布式"],"title":"Golang分布式应用之Redis","uri":"/golang-distributed-system-x-redis/#"},{"categories":["code"],"content":" 分布式锁单机系统中我们可以使用sync.Mutex来保护临界资源，在分布式系统中同样有这样的需求，当多个主机抢占同一个资源，需要加对应的“分布式锁”。 在Redis中我们可以通过setnx命令来实现 如果key不存在可以设置对应的值，设置成功则加锁成功，key不存在返回失败 释放锁可以通过del实现。 主要逻辑如下： type RedisLock struct { client *redis.Client key string expiration time.Duration // 过期时间，防止宕机或者异常 } func NewLock(client *redis.Client, key string, expiration time.Duration) *RedisLock { return \u0026RedisLock{ client: client, key: key, expiration: expiration, } } // 加锁将成功会将调用者id保存到redis中 func (l *RedisLock) Lock(id string) (bool, error) { return l.client.SetNX(context.TODO(), l.key, id, l.expiration).Result() } const unLockScript = ` if (redis.call(\"get\", KEYS[1]) == KEYS[2]) then redis.call(\"del\", KEYS[1]) return true end return false ` // 解锁通过lua脚本来保证原子性，只能解锁当前调用者加的锁 func (l *RedisLock) UnLock(id string) error { _, err := l.client.Eval(context.TODO(), unLockScript, []string{l.key, id}).Result() if err != nil \u0026\u0026 err != redis.Nil { return err } return nil } 需要加一个额外的超时时间来防止系统宕机或者异常请求造成的死锁，通过超时时间为最大预估运行时间的2倍。 解锁时通过lua脚本来保证原子性，调用者只会解自己加的锁。避免由于超时造成的混乱，例如：进程A在时间t1获取了锁，但由于执行缓慢，在时间t2锁超时失效，进程B在t3获取了锁，这是如果进程A执行完去解锁会取消进程B的锁。 运行测试 func main() { client := redis.NewClient(\u0026redis.Options{ Addr: \"localhost:6379\", Password: \"123456\", DB: 0, // use default DB }) lock := NewLock(client, \"counter\", 30*time.Second) counter := 0 worker := func(i int) { for { id := fmt.Sprintf(\"worker%d\", i) ok, err := lock.Lock(id) log.Printf(\"worker %d attempt to obtain lock, ok: %v, err: %v\", i, ok, err) if !ok { time.Sleep(100 * time.Millisecond) continue } defer lock.UnLock(id) counter++ log.Printf(\"worker %d, add counter %d\", i, counter) break } } wg := sync.WaitGroup{} for i := 1; i \u003c= 5; i++ { wg.Add(1) id := i go func() { defer wg.Done() worker(id) }() } wg.Wait() } 运行结果，可以看到与sync.Mutex使用效果类似 2022/07/22 09:58:09 worker 5 attempt to obtain lock, ok: true, err: \u003cnil\u003e 2022/07/22 09:58:09 worker 5, add counter 1 2022/07/22 09:58:09 worker 4 attempt to obtain lock, ok: false, err: \u003cnil\u003e 2022/07/22 09:58:09 worker 1 attempt to obtain lock, ok: false, err: \u003cnil\u003e 2022/07/22 09:58:09 worker 2 attempt to obtain lock, ok: false, err: \u003cnil\u003e 2022/07/22 09:58:09 worker 3 attempt to obtain lock, ok: false, err: \u003cnil\u003e 2022/07/22 09:58:10 worker 3 attempt to obtain lock, ok: false, err: \u003cnil\u003e 2022/07/22 09:58:10 worker 1 attempt to obtain lock, ok: false, err: \u003cnil\u003e 2022/07/22 09:58:10 worker 2 attempt to obtain lock, ok: false, err: \u003cnil\u003e 2022/07/22 09:58:10 worker 4 attempt to obtain lock, ok: true, err: \u003cnil\u003e 2022/07/22 09:58:10 worker 4, add counter 2 2022/07/22 09:58:10 worker 1 attempt to obtain lock, ok: true, err: \u003cnil\u003e 2022/07/22 09:58:10 worker 1, add counter 3 2022/07/22 09:58:10 worker 3 attempt to obtain lock, ok: false, err: \u003cnil\u003e 2022/07/22 09:58:10 worker 2 attempt to obtain lock, ok: false, err: \u003cnil\u003e 2022/07/22 09:58:10 worker 2 attempt to obtain lock, ok: true, err: \u003cnil\u003e 2022/07/22 09:58:10 worker 2, add counter 4 2022/07/22 09:58:10 worker 3 attempt to obtain lock, ok: false, err: \u003cnil\u003e 2022/07/22 09:58:10 worker 3 attempt to obtain lock, ok: true, err: \u003cnil\u003e 2022/07/22 09:58:10 worker 3, add counter 5 特别注意的是，在分布式Redis集群中，如果发生异常时(主节点宕机)，可能会降低分布式锁的可用性，可以通过强一致性的组件etcd、ZooKeeper等实现。 ","date":"Jul 22, 2022","objectID":"/golang-distributed-system-x-redis/:1:0","series":null,"tags":["redis","golang","分布式"],"title":"Golang分布式应用之Redis","uri":"/golang-distributed-system-x-redis/#分布式锁"},{"categories":["code"],"content":" 分布式过滤器假设要开发一个爬虫服务，爬取百万级的网页，怎么判断某一个网页是否爬取过，除了借助数据库和HashMap，我们可以借助布隆过滤器来做。相比其他方式布隆过滤器占用极低的空间，而且插入查询时间非常快。 布隆过滤器用来判断某个元素是否在集合中，利用BitSet 插入数据时将值进行多次Hash，将BitSet对应位置1 查询时同样进行多次Hash对比所有位上是否为1，如是则存在。 布隆过滤器有一定的误判率，不适合精确查询的场景。另外也不支持删除元素。通常适用于URL去重、垃圾邮件过滤、防止缓存击穿等场景中。 在Redis中，我们可以使用自带的BitSet实现，同样也借助lua脚本的原子性来避免多次查询数据不一致。 const ( // 插入数据，调用setbit设置对应位 setScript = ` for _, offset in ipairs(ARGV) do redis.call(\"setbit\", KEYS[1], offset, 1) end ` // 查询数据，如果所有位都为1返回true getScript = ` for _, offset in ipairs(ARGV) do if tonumber(redis.call(\"getbit\", KEYS[1], offset)) == 0 then return false end end return true ` ) type BloomFilter struct { client *redis.Client key string // 存在redis中的key bits uint // BitSet的大小 maps uint // Hash的次数 } func NewBloomFilter(client *redis.Client, key string, bits, maps uint) *BloomFilter { client.Del(context.TODO(), key) if maps == 0 { maps = 14 } return \u0026BloomFilter{ key: key, client: client, bits: bits, maps: maps, } } // 进行多次Hash, 得到位置列表 func (f *BloomFilter) getLocations(data []byte) []uint { locations := make([]uint, f.maps) for i := 0; i \u003c int(f.maps); i++ { val := murmur3.Sum64(append(data, byte(i))) locations[i] = uint(val) % f.bits } return locations } func (f *BloomFilter) Add(data []byte) error { args := getArgs(f.getLocations(data)) _, err := f.client.Eval(context.TODO(), setScript, []string{f.key}, args).Result() if err != nil \u0026\u0026 err != redis.Nil { return err } return nil } func (f *BloomFilter) Exists(data []byte) (bool, error) { args := getArgs(f.getLocations(data)) resp, err := f.client.Eval(context.TODO(), getScript, []string{f.key}, args).Result() if err != nil { if err == redis.Nil { return false, nil } return false, err } exists, ok := resp.(int64) if !ok { return false, nil } return exists == 1, nil } func getArgs(locations []uint) []string { args := make([]string, 0) for _, l := range locations { args = append(args, strconv.FormatUint(uint64(l), 10)) } return args } 运行测试 func main() { bf := NewBloomFilter(client,\"bf-test\", 2^16, 14) exists, err := bf.Exists([]byte(\"test1\")) log.Printf(\"exist %t, err %v\", exists, err) if err := bf.Add([]byte(\"test1\")); err != nil { log.Printf(\"add err: %v\", err) } exists, err = bf.Exists([]byte(\"test1\")) log.Printf(\"exist %t, err %v\", exists, err) exists, err = bf.Exists([]byte(\"test2\")) log.Printf(\"exist %t, err %v\", exists, err) // output // 2022/07/22 10:05:58 exist false, err \u003cnil\u003e // 2022/07/22 10:05:58 exist true, err \u003cnil\u003e // 2022/07/22 10:05:58 exist false, err \u003cnil\u003e } ","date":"Jul 22, 2022","objectID":"/golang-distributed-system-x-redis/:2:0","series":null,"tags":["redis","golang","分布式"],"title":"Golang分布式应用之Redis","uri":"/golang-distributed-system-x-redis/#分布式过滤器"},{"categories":["code"],"content":" 分布式限流器在golang.org/x/time/rate包中提供了基于令牌桶的限流器，如果要实现分布式环境的限流可以基于Redis Lua脚本实现。 令牌桶的主要原理如下： 假设一个令牌桶容量为burst，每秒按照qps的速率往里面放置令牌 初始时放满令牌，令牌溢出则直接丢弃，请求令牌时，如果桶中有足够令牌则允许，否则拒绝 当burst==qps时，严格按照qps限流；当burst\u003eqps时，可以允许一定的突增流量 这里主要参考了官方rate包的实现，将核心逻辑改为Lua实现。 --- 相关Key --- limit rate key值，对应value为当前令牌数 local limit_key = KEYS[1] --- 输入参数 --[[ qps: 每秒请求数; burst: 令牌桶容量; now: 当前Timestamp; cost: 请求令牌数; max_wait: 最大等待时间 --]] local qps = tonumber(ARGV[1]) local burst = tonumber(ARGV[2]) local now = ARGV[3] local cost = tonumber(ARGV[4]) local max_wait = tonumber(ARGV[5]) --- 获取redis中的令牌数 local tokens = redis.call(\"hget\", limit_key, \"token\") if not tokens then tokens = burst end --- 上次修改时间 local last_time = redis.call(\"hget\", limit_key, \"last_time\") if not last_time then last_time = 0 end --- 最新等待时间 local last_event = redis.call(\"hget\", limit_key, \"last_event\") if not last_event then last_event = 0 end --- 通过当前时间与上次修改时间的差值，qps计算出当前时间得令牌数 local delta = math.max(0, now-last_time) local new_tokens = math.min(burst, delta * qps + tokens) new_tokens = new_tokens - cost --- 最新令牌数，减少请求令牌 --- 如果最新令牌数小于0，计算需要等待的时间 local wait_period = 0 if new_tokens \u003c 0 and qps \u003e 0 then wait_period = wait_period - new_tokens / qps end wait_period = math.ceil(wait_period) local time_act = now + wait_period --- 满足等待间隔的时间戳 --- 允许请求有两种情况 --- 当请求令牌数小于burst, 等待时间不超过最大等待时间，可以通过补充令牌满足请求 --- qps为0时，只要最新令牌数不小于0即可 local ok = (cost \u003c= burst and wait_period \u003c= max_wait and qps \u003e 0) or (qps == 0 and new_tokens \u003e= 0) --- 设置对应值 if ok then redis.call(\"set\", limit_key, new_tokens) redis.call(\"set\", last_time_key, now) redis.call(\"set\", last_event_key, time_act) end --- 返回列表，{是否允许， 等待时间} return {ok, wait_period} 在Golang中的相关接口Allow、AllowN、Wait等都是通过调用reserveN实现 // 调用lua脚本 func (lim *RedisLimiter) reserveN(now time.Time, n int, maxFutureReserveSecond int) (*Reservation, error) { // ... res, err := lim.rdb.Eval(context.TODO(), reserveNScript, []string{lim.limitKey}, lim.qps, lim.burst, now.Unix(), n, maxFutureReserveSecond).Result() if err != nil \u0026\u0026 err != redis.Nil { return nil, err } //... return \u0026Reservation{ ok: allow == 1, lim: lim, tokens: n, timeToAct: now.Add(time.Duration(wait) * time.Second), }, nil } 运行测试 func main() { rdb := redis.NewClient(\u0026redis.Options{ Addr: \"localhost:6379\", Password: \"123456\", DB: 0, // use default DB }) r, err := NewRedisLimiter(rdb, 1, 2, \"testrate\") if err != nil { log.Fatal(err) } r.Reset() for i := 0; i \u003c 5; i++ { err := r.Wait(context.TODO()) log.Printf(\"worker %d allowed: %v\", i, err) } } // output // 2022/07/22 12:50:31 worker 0 allowed: \u003cnil\u003e // 2022/07/22 12:50:31 worker 1 allowed: \u003cnil\u003e // 2022/07/22 12:50:32 worker 2 allowed: \u003cnil\u003e // 2022/07/22 12:50:33 worker 3 allowed: \u003cnil\u003e // 2022/07/22 12:50:34 worker 4 allowed: \u003cnil\u003e 前两个请求在burst内，直接可以获得，后面的请求按照qps的速率生成。 ","date":"Jul 22, 2022","objectID":"/golang-distributed-system-x-redis/:3:0","series":null,"tags":["redis","golang","分布式"],"title":"Golang分布式应用之Redis","uri":"/golang-distributed-system-x-redis/#分布式限流器"},{"categories":["code"],"content":" 其他除此之外，Redis还可以用作全局计数、去重(set)、发布订阅等场景。Redis官方也提供了一些通用模块，通过加载这些模块也可以实现过滤、限流等特性，参考modules。 本文所有代码见https://github.com/qingwave/gocorex，欢迎批评指正 ","date":"Jul 22, 2022","objectID":"/golang-distributed-system-x-redis/:4:0","series":null,"tags":["redis","golang","分布式"],"title":"Golang分布式应用之Redis","uri":"/golang-distributed-system-x-redis/#其他"},{"categories":["code"],"content":" 参考 https://github.com/qingwave/gocorex https://go-zero.dev/ Explore more in https://qingwave.github.io ","date":"Jul 22, 2022","objectID":"/golang-distributed-system-x-redis/:5:0","series":null,"tags":["redis","golang","分布式"],"title":"Golang分布式应用之Redis","uri":"/golang-distributed-system-x-redis/#参考"},{"categories":["cloud"],"content":"最近在工作中需要捕获Kubernetes的Pod驱逐事件，再做额外的操作。第一个想法是能不能监听（watch）驱逐对象（Eviction Resource），很遗憾Eviction并没有watch接口，只是Pod下的一个子资源，和Scale、Status类似。等等，既然是子资源那能不能通过Webhook获取。 ","date":"Jul 08, 2022","objectID":"/k8s-watch-eviction-event/:0:0","series":null,"tags":["k8s"],"title":"捕获Kubernetes中Pod驱逐事件","uri":"/k8s-watch-eviction-event/#"},{"categories":["cloud"],"content":" 实现峰回路转，在kubernetes#pr76910中已经实现对pod/eviction子资源的支持。 简单验证一下 ","date":"Jul 08, 2022","objectID":"/k8s-watch-eviction-event/:1:0","series":null,"tags":["k8s"],"title":"捕获Kubernetes中Pod驱逐事件","uri":"/k8s-watch-eviction-event/#实现"},{"categories":["cloud"],"content":" 生成项目通过kubebuilder快速生成项目 kubebuilder init --component-config --domain qinng.io --repo github.com/qingwave/k8s -eviction-operator ","date":"Jul 08, 2022","objectID":"/k8s-watch-eviction-event/:1:1","series":null,"tags":["k8s"],"title":"捕获Kubernetes中Pod驱逐事件","uri":"/k8s-watch-eviction-event/#生成项目"},{"categories":["cloud"],"content":" 编写Webhook由于pod/eviction不是自定义资源，无法通过kubebuilder直接生成，可按照如下逻辑生成ValidatingAdmissionWebhook package webhook import ( \"context\" \"fmt\" \"net/http\" \"github.com/go-logr/logr\" admissionv1 \"k8s.io/api/admission/v1\" corev1 \"k8s.io/api/core/v1\" policyv1 \"k8s.io/api/policy/v1\" \"k8s.io/apimachinery/pkg/apis/meta/v1/unstructured\" \"k8s.io/apimachinery/pkg/runtime\" \"k8s.io/apimachinery/pkg/types\" ctrl \"sigs.k8s.io/controller-runtime\" \"sigs.k8s.io/controller-runtime/pkg/client\" \"sigs.k8s.io/controller-runtime/pkg/webhook\" \"sigs.k8s.io/controller-runtime/pkg/webhook/admission\" ) const ( WebhookName = \"Eviction\" EvictionKind = \"Eviction\" ) // rbac注解与webhook注解 // +kubebuilder:rbac:groups=\"\",resources=pods,verbs=get;list // +kubebuilder:webhook:path=/validate-v1-pod-eviction,admissionReviewVersions=v1;v1beta1,sideEffects=NoneOnDryRun,matchPolicy=Equivalent,mutating=false,failurePolicy=fail,groups=\"\",resources=pods/eviction,verbs=create,versions=v1,name=veviction.kb.io // EvictionValidator validates Pods Eviction event type EvictionValidator struct { scheme *runtime.Scheme client client.Client log logr.Logger decoder *admission.Decoder } // EvictionValidator 解析Eviction, 格式化返回 func (v *EvictionValidator) Handle(ctx context.Context, req admission.Request) admission.Response { logger := v.log.WithValues(\"eviction\", fmt.Sprintf(\"%s/%s\", req.Namespace, req.Name)) logger.Info(\"start handle eviction\") if req.Operation != admissionv1.Create { logger.Info(fmt.Sprintf(\"skip none create request, verb: %s\", req.Operation)) return admission.Allowed(\"\") } if req.DryRun != nil \u0026\u0026 *req.DryRun { logger.Info(\"skip dry run request\") return admission.Allowed(\"\") } if req.Kind.Kind != EvictionKind { logger.Info(fmt.Sprintf(\"expected request %s but got %s\", EvictionKind, req.Kind)) return admission.Errored(http.StatusBadRequest, fmt.Errorf(\"unexpected kind %v\", req.Kind)) } eviction, err := v.getEviction(req) if err != nil { logger.Error(err, \"failed to decode eviction\") return admission.Errored(http.StatusBadRequest, err) } logger.Info(fmt.Sprintf(\"reveice new obj, obj: %+#v\", *eviction)) if eviction.DeleteOptions != nil \u0026\u0026 len(eviction.DeleteOptions.DryRun) \u003e 0 { logger.Info(\"skip eviction dry run request\") return admission.Allowed(\"\") } if err := v.handleEviction(eviction); err != nil { return admission.Errored(http.StatusInternalServerError, err) } logger.Info(\"handle eviction success\") return admission.Allowed(\"\") } // EvictionValidator implements admission.DecoderInjector. // A decoder will be automatically injected. // InjectDecoder injects the decoder. func (v *EvictionValidator) InjectDecoder(d *admission.Decoder) error { v.decoder = d return nil } // 解析Eviction func (v *EvictionValidator) getEviction(req admission.Request) (*policyv1.Eviction, error) { obj := \u0026unstructured.Unstructured{} if err := v.decoder.Decode(req, obj); err != nil { return nil, err } eviction := \u0026policyv1.Eviction{} if err := v.scheme.Convert(obj, eviction, nil); err != nil { return nil, err } return eviction, nil } // 处理驱逐事件 func (v *EvictionValidator) handleEviction(eviction *policyv1.Eviction) error { podNamespacedName := types.NamespacedName{Namespace: eviction.Namespace, Name: eviction.Name} pod := \u0026corev1.Pod{} if err := v.client.Get(context.TODO(), podNamespacedName, pod); err != nil { return err } v.log.Info(fmt.Sprintf(\"get eviction pod: %#v\", pod)) return nil } // 注册Webhook func NewEvictionWebhook(mgr ctrl.Manager) error { w := \u0026EvictionValidator{ scheme: mgr.GetScheme(), client: mgr.GetClient(), log: mgr.GetLogger().WithName(WebhookName), } mgr.GetWebhookServer().Register(\"/validate-v1-pod-eviction\", \u0026webhook.Admission{ Handler: w, }) return nil } 特别注意的是，Eviction包括v1、v1beat1两个版本，解析时需要可以全部转换为v1方便处理 package webhook import ( policyv1 \"k8s.io/api/policy/v1\" policyv1beta1 \"k8s.io/api/policy/v1beta1\" \"k8s.io/apimachinery/pkg/conversion\" \"k8s.io/apimachinery/pkg/runtime\" ) func RegisterConversion(s *runtime.Scheme) error { return s.AddConversionFunc((*policyv1beta1.Eviction)(nil), (*pol","date":"Jul 08, 2022","objectID":"/k8s-watch-eviction-event/:1:2","series":null,"tags":["k8s"],"title":"捕获Kubernetes中Pod驱逐事件","uri":"/k8s-watch-eviction-event/#编写webhook"},{"categories":["cloud"],"content":" 测试通过kubectl-evict驱逐pod，在operator日志中显示已捕获事件： 1.6572752110781207e+09 DEBUG controller-runtime.webhook.webhooks received request {\"webhook\": \"/validate-v1-pod-eviction\", \"UID\": \"4f98f064-97c1-4ada-a9d8-0946afb11eba\", \"kind\": \"policy/v1beta1, Kind=Eviction\", \"resource\": {\"group\":\"\",\"version\":\"v1\",\"resource\":\"pods\"}} 1.6572752110781898e+09 INFO Eviction start handle eviction {\"eviction\": \"default/nginx-6799fc88d8-drkc4\"} 1.6572752110787241e+09 INFO Eviction reveice new obj, obj: v1.Eviction{TypeMeta:v1.TypeMeta{Kind:\"\", APIVersion:\"\"}, ObjectMeta:v1.ObjectMeta{Name:\"nginx-6799fc88d8-drkc4\", GenerateName:\"\", Namespace:\"default\", SelfLink:\"\", UID:\"\", ResourceVersion:\"\", Generation:0, CreationTimestamp:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), DeletionTimestamp:\u003cnil\u003e, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string(nil), Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ZZZ_DeprecatedClusterName:\"\", ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:\"kubectl-evict\", Operation:\"Update\", APIVersion:\"policy/v1beta1\", Time:time.Date(2022, time.July, 8, 10, 13, 31, 0, time.Local), FieldsType:\"FieldsV1\", FieldsV1:(*v1.FieldsV1)(0xc000479a10), Subresource:\"\"}}}, DeleteOptions:(*v1.DeleteOptions)(0xc00060dc20)} {\"eviction\": \"default/nginx-6799fc88d8-drkc4\"} 1.6572752111794329e+09 INFO Eviction get eviction pod: \u0026v1.Pod{TypeMeta:v1.TypeMeta{Kind:\"Pod\", APIVersion:\"v1\"}, ObjectMeta:v1.ObjectMeta{Name:\"nginx-6799fc88d8-drkc4\"... ","date":"Jul 08, 2022","objectID":"/k8s-watch-eviction-event/:1:3","series":null,"tags":["k8s"],"title":"捕获Kubernetes中Pod驱逐事件","uri":"/k8s-watch-eviction-event/#测试"},{"categories":["cloud"],"content":" 总结通过Webhook可以实现对驱逐事件的捕捉，但也有一些地方需要注意 如果处理逻辑比较复杂，尽量通过Webhook生成其他资源如CRD，Controller监听CRD再来处理其他的处理，防止Webhook处理超时，而且Controller遇到异常会再次重试 目前Webhook对于Eviction子资源，无法通过objectSelector选择特定的Pod，除非调用者在Eviction对象中包含了Pod的Labels Explore more in https://qingwave.github.io ","date":"Jul 08, 2022","objectID":"/k8s-watch-eviction-event/:2:0","series":null,"tags":["k8s"],"title":"捕获Kubernetes中Pod驱逐事件","uri":"/k8s-watch-eviction-event/#总结"},{"categories":["code"],"content":"大型项目中基本都包含有复杂的访问控制策略，特别是在一些多租户场景中，例如Kubernetes中就支持RBAC，ABAC等多种授权类型。在Golang中目前比较热门的访问控制框架有Open Policy Agent与Casbin，本文主要分析其异同与选型策略。 ","date":"May 20, 2022","objectID":"/openpolicyagent-vs-casbin/:0:0","series":null,"tags":["rbac","authorization"],"title":"Open Policy Agent vs Casbin","uri":"/openpolicyagent-vs-casbin/#"},{"categories":["code"],"content":" Open Policy AgentOpen Policy Agent(简称OPA)是一个开源的策略引擎，托管于CNCF，通常用来做在微服务、API网关、Kubernetes、CI/CD等系统中做策略管理。 OPA将策略从代码中分离出来，按照官网的说法OPA实现了策略即代码，通过Rego声明式语言实现决策逻辑，当系统需要做出策略时，只需携带请求查询OPA即可，OPA会返回决策结果。 ","date":"May 20, 2022","objectID":"/openpolicyagent-vs-casbin/:1:0","series":null,"tags":["rbac","authorization"],"title":"Open Policy Agent vs Casbin","uri":"/openpolicyagent-vs-casbin/#open-policy-agent"},{"categories":["code"],"content":" 那么我们为什么需要OPA?大型软件中各个组件都需要进行一些策略控制，比如用户权限校验、创建资源校验、某个时间段允许访问，如果每个组件都需要实现一套策略控制，那么彼此之间会不统一，维护困难。一个自然的想法是能否将这些策略逻辑抽离出来，形成一个单独的服务，同时这个服务可能需要提供各种不同sdk来屏蔽语言差异。 OPA正是解决这个问题，将散落在系统各处的策略进行统一，所有服务直接请求OPA即可。通过引入OPA可以降低系统耦合性，减少维护复杂度。 ","date":"May 20, 2022","objectID":"/openpolicyagent-vs-casbin/:1:1","series":null,"tags":["rbac","authorization"],"title":"Open Policy Agent vs Casbin","uri":"/openpolicyagent-vs-casbin/#那么我们为什么需要opa"},{"categories":["code"],"content":" Http API中使用OPA授权我们在Gin实现的Http服务中（原生http库也类似）引入OPA来实现Http API授权。示例代码见https://github.com/qingwave/opa-gin-authz 首先需要实现策略，我们允许所有用户访问非api的接口，拒绝未认证用户访问api资源，通过Rego实现如下： package authz default allow = false allow { input.method == \"GET\" not startswith(input.path, \"/api\") #如果请求方法为GET并且path不以/api开头则允许 } allow { input.method == \"GET\" input.subject.user != \"\" #用户名不为空 } 在Gin中实现OPA插件，这里通过嵌入OPA到代码中来实现授权，也可以将OPA单独部署 func WithOPA(opa *rego.PreparedEvalQuery, logger *zap.Logger) gin.HandlerFunc { return func(c *gin.Context) { user := c.Query(\"user\") groups := c.QueryArray(\"groups\") input := map[string]interface{}{ //构造OPA输入 \"method\": c.Request.Method, \"path\": c.Request.RequestURI, \"subject\": map[string]interface{}{ \"user\": user, \"group\": groups, }, } logger.Info(fmt.Sprintf(\"start opa middleware %s, %#v\", c.Request.URL.String(), input)) res, err := opa.Eval(context.TODO(), rego.EvalInput(input)) // 验证用户请求 if err != nil { c.JSON(http.StatusInternalServerError, err) c.Abort() return } defer logger.Info(fmt.Sprintf(\"opa result: %v, %#v\", res.Allowed(), res)) if !res.Allowed() { c.JSON(http.StatusForbidden, gin.H{ \"msg\": \"forbidden\", }) c.Abort() return } c.Next() } } ","date":"May 20, 2022","objectID":"/openpolicyagent-vs-casbin/:1:2","series":null,"tags":["rbac","authorization"],"title":"Open Policy Agent vs Casbin","uri":"/openpolicyagent-vs-casbin/#http-api中使用opa授权"},{"categories":["code"],"content":" CasbinCasbin是一个Golang实现的开源访问控制框架，支持RBAC、ACL等多种访问控制策略，也支持Golang、Java、JavaScript等多种语言。 在Casbin中, 访问控制模型被抽象为基于PERM(Policy, Effect, Request, Matcher) 的一个文件。通过定义PERM模型来描述资源与用户之间的关系，使用时将具体请求传入Casbin sdk即可返回决策结果。 ","date":"May 20, 2022","objectID":"/openpolicyagent-vs-casbin/:2:0","series":null,"tags":["rbac","authorization"],"title":"Open Policy Agent vs Casbin","uri":"/openpolicyagent-vs-casbin/#casbin"},{"categories":["code"],"content":" 为什么需要Casbin借助Casbin可以轻松实现比如RBAC的访问控制，不需要额外的代码。同时引入Casbin可以简化表结构，如果我们资源实现RBAC策略需要实现：用户表、角色表、操作表、用户角色表、角色操作表，通过RBAC实现，我们只需实现基础表即可，关系表由Casbin实现。 ","date":"May 20, 2022","objectID":"/openpolicyagent-vs-casbin/:2:1","series":null,"tags":["rbac","authorization"],"title":"Open Policy Agent vs Casbin","uri":"/openpolicyagent-vs-casbin/#为什么需要casbin"},{"categories":["code"],"content":" Casbin实现Http API访问控制首先，我们需要实现Casbin模式，包含请求与策略格式定义，Matchers即策略逻辑 [request_definition] r = sub, obj, act [policy_definition] p = sub, obj, act [policy_effect] e = some(where (p.eft == allow)) #其中一个策略生效则返回True [matchers] m = r.sub == p.sub \u0026\u0026 keyMatch(r.obj, p.obj) \u0026\u0026 r.act == p.act 预定义一些策略，也可以存储到数据库, alice可以访问所有/api开头的路径，bob只能访问/version路径 p, alice, /api/*, read p, bob, /version, write 通过各种需要的sdk可以轻松接入Casbin // 加载模型与策略，也可以存储到数据库 e, err := casbin.NewEnforcer(\"path/to/model.conf\", \"path/to/policy.csv\") sub := \"alice\" // the user that wants to access a resource. obj := \"data1\" // the resource that is going to be accessed. act := \"read\" // the operation that the user performs on the resource. ok, err := e.Enforce(sub, obj, act) //判断用户是否有权限 ","date":"May 20, 2022","objectID":"/openpolicyagent-vs-casbin/:2:2","series":null,"tags":["rbac","authorization"],"title":"Open Policy Agent vs Casbin","uri":"/openpolicyagent-vs-casbin/#casbin实现http-api访问控制"},{"categories":["code"],"content":" OPA vs Casbin那么，在项目中我们需要如何选择合适的策略引擎，如果项目授权方式比较简单，首先推荐通过代码实现，不需要引入第三方库。需要确实需要借助额外的框架可以考虑以下几点角度。 对比项 OPA Casbin 访问控制策略 通过Rego可以实现多种策略 原生支持ACL、ABAC、RBAC等多种策略 自定义策略 支持 通过自定义函数和Model实现，灵活性一般 调整策略复杂度 更改/添加Rego逻辑即可 如果已存在大量策略数据，需要考虑数据迁移 存储数据 不支持 支持存储策略存储到文件或数据库 运行方式 内嵌、单独部署 通常为内嵌 sdk支持语言 Go、WASM(nodejs)、Python-rego，其他通过Restful API 支持Java、Go、Python等多种常用语言 策略返回格式 Json数据 True/False 性能 评估时间随着策略数据量会增加，支持多节点部署 对于HTTP服务评估时间在1ms内 简而言之，如果系统策略模型固定，可以引入Casbin简化授权系统设计。如果策略需要经常调整、扩展，或者微服务系统中多个组件都需要策略控制，使用OPA可以将策略实现抽离出来。 ","date":"May 20, 2022","objectID":"/openpolicyagent-vs-casbin/:3:0","series":null,"tags":["rbac","authorization"],"title":"Open Policy Agent vs Casbin","uri":"/openpolicyagent-vs-casbin/#opa-vs-casbin"},{"categories":["code"],"content":" 引用 https://www.openpolicyagent.org/docs/latest/ https://casbin.org/docs/zh-CN/ Explore more in https://qingwave.github.io ","date":"May 20, 2022","objectID":"/openpolicyagent-vs-casbin/:4:0","series":null,"tags":["rbac","authorization"],"title":"Open Policy Agent vs Casbin","uri":"/openpolicyagent-vs-casbin/#引用"},{"categories":["code"],"content":"随着Kubernetes成为容器编排领域的事实标准，Golang在云原生方面应用的也越来越多。今天我们跟随K8s的脚步，学习下在K8s中使用哪些经典的设计模式。 ","date":"Apr 14, 2022","objectID":"/k8s-golang-design-pattern/:0:0","series":null,"tags":["golang","k8s"],"title":"Kubernetes中的Golang设计模式","uri":"/k8s-golang-design-pattern/#"},{"categories":["code"],"content":" 创建型模式创建型模式顾名思义提供了对象的创建机制，封装了内部的复杂性，提高代码复用和灵活性。包括： 单例模式 工厂模式 建造者模式 原型模式 ","date":"Apr 14, 2022","objectID":"/k8s-golang-design-pattern/:1:0","series":null,"tags":["golang","k8s"],"title":"Kubernetes中的Golang设计模式","uri":"/k8s-golang-design-pattern/#创建型模式"},{"categories":["code"],"content":" 单例模式单例模式用来保证一个类只有一个实例，并提供调用它的一个全局访问点。单例模式是设计模式中最简单，使用最广的一个，通常用来创建一个共享的实例，比如数据库连接池、线程池等。 单例模式分为懒汉式（使用时创建，延迟调用）与饿汉式（初始化时创建），通常我们使用once.Do来实现懒汉式，保证其线程安全。 在kubeadm中使用了单例模式来创建用户与用户组 https://github.com/kubernetes/kubernetes/tree/master/cmd/kubeadm/app/util/staticpod/utils.go var ( usersAndGroups *users.UsersAndGroups usersAndGroupsOnce sync.Once ) func GetUsersAndGroups() (*users.UsersAndGroups, error) { var err error usersAndGroupsOnce.Do(func() { usersAndGroups, err = users.AddUsersAndGroups() }) return usersAndGroups, err } ","date":"Apr 14, 2022","objectID":"/k8s-golang-design-pattern/:1:1","series":null,"tags":["golang","k8s"],"title":"Kubernetes中的Golang设计模式","uri":"/k8s-golang-design-pattern/#单例模式"},{"categories":["code"],"content":" 工厂模式工厂模式通过一个工厂方法来创建不同的产品，又分为简单工厂、工厂方法、抽象工厂，一般用来创建一类相似的产品，方便扩展。 简单工厂根据不同的输入创建不同的产品，在Golang中采用Newxxx的方式实现。 在kubelet中通过输入同创建不同的认证类型 https://github.com/kubernetes/kubernetes/tree/master/cmd/kubelet/app/auth.go func BuildAuthz(client authorizationclient.AuthorizationV1Interface, authz kubeletconfig.KubeletAuthorization) (authorizer.Authorizer, error) { switch authz.Mode { case kubeletconfig.KubeletAuthorizationModeAlwaysAllow: return authorizerfactory.NewAlwaysAllowAuthorizer(), nil case kubeletconfig.KubeletAuthorizationModeWebhook: if client == nil { return nil, errors.New(\"no client provided, cannot use webhook authorization\") } authorizerConfig := authorizerfactory.DelegatingAuthorizerConfig{ SubjectAccessReviewClient: client, AllowCacheTTL: authz.Webhook.CacheAuthorizedTTL.Duration, DenyCacheTTL: authz.Webhook.CacheUnauthorizedTTL.Duration, WebhookRetryBackoff: genericoptions.DefaultAuthWebhookRetryBackoff(), } return authorizerConfig.New() case \"\": return nil, fmt.Errorf(\"no authorization mode specified\") default: return nil, fmt.Errorf(\"unknown authorization mode %s\", authz.Mode) } } 以及https://github.com/kubernetes/client-go/blob/master/tools/cache/store.go func NewStore(keyFunc KeyFunc) Store { return \u0026cache{ cacheStorage: NewThreadSafeStore(Indexers{}, Indices{}), keyFunc: keyFunc, } } type cache struct { // cacheStorage bears the burden of thread safety for the cache cacheStorage ThreadSafeStore // keyFunc is used to make the key for objects stored in and retrieved from items, and // should be deterministic. keyFunc KeyFunc } type Store interface { Add(obj interface{}) error Update(obj interface{}) error Delete(obj interface{}) error List() []interface{} ListKeys() []string Get(obj interface{}) (item interface{}, exists bool, err error) // GetByKey returns the accumulator associated with the given key GetByKey(key string) (item interface{}, exists bool, err error) Replace([]interface{}, string) error Resync() error } 抽象工厂用来构建复杂的一组产品，在informer的实现中使用了抽象工厂 https://github.com/kubernetes/client-go/blob/master/informers/factory.go // NewSharedInformerFactoryWithOptions constructs a new instance of a SharedInformerFactory with additional options. func NewSharedInformerFactoryWithOptions(client kubernetes.Interface, defaultResync time.Duration, options ...SharedInformerOption) SharedInformerFactory { factory := \u0026sharedInformerFactory{ client: client, namespace: v1.NamespaceAll, defaultResync: defaultResync, informers: make(map[reflect.Type]cache.SharedIndexInformer), startedInformers: make(map[reflect.Type]bool), customResync: make(map[reflect.Type]time.Duration), } // Apply all options for _, opt := range options { factory = opt(factory) } return factory } // SharedInformerFactory provides shared informers for resources in all known // API group versions. type SharedInformerFactory interface { internalinterfaces.SharedInformerFactory ForResource(resource schema.GroupVersionResource) (GenericInformer, error) WaitForCacheSync(stopCh \u003c-chan struct{}) map[reflect.Type]bool Admissionregistration() admissionregistration.Interface Internal() apiserverinternal.Interface Apps() apps.Interface Autoscaling() autoscaling.Interface Batch() batch.Interface Certificates() certificates.Interface Coordination() coordination.Interface Core() core.Interface Discovery() discovery.Interface Events() events.Interface Extensions() extensions.Interface Flowcontrol() flowcontrol.Interface Networking() networking.Interface Node() node.Interface Policy() policy.Interface Rbac() rbac.Interface Scheduling() scheduling.Interface Storage() storage.Interface } func (f *sharedInformerFactory) Apps() apps.Interface { return apps.New(f, f.namespace, f.tweakListOptions) } ","date":"Apr 14, 2022","objectID":"/k8s-golang-design-pattern/:1:2","series":null,"tags":["golang","k8s"],"title":"Kubernetes中的Golang设计模式","uri":"/k8s-golang-design-pattern/#工厂模式"},{"categories":["code"],"content":" 建造者模式建造者模式通过逐步构建复杂的对象，降低创建对象的复杂度。通常多个步骤返回中间对象，最后通过Build完成检验与构建工作。 在controller-runtime中使用了建造者模式来创建controller https://github.com/kubernetes-sigs/controller-runtime/tree/master/pkg/builder // Builder builds a Controller. type Builder struct { forInput ForInput ownsInput []OwnsInput watchesInput []WatchesInput mgr manager.Manager globalPredicates []predicate.Predicate ctrl controller.Controller ctrlOptions controller.Options name string } func (blder *Builder) For(object client.Object, opts ...ForOption) *Builder { if blder.forInput.object != nil { blder.forInput.err = fmt.Errorf(\"For(...) should only be called once, could not assign multiple objects for reconciliation\") return blder } input := ForInput{object: object} for _, opt := range opts { opt.ApplyToFor(\u0026input) } blder.forInput = input return blder } // Watches exposes the lower-level ControllerManagedBy Watches functions through the builder. Consider using // Owns or For instead of Watches directly. // Specified predicates are registered only for given source. func (blder *Builder) Watches(src source.Source, eventhandler handler.EventHandler, opts ...WatchesOption) *Builder { input := WatchesInput{src: src, eventhandler: eventhandler} for _, opt := range opts { opt.ApplyToWatches(\u0026input) } blder.watchesInput = append(blder.watchesInput, input) return blder } // WithOptions overrides the controller options use in doController. Defaults to empty. func (blder *Builder) WithOptions(options controller.Options) *Builder { blder.ctrlOptions = options return blder } // WithLogger overrides the controller options's logger used. func (blder *Builder) WithLogger(log logr.Logger) *Builder { blder.ctrlOptions.Log = log return blder } // Build builds the Application Controller and returns the Controller it created. func (blder *Builder) Build(r reconcile.Reconciler) (controller.Controller, error) { if r == nil { return nil, fmt.Errorf(\"must provide a non-nil Reconciler\") } if blder.mgr == nil { return nil, fmt.Errorf(\"must provide a non-nil Manager\") } if blder.forInput.err != nil { return nil, blder.forInput.err } // Checking the reconcile type exist or not if blder.forInput.object == nil { return nil, fmt.Errorf(\"must provide an object for reconciliation\") } // Set the ControllerManagedBy if err := blder.doController(r); err != nil { return nil, err } // Set the Watch if err := blder.doWatch(); err != nil { return nil, err } return blder.ctrl, nil } ","date":"Apr 14, 2022","objectID":"/k8s-golang-design-pattern/:1:3","series":null,"tags":["golang","k8s"],"title":"Kubernetes中的Golang设计模式","uri":"/k8s-golang-design-pattern/#建造者模式"},{"categories":["code"],"content":" 原型模式原型模式用来解决对象复制问题，通过Clone方法，返回对象的复制品。将实现细节与使用解耦。 在k8s中所有资源都需要使用DeepCopy接口即原型模式 https://github.com/kubernetes/kubernetes/blob/master/staging/src/k8s.io/api/core/v1/zz_generated.deepcopy.go // DeepCopyInto is an autogenerated deepcopy function, copying the receiver, writing into out. in must be non-nil. func (in *Pod) DeepCopyInto(out *Pod) { *out = *in out.TypeMeta = in.TypeMeta in.ObjectMeta.DeepCopyInto(\u0026out.ObjectMeta) in.Spec.DeepCopyInto(\u0026out.Spec) in.Status.DeepCopyInto(\u0026out.Status) return } // DeepCopy is an autogenerated deepcopy function, copying the receiver, creating a new Pod. func (in *Pod) DeepCopy() *Pod { if in == nil { return nil } out := new(Pod) in.DeepCopyInto(out) return out } ","date":"Apr 14, 2022","objectID":"/k8s-golang-design-pattern/:1:4","series":null,"tags":["golang","k8s"],"title":"Kubernetes中的Golang设计模式","uri":"/k8s-golang-design-pattern/#原型模式"},{"categories":["code"],"content":" 结构型模式结构型模式通过将对象组合成更大的结构，从而提供系统的灵活性。包括： 适配器模式 桥接模式 组合模式 代理模式 外观模式 装饰模式 享元模式 ","date":"Apr 14, 2022","objectID":"/k8s-golang-design-pattern/:2:0","series":null,"tags":["golang","k8s"],"title":"Kubernetes中的Golang设计模式","uri":"/k8s-golang-design-pattern/#结构型模式"},{"categories":["code"],"content":" 适配器模式通过适配器模式能使不兼容的对象相互协作，通常做一些兼容性工作（老版本、外部服务）时会使用到。 k8s中有很多适配器的例子, 通过Adapter去包裹其他对象转换成统一的接口 https://github.com/kubernetes/kubernetes/blob/master/pkg/controller/replication/conversion.go // informerAdapter implements ReplicaSetInformer by wrapping ReplicationControllerInformer // and converting objects. type informerAdapter struct { rcInformer coreinformers.ReplicationControllerInformer } func (i informerAdapter) Informer() cache.SharedIndexInformer { return conversionInformer{i.rcInformer.Informer()} } func (i informerAdapter) Lister() appslisters.ReplicaSetLister { return conversionLister{i.rcInformer.Lister()} } https://github.com/kubernetes/client-go/blob/master/tools/events/event_broadcaster.go type eventBroadcasterAdapterImpl struct { coreClient typedv1core.EventsGetter coreBroadcaster record.EventBroadcaster eventsv1Client typedeventsv1.EventsV1Interface eventsv1Broadcaster EventBroadcaster } // NewEventBroadcasterAdapter creates a wrapper around new and legacy broadcasters to simplify // migration of individual components to the new Event API. func NewEventBroadcasterAdapter(client clientset.Interface) EventBroadcasterAdapter { eventClient := \u0026eventBroadcasterAdapterImpl{} if _, err := client.Discovery().ServerResourcesForGroupVersion(eventsv1.SchemeGroupVersion.String()); err == nil { eventClient.eventsv1Client = client.EventsV1() eventClient.eventsv1Broadcaster = NewBroadcaster(\u0026EventSinkImpl{Interface: eventClient.eventsv1Client}) } // Even though there can soon exist cases when coreBroadcaster won't really be needed, // we create it unconditionally because its overhead is minor and will simplify using usage // patterns of this library in all components. eventClient.coreClient = client.CoreV1() eventClient.coreBroadcaster = record.NewBroadcaster() return eventClient } // StartRecordingToSink starts sending events received from the specified eventBroadcaster to the given sink. func (e *eventBroadcasterAdapterImpl) StartRecordingToSink(stopCh \u003c-chan struct{}) { if e.eventsv1Broadcaster != nil \u0026\u0026 e.eventsv1Client != nil { e.eventsv1Broadcaster.StartRecordingToSink(stopCh) } if e.coreBroadcaster != nil \u0026\u0026 e.coreClient != nil { e.coreBroadcaster.StartRecordingToSink(\u0026typedv1core.EventSinkImpl{Interface: e.coreClient.Events(\"\")}) } } func (e *eventBroadcasterAdapterImpl) NewRecorder(name string) EventRecorder { if e.eventsv1Broadcaster != nil \u0026\u0026 e.eventsv1Client != nil { return e.eventsv1Broadcaster.NewRecorder(scheme.Scheme, name) } return record.NewEventRecorderAdapter(e.DeprecatedNewLegacyRecorder(name)) } ","date":"Apr 14, 2022","objectID":"/k8s-golang-design-pattern/:2:1","series":null,"tags":["golang","k8s"],"title":"Kubernetes中的Golang设计模式","uri":"/k8s-golang-design-pattern/#适配器模式"},{"categories":["code"],"content":" 桥接模式桥接模式将实现与抽象解耦，可提供系统的系统的灵活性与可扩展性。 在k8s中大量使用，如DiscoveryClient的实现 https://github.com/kubernetes/client-go/blob/master/discovery/discovery_client.go type DiscoveryClient struct { restClient restclient.Interface LegacyPrefix string } type DiscoveryInterface interface { RESTClient() restclient.Interface ServerGroupsInterface ServerResourcesInterface ServerVersionInterface OpenAPISchemaInterface OpenAPIV3SchemaInterface } // NewDiscoveryClient returns a new DiscoveryClient for the given RESTClient. func NewDiscoveryClient(c restclient.Interface) *DiscoveryClient { return \u0026DiscoveryClient{restClient: c, LegacyPrefix: \"/api\"} } // RESTClient returns a RESTClient that is used to communicate // with API server by this client implementation. func (d *DiscoveryClient) RESTClient() restclient.Interface { if d == nil { return nil } return d.restClient } ","date":"Apr 14, 2022","objectID":"/k8s-golang-design-pattern/:2:2","series":null,"tags":["golang","k8s"],"title":"Kubernetes中的Golang设计模式","uri":"/k8s-golang-design-pattern/#桥接模式"},{"categories":["code"],"content":" 组合模式组合模式通过组合小对象形成更大的结构，并且具有相同的接口。和Golang中的组合非常相似，使用也非常广泛。 https://github.com/kubernetes-sigs/controller-runtime/tree/master/pkg/cache/cache.go // Cache knows how to load Kubernetes objects, fetch informers to request // to receive events for Kubernetes objects (at a low-level), // and add indices to fields on the objects stored in the cache. type Cache interface { // Cache acts as a client to objects stored in the cache. client.Reader // Cache loads informers and adds field indices. Informers } type Informers interface { GetInformer(ctx context.Context, obj client.Object) (Informer, error) GetInformerForKind(ctx context.Context, gvk schema.GroupVersionKind) (Informer, error) Start(ctx context.Context) error WaitForCacheSync(ctx context.Context) bool client.FieldIndexer } ","date":"Apr 14, 2022","objectID":"/k8s-golang-design-pattern/:2:3","series":null,"tags":["golang","k8s"],"title":"Kubernetes中的Golang设计模式","uri":"/k8s-golang-design-pattern/#组合模式"},{"categories":["code"],"content":" 代理模式代理模式通过代理来替代真实服务，通常代理类与真实类具有相同的接口，在代理类中可以做一些额外操作（访问控制、缓存等） 在k8s中通过代理来实现访问Node、Pod、Service。 ","date":"Apr 14, 2022","objectID":"/k8s-golang-design-pattern/:2:4","series":null,"tags":["golang","k8s"],"title":"Kubernetes中的Golang设计模式","uri":"/k8s-golang-design-pattern/#代理模式"},{"categories":["code"],"content":" 外观模式外观模式通过一个高度抽象的接口，使子系统更加容器使用，使用也很广泛 比如controller-runtime中创建时controllerManager时调用了很多子系统，使用时只需通过GetClient()便可得到Client // New returns a new Manager for creating Controllers. func New(config *rest.Config, options Options) (Manager, error) { // Set default values for options fields options = setOptionsDefaults(options) cluster, err := cluster.New(config, func(clusterOptions *cluster.Options) { clusterOptions.Scheme = options.Scheme clusterOptions.MapperProvider = options.MapperProvider clusterOptions.Logger = options.Logger clusterOptions.SyncPeriod = options.SyncPeriod clusterOptions.Namespace = options.Namespace clusterOptions.NewCache = options.NewCache clusterOptions.NewClient = options.NewClient clusterOptions.ClientDisableCacheFor = options.ClientDisableCacheFor clusterOptions.DryRunClient = options.DryRunClient clusterOptions.EventBroadcaster = options.EventBroadcaster //nolint:staticcheck }) if err != nil { return nil, err } //... return \u0026controllerManager{ cluster: cluster, //... }, nil } func (c *cluster) GetClient() client.Client { return c.client } ","date":"Apr 14, 2022","objectID":"/k8s-golang-design-pattern/:2:5","series":null,"tags":["golang","k8s"],"title":"Kubernetes中的Golang设计模式","uri":"/k8s-golang-design-pattern/#外观模式"},{"categories":["code"],"content":" 装饰模式装饰模式通过原有对象多次包装从而添加新功能，典型的一些Http中间件实现（日志、认证） admission中装饰器的使用 https://github.com/kubernetes/kubernetes/blob/master/staging/src/k8s.io/apiserver/pkg/admission/decorator.go type Decorator interface { Decorate(handler Interface, name string) Interface } type DecoratorFunc func(handler Interface, name string) Interface func (d DecoratorFunc) Decorate(handler Interface, name string) Interface { return d(handler, name) } type Decorators []Decorator // Decorate applies the decorator in inside-out order, i.e. the first decorator in the slice is first applied to the given handler. func (d Decorators) Decorate(handler Interface, name string) Interface { result := handler for _, d := range d { result = d.Decorate(result, name) } return result } ","date":"Apr 14, 2022","objectID":"/k8s-golang-design-pattern/:2:6","series":null,"tags":["golang","k8s"],"title":"Kubernetes中的Golang设计模式","uri":"/k8s-golang-design-pattern/#装饰模式"},{"categories":["code"],"content":" 享元模式享元模式通过共享多个对象共有的子对象，从而节省内存。如连接池、对象池的实现等，在Golang中通过sync.Pool可实现对象复用即享元模式。 在apiserver/endpoints中通过共享gzip对象，减少内存分配以及gc时间 var gzipPool = \u0026sync.Pool{ New: func() interface{} { gw, err := gzip.NewWriterLevel(nil, defaultGzipContentEncodingLevel) if err != nil { panic(err) } return gw }, } func (w *deferredResponseWriter) Write(p []byte) (n int, err error) { // ... hw := w.hw header := hw.Header() switch { case w.contentEncoding == \"gzip\" \u0026\u0026 len(p) \u003e defaultGzipThresholdBytes: header.Set(\"Content-Encoding\", \"gzip\") header.Add(\"Vary\", \"Accept-Encoding\") gw := gzipPool.Get().(*gzip.Writer) gw.Reset(hw) w.w = gw default: w.w = hw } header.Set(\"Content-Type\", w.mediaType) hw.WriteHeader(w.statusCode) return w.w.Write(p) } func (w *deferredResponseWriter) Close() error { if !w.hasWritten { return nil } var err error switch t := w.w.(type) { case *gzip.Writer: err = t.Close() t.Reset(nil) gzipPool.Put(t) } return err } ","date":"Apr 14, 2022","objectID":"/k8s-golang-design-pattern/:2:7","series":null,"tags":["golang","k8s"],"title":"Kubernetes中的Golang设计模式","uri":"/k8s-golang-design-pattern/#享元模式"},{"categories":["code"],"content":" 行为型模式行为型模式负责对象间的通信和职责委派，常用的包括： 观察者模式 中介者模式 命令模式 迭代器模式 策略模式 状态模式 备忘录模式 职责链模式 访问者模式 解释器模式 ","date":"Apr 14, 2022","objectID":"/k8s-golang-design-pattern/:3:0","series":null,"tags":["golang","k8s"],"title":"Kubernetes中的Golang设计模式","uri":"/k8s-golang-design-pattern/#行为型模式"},{"categories":["code"],"content":" 观察者模式观察者模式允许观察者订阅事件，当事件触发时会通知观察对象。 在shardInformer订阅事件时使用了观察者模式 https://github.com/kubernetes/client-go/blob/master/tools/cache/shared_informer.go func (s *sharedIndexInformer) AddEventHandlerWithResyncPeriod(handler ResourceEventHandler, resyncPeriod time.Duration) { //... s.processor.addListener(listener) for _, item := range s.indexer.List() { listener.add(addNotification{newObj: item}) } } // 事件触发时通知所有对象 func (s *sharedIndexInformer) OnAdd(obj interface{}) { // Invocation of this function is locked under s.blockDeltas, so it is // save to distribute the notification s.cacheMutationDetector.AddObject(obj) s.processor.distribute(addNotification{newObj: obj}, false) } func (p *sharedProcessor) distribute(obj interface{}, sync bool) { p.listenersLock.RLock() defer p.listenersLock.RUnlock() if sync { for _, listener := range p.syncingListeners { listener.add(obj) } } else { for _, listener := range p.listeners { listener.add(obj) } } } ","date":"Apr 14, 2022","objectID":"/k8s-golang-design-pattern/:3:1","series":null,"tags":["golang","k8s"],"title":"Kubernetes中的Golang设计模式","uri":"/k8s-golang-design-pattern/#观察者模式"},{"categories":["code"],"content":" 命令模式命令模式通过将请求封装为对象，方便存储调用。 在k8s中所有组件启动都是通过github.com/spf13/cobra工具包 https://github.com/kubernetes/kubernetes/blob/master/cmd/kube-apiserver/apiserver.go func main() { command := app.NewAPIServerCommand() code := cli.Run(command) os.Exit(code) } ","date":"Apr 14, 2022","objectID":"/k8s-golang-design-pattern/:3:2","series":null,"tags":["golang","k8s"],"title":"Kubernetes中的Golang设计模式","uri":"/k8s-golang-design-pattern/#命令模式"},{"categories":["code"],"content":" 迭代器模式迭代器允许顺序遍历复杂的数据结构而不暴露其内部细节。通常通过Next方法来迭代下一个对象。 k8s在对象序列化时使用了迭代器。 ","date":"Apr 14, 2022","objectID":"/k8s-golang-design-pattern/:3:3","series":null,"tags":["golang","k8s"],"title":"Kubernetes中的Golang设计模式","uri":"/k8s-golang-design-pattern/#迭代器模式"},{"categories":["code"],"content":" 策略模式策略模式通过定义一系列算法，允许运行时可替换算法，从而实现算法分离。 策略模式与桥接模式非常像，只是桥接模式的抽象程度更高一点。 https://github.com/kubernetes/kubernetes/blob/master/pkg/registry/admissionregistration/mutatingwebhookconfiguration/storage/storage.go // NewREST returns a RESTStorage object that will work against mutatingWebhookConfiguration. func NewREST(optsGetter generic.RESTOptionsGetter) (*REST, error) { store := \u0026genericregistry.Store{ NewFunc: func() runtime.Object { return \u0026admissionregistration.MutatingWebhookConfiguration{} }, NewListFunc: func() runtime.Object { return \u0026admissionregistration.MutatingWebhookConfigurationList{} }, ObjectNameFunc: func(obj runtime.Object) (string, error) { return obj.(*admissionregistration.MutatingWebhookConfiguration).Name, nil }, DefaultQualifiedResource: admissionregistration.Resource(\"mutatingwebhookconfigurations\"), CreateStrategy: mutatingwebhookconfiguration.Strategy, UpdateStrategy: mutatingwebhookconfiguration.Strategy, DeleteStrategy: mutatingwebhookconfiguration.Strategy, TableConvertor: printerstorage.TableConvertor{TableGenerator: printers.NewTableGenerator().With(printersinternal.AddHandlers)}, } options := \u0026generic.StoreOptions{RESTOptions: optsGetter} if err := store.CompleteWithOptions(options); err != nil { return nil, err } return \u0026REST{store}, nil } ","date":"Apr 14, 2022","objectID":"/k8s-golang-design-pattern/:3:4","series":null,"tags":["golang","k8s"],"title":"Kubernetes中的Golang设计模式","uri":"/k8s-golang-design-pattern/#策略模式"},{"categories":["code"],"content":" 状态模式状态模式将状态与行为分离，例如状态机的实现 如在容器运行时的接口中，可以获取容器状态 type Runtime interface { //... Status() (*RuntimeStatus, error) // SyncPod syncs the running pod into the desired pod. SyncPod(pod *v1.Pod, podStatus *PodStatus, pullSecrets []v1.Secret, backOff *flowcontrol.Backoff) PodSyncResult KillPod(pod *v1.Pod, runningPod Pod, gracePeriodOverride *int64) error DeleteContainer(containerID ContainerID) error //... } ","date":"Apr 14, 2022","objectID":"/k8s-golang-design-pattern/:3:5","series":null,"tags":["golang","k8s"],"title":"Kubernetes中的Golang设计模式","uri":"/k8s-golang-design-pattern/#状态模式"},{"categories":["code"],"content":" 备忘录模式备忘录模式可以保存程序内部状态到外部，又不希望暴露内部状态的情形。例如快照可保存对象状态，用于恢复。 https://github.com/kubernetes/kubernetes/blob/master/pkg/registry/core/service/ipallocator/allocator.go // NewFromSnapshot allocates a Range and initializes it from a snapshot. func NewFromSnapshot(snap *api.RangeAllocation) (*Range, error) { _, ipnet, err := netutils.ParseCIDRSloppy(snap.Range) if err != nil { return nil, err } r, err := NewInMemory(ipnet) if err != nil { return nil, err } if err := r.Restore(ipnet, snap.Data); err != nil { return nil, err } return r, nil } ","date":"Apr 14, 2022","objectID":"/k8s-golang-design-pattern/:3:6","series":null,"tags":["golang","k8s"],"title":"Kubernetes中的Golang设计模式","uri":"/k8s-golang-design-pattern/#备忘录模式"},{"categories":["code"],"content":" 职责链模式通过职责链分离不同的功能，可以动态组合。与装饰模式很相似，实际使用中也不需要区分其差异。 在apiserver的handler实现中，通过职责链来增加认证、授权、限流等操作 https://github.com/kubernetes/kubernetes/blob/master/staging/src/k8s.io/apiserver/pkg/server/config.go func DefaultBuildHandlerChain(apiHandler http.Handler, c *Config) http.Handler { handler := filterlatency.TrackCompleted(apiHandler) handler = genericapifilters.WithAuthorization(handler, c.Authorization.Authorizer, c.Serializer) handler = filterlatency.TrackStarted(handler, \"authorization\") if c.FlowControl != nil { requestWorkEstimator := flowcontrolrequest.NewWorkEstimator(c.StorageObjectCountTracker.Get, c.FlowControl.GetInterestedWatchCount) handler = filterlatency.TrackCompleted(handler) handler = genericfilters.WithPriorityAndFairness(handler, c.LongRunningFunc, c.FlowControl, requestWorkEstimator) handler = filterlatency.TrackStarted(handler, \"priorityandfairness\") } else { handler = genericfilters.WithMaxInFlightLimit(handler, c.MaxRequestsInFlight, c.MaxMutatingRequestsInFlight, c.LongRunningFunc) } //... handler = genericapifilters.WithLatencyTrackers(handler) handler = genericapifilters.WithRequestInfo(handler, c.RequestInfoResolver) handler = genericapifilters.WithRequestReceivedTimestamp(handler) handler = genericapifilters.WithMuxAndDiscoveryComplete(handler, c.lifecycleSignals.MuxAndDiscoveryComplete.Signaled()) handler = genericfilters.WithPanicRecovery(handler, c.RequestInfoResolver) handler = genericapifilters.WithAuditID(handler) return handler } ","date":"Apr 14, 2022","objectID":"/k8s-golang-design-pattern/:3:7","series":null,"tags":["golang","k8s"],"title":"Kubernetes中的Golang设计模式","uri":"/k8s-golang-design-pattern/#职责链模式"},{"categories":["code"],"content":" 访问者模式访问者模式可以给一系列对象透明的添加功能，并且把相关代码封装到一个类中, 对象只要预留访问者接口Accept则后期为对象添加功能的时就不需要改动对象。 例如动物园内有多个场馆，有些场馆（熊猫馆、海洋馆）需要单独收费，那么每个场馆（对象）可以通过Accept接待游客（Vistor）。访问者模式的关键是将对象的操作分离出来形成单独的类，对象可以选择对应的操作。 在kubectl中使用访问者模式，通过不同的访问者实现不同的参数，从而拼接成Rest请求。 https://github.com/kubernetes/kubernetes/blob/master/staging/src/k8s.io/kubectl/pkg/apps/kind_visitor.go#L39 type KindVisitor interface { VisitDaemonSet(kind GroupKindElement) VisitDeployment(kind GroupKindElement) VisitJob(kind GroupKindElement) VisitPod(kind GroupKindElement) VisitReplicaSet(kind GroupKindElement) VisitReplicationController(kind GroupKindElement) VisitStatefulSet(kind GroupKindElement) VisitCronJob(kind GroupKindElement) } // GroupKindElement defines a Kubernetes API group elem type GroupKindElement schema.GroupKind // Accept calls the Visit method on visitor that corresponds to elem's Kind func (elem GroupKindElement) Accept(visitor KindVisitor) error { switch { case elem.GroupMatch(\"apps\", \"extensions\") \u0026\u0026 elem.Kind == \"DaemonSet\": visitor.VisitDaemonSet(elem) case elem.GroupMatch(\"apps\", \"extensions\") \u0026\u0026 elem.Kind == \"Deployment\": visitor.VisitDeployment(elem) case elem.GroupMatch(\"batch\") \u0026\u0026 elem.Kind == \"Job\": visitor.VisitJob(elem) case elem.GroupMatch(\"\", \"core\") \u0026\u0026 elem.Kind == \"Pod\": visitor.VisitPod(elem) case elem.GroupMatch(\"apps\", \"extensions\") \u0026\u0026 elem.Kind == \"ReplicaSet\": visitor.VisitReplicaSet(elem) case elem.GroupMatch(\"\", \"core\") \u0026\u0026 elem.Kind == \"ReplicationController\": visitor.VisitReplicationController(elem) case elem.GroupMatch(\"apps\") \u0026\u0026 elem.Kind == \"StatefulSet\": visitor.VisitStatefulSet(elem) case elem.GroupMatch(\"batch\") \u0026\u0026 elem.Kind == \"CronJob\": visitor.VisitCronJob(elem) default: return fmt.Errorf(\"no visitor method exists for %v\", elem) } return nil } ","date":"Apr 14, 2022","objectID":"/k8s-golang-design-pattern/:3:8","series":null,"tags":["golang","k8s"],"title":"Kubernetes中的Golang设计模式","uri":"/k8s-golang-design-pattern/#访问者模式"},{"categories":["code"],"content":" 总结K8s中包含了不少经典设计模式的例子，部分没找到合适的例子便没有提及。实际使用过程中可能多种模式都有涉及，或者是一些变种，不简单的是严格的标准定义，学会灵活应用才能提高的代码质量。 ","date":"Apr 14, 2022","objectID":"/k8s-golang-design-pattern/:4:0","series":null,"tags":["golang","k8s"],"title":"Kubernetes中的Golang设计模式","uri":"/k8s-golang-design-pattern/#总结"},{"categories":["code"],"content":" 引用 https://github.com/kubernetes/kubernetes https://aly.arriqaaq.com/golang-design-patterns/ Explore more in https://qingwave.github.io ","date":"Apr 14, 2022","objectID":"/k8s-golang-design-pattern/:5:0","series":null,"tags":["golang","k8s"],"title":"Kubernetes中的Golang设计模式","uri":"/k8s-golang-design-pattern/#引用"},{"categories":["cloud"],"content":"CNI(Container Network Interface) 即容器的网络API接口，在Kubernetes中通过CNI来扩展网络功能，今天我们从零开始实现一个自己的CNI网络插件。 本文所有代码见: https://github.com/qingwave/mycni ","date":"Apr 01, 2022","objectID":"/how-to-write-k8s-cni/:0:0","series":null,"tags":["k8s","cni"],"title":"手写一个Kubernetes CNI网络插件","uri":"/how-to-write-k8s-cni/#"},{"categories":["cloud"],"content":" CNI简介Kubernetes提供了很多扩展点，通过CNI网络插件可以支持不同的网络设施，大大提供了系统的灵活性，目前也已成为容器网络领域的标准。 Kubernetes与CNI的交互逻辑如下： Kubelet监听到Pod调度到当前节点后，通过rpc调用CRI(containerd, cri-o等)，CRI创建Sandbox容器，初始化Cgroup与Namespace，然后再调用CNI插件分配IP，最后完成容器创建与启动。 不同于CRI、CSI通过rpc通信，CNI是通过二进制接口调用的，通过环境变量和标准输入传递具体网络配置，下图为Flannel CNI插件的工作流程，通过链式调用CNI插件实现对Pod的IP分配、网络配置： ","date":"Apr 01, 2022","objectID":"/how-to-write-k8s-cni/:1:0","series":null,"tags":["k8s","cni"],"title":"手写一个Kubernetes CNI网络插件","uri":"/how-to-write-k8s-cni/#cni简介"},{"categories":["cloud"],"content":" 实现CNI实现一个完整的K8s CNI插件需要满足以下几点要求： Pod IP分配，即IPAM功能 节点与其上所有Pod网络互通，以实现健康检查 集群内所有Pod可通信，包括同节点与不同节点 其他功能的支持，比如hostPort、兼容kube-proxy的iptables规则等 我们主要实现前三点需求，通过Linux Bridge、Veth Pair以及路由来实现K8s网络方案。 网络架构如下： 包括两个组件： mycni: CNI插件，实现IPAM，为Pod分配IP配置路由，通过网桥实现同节点上不同Pod的通信 mycnid: 节点上守护进程，监听K8s Node，获取各个节点CIDR写入路由 ","date":"Apr 01, 2022","objectID":"/how-to-write-k8s-cni/:2:0","series":null,"tags":["k8s","cni"],"title":"手写一个Kubernetes CNI网络插件","uri":"/how-to-write-k8s-cni/#实现cni"},{"categories":["cloud"],"content":" MycniCNI官方已经提供了工具包，我们只需要实现cmdAdd, cmdCheck, cmdDel接口即可实现一个CNI插件。 func PluginMainWithError(cmdAdd, cmdCheck, cmdDel func(_ *CmdArgs) error, versionInfo version.PluginInfo, about string) *types.Error { return (\u0026dispatcher{ Getenv: os.Getenv, Stdin: os.Stdin, Stdout: os.Stdout, Stderr: os.Stderr, }).pluginMain(cmdAdd, cmdCheck, cmdDel, versionInfo, about) } 我们主要关注网络创建过程，实现IP分配与网桥配置： func cmdAdd(args *skel.CmdArgs) error { // 加载配置 conf, err := config.LoadCNIConfig(args.StdinData) // 存储本机IP分配列表 s, err := store.NewStore(conf.DataDir, conf.Name) defer s.Close() // ipam服务，分配ip ipam, err := ipam.NewIPAM(conf, s) gateway := ipam.Gateway() ip, err := ipam.AllocateIP(args.ContainerID, args.IfName) // 创建网桥，虚拟设备，并绑定到网桥 br, err := bridge.CreateBridge(conf.Bridge, mtu, ipam.IPNet(gateway)) bridge.SetupVeth(netns, br, mtu, args.IfName, ipam.IPNet(ip), gateway) // 返回网络配置信息 result := \u0026current.Result{ CNIVersion: current.ImplementedSpecVersion, IPs: []*current.IPConfig{ { Address: net.IPNet{IP: ip, Mask: ipam.Mask()}, Gateway: gateway, }, }, } return types.PrintResult(result, conf.CNIVersion) } IPAMIPAM服务需要保证为Pod分配唯一的IP，K8s会为每个节点分配PodCIDR，只需要保证节点上所有Pod IP不冲突即可。 通过本地文件来存储已分配的IP，当新Pod创建时只需要检查已分配IP，通过CIDR取一个未使用的IP。通常做法是将IP信息存储在数据库中(etcd)，简单期间本文只使用文件存储。 首先需要保证并发请求时，IP分配不会冲突，可通过文件锁实现，存储实现如下： type data struct { IPs map[string]containerNetINfo `json:\"ips\"` // 存储IP信息 Last string `json:\"last\"`// 上一个分配IP } type Store struct { *filemutex.FileMutex // 文件锁 dir string data *data dataFile string } 分配IP代码： func (im *IPAM) AllocateIP(id, ifName string) (net.IP, error) { im.store.Lock() // 上锁，防止冲突 defer im.store.Unlock() im.store.LoadData(); // 加载存储中IP数据 // 根据容器id查询ip是否已分配 ip, _ := im.store.GetIPByID(id) if len(ip) \u003e 0 { return ip, nil } // 从上次已分配IP开始，依次检查，如果IP未使用则添加到文件中 start := make(net.IP, len(last)) for { next, err := im.NextIP(start) if err == IPOverflowError \u0026\u0026 !last.Equal(im.gateway) { start = im.gateway continue } else if err != nil { return nil, err } // 分配IP if !im.store.Contain(next) { err := im.store.Add(next, id, ifName) return next, err } start = next if start.Equal(last) { break } fmt.Printf(\"ip: %s\", next) } return nil, fmt.Errorf(\"no available ip\") } IP分配过程如下： 加文件锁，读取已分配IP 从上一个分配IP开始，遍历判断是否未分配 如果IP未使用，存储到文件中并返回 节点内通信节点内通信通过网桥实现，创建一个虚拟设备对，分别绑定到Pod所在Namespace与网桥上，绑定IPAM分配的IP, 并设置默认路由。从而实现同一个节点上，Node-\u003ePod与Pod之间的通信。 首先，如果网桥不存在则通过netlink库创建 func CreateBridge(bridge string, mtu int, gateway *net.IPNet) (netlink.Link, error) { if l, _ := netlink.LinkByName(bridge); l != nil { return l, nil } br := \u0026netlink.Bridge{ LinkAttrs: netlink.LinkAttrs{ Name: bridge, MTU: mtu, TxQLen: -1, }, } if err := netlink.LinkAdd(br); err != nil \u0026\u0026 err != syscall.EEXIST { return nil, err } dev, err := netlink.LinkByName(bridge) if err != nil { return nil, err } // 添加地址，即Pod默认网关地址 if err := netlink.AddrAdd(dev, \u0026netlink.Addr{IPNet: gateway}); err != nil { return nil, err } // 启动网桥 if err := netlink.LinkSetUp(dev); err != nil { return nil, err } return dev, nil } 为容器创建虚拟网卡 func SetupVeth(netns ns.NetNS, br netlink.Link, mtu int, ifName string, podIP *net.IPNet, gateway net.IP) error { hostIface := \u0026current.Interface{} err := netns.Do(func(hostNS ns.NetNS) error { // 在容器网络空间创建虚拟网卡 hostVeth, containerVeth, err := ip.SetupVeth(ifName, mtu, \"\", hostNS) if err != nil { return err } hostIface.Name = hostVeth.Name // set ip for container veth conLink, err := netlink.LinkByName(containerVeth.Name) if err != nil { return err } // 绑定Pod IP if err := netlink.AddrAdd(conLink, \u0026netlink.Addr{IPNet: podIP}); err != nil { return err } // 启动网卡 if err := netlink.LinkSetUp(conLink); err != nil { return err } // 添加默认路径，网关即网桥的地址 if err := ip.AddDefaultRoute(gateway, conLink); err != nil { return err } return nil }) // need to lookup hostVeth again as its index has changed during ns move hostVeth, err := netlink.LinkByName(hostIface.Name) if err != nil { return fmt.Errorf(\"failed to lookup %q: %v\", hostIface.Name, err) } // 将虚拟网卡另一端绑定到网桥上 if err := netlink.LinkSetMaster(hostVeth, br); err != nil {","date":"Apr 01, 2022","objectID":"/how-to-write-k8s-cni/:2:1","series":null,"tags":["k8s","cni"],"title":"手写一个Kubernetes CNI网络插件","uri":"/how-to-write-k8s-cni/#mycni"},{"categories":["cloud"],"content":" MycniCNI官方已经提供了工具包，我们只需要实现cmdAdd, cmdCheck, cmdDel接口即可实现一个CNI插件。 func PluginMainWithError(cmdAdd, cmdCheck, cmdDel func(_ *CmdArgs) error, versionInfo version.PluginInfo, about string) *types.Error { return (\u0026dispatcher{ Getenv: os.Getenv, Stdin: os.Stdin, Stdout: os.Stdout, Stderr: os.Stderr, }).pluginMain(cmdAdd, cmdCheck, cmdDel, versionInfo, about) } 我们主要关注网络创建过程，实现IP分配与网桥配置： func cmdAdd(args *skel.CmdArgs) error { // 加载配置 conf, err := config.LoadCNIConfig(args.StdinData) // 存储本机IP分配列表 s, err := store.NewStore(conf.DataDir, conf.Name) defer s.Close() // ipam服务，分配ip ipam, err := ipam.NewIPAM(conf, s) gateway := ipam.Gateway() ip, err := ipam.AllocateIP(args.ContainerID, args.IfName) // 创建网桥，虚拟设备，并绑定到网桥 br, err := bridge.CreateBridge(conf.Bridge, mtu, ipam.IPNet(gateway)) bridge.SetupVeth(netns, br, mtu, args.IfName, ipam.IPNet(ip), gateway) // 返回网络配置信息 result := ¤t.Result{ CNIVersion: current.ImplementedSpecVersion, IPs: []*current.IPConfig{ { Address: net.IPNet{IP: ip, Mask: ipam.Mask()}, Gateway: gateway, }, }, } return types.PrintResult(result, conf.CNIVersion) } IPAMIPAM服务需要保证为Pod分配唯一的IP，K8s会为每个节点分配PodCIDR，只需要保证节点上所有Pod IP不冲突即可。 通过本地文件来存储已分配的IP，当新Pod创建时只需要检查已分配IP，通过CIDR取一个未使用的IP。通常做法是将IP信息存储在数据库中(etcd)，简单期间本文只使用文件存储。 首先需要保证并发请求时，IP分配不会冲突，可通过文件锁实现，存储实现如下： type data struct { IPs map[string]containerNetINfo `json:\"ips\"` // 存储IP信息 Last string `json:\"last\"`// 上一个分配IP } type Store struct { *filemutex.FileMutex // 文件锁 dir string data *data dataFile string } 分配IP代码： func (im *IPAM) AllocateIP(id, ifName string) (net.IP, error) { im.store.Lock() // 上锁，防止冲突 defer im.store.Unlock() im.store.LoadData(); // 加载存储中IP数据 // 根据容器id查询ip是否已分配 ip, _ := im.store.GetIPByID(id) if len(ip) \u003e 0 { return ip, nil } // 从上次已分配IP开始，依次检查，如果IP未使用则添加到文件中 start := make(net.IP, len(last)) for { next, err := im.NextIP(start) if err == IPOverflowError \u0026\u0026 !last.Equal(im.gateway) { start = im.gateway continue } else if err != nil { return nil, err } // 分配IP if !im.store.Contain(next) { err := im.store.Add(next, id, ifName) return next, err } start = next if start.Equal(last) { break } fmt.Printf(\"ip: %s\", next) } return nil, fmt.Errorf(\"no available ip\") } IP分配过程如下： 加文件锁，读取已分配IP 从上一个分配IP开始，遍历判断是否未分配 如果IP未使用，存储到文件中并返回 节点内通信节点内通信通过网桥实现，创建一个虚拟设备对，分别绑定到Pod所在Namespace与网桥上，绑定IPAM分配的IP, 并设置默认路由。从而实现同一个节点上，Node-\u003ePod与Pod之间的通信。 首先，如果网桥不存在则通过netlink库创建 func CreateBridge(bridge string, mtu int, gateway *net.IPNet) (netlink.Link, error) { if l, _ := netlink.LinkByName(bridge); l != nil { return l, nil } br := \u0026netlink.Bridge{ LinkAttrs: netlink.LinkAttrs{ Name: bridge, MTU: mtu, TxQLen: -1, }, } if err := netlink.LinkAdd(br); err != nil \u0026\u0026 err != syscall.EEXIST { return nil, err } dev, err := netlink.LinkByName(bridge) if err != nil { return nil, err } // 添加地址，即Pod默认网关地址 if err := netlink.AddrAdd(dev, \u0026netlink.Addr{IPNet: gateway}); err != nil { return nil, err } // 启动网桥 if err := netlink.LinkSetUp(dev); err != nil { return nil, err } return dev, nil } 为容器创建虚拟网卡 func SetupVeth(netns ns.NetNS, br netlink.Link, mtu int, ifName string, podIP *net.IPNet, gateway net.IP) error { hostIface := ¤t.Interface{} err := netns.Do(func(hostNS ns.NetNS) error { // 在容器网络空间创建虚拟网卡 hostVeth, containerVeth, err := ip.SetupVeth(ifName, mtu, \"\", hostNS) if err != nil { return err } hostIface.Name = hostVeth.Name // set ip for container veth conLink, err := netlink.LinkByName(containerVeth.Name) if err != nil { return err } // 绑定Pod IP if err := netlink.AddrAdd(conLink, \u0026netlink.Addr{IPNet: podIP}); err != nil { return err } // 启动网卡 if err := netlink.LinkSetUp(conLink); err != nil { return err } // 添加默认路径，网关即网桥的地址 if err := ip.AddDefaultRoute(gateway, conLink); err != nil { return err } return nil }) // need to lookup hostVeth again as its index has changed during ns move hostVeth, err := netlink.LinkByName(hostIface.Name) if err != nil { return fmt.Errorf(\"failed to lookup %q: %v\", hostIface.Name, err) } // 将虚拟网卡另一端绑定到网桥上 if err := netlink.LinkSetMaster(hostVeth, br); err != nil {","date":"Apr 01, 2022","objectID":"/how-to-write-k8s-cni/:2:1","series":null,"tags":["k8s","cni"],"title":"手写一个Kubernetes CNI网络插件","uri":"/how-to-write-k8s-cni/#ipam"},{"categories":["cloud"],"content":" MycniCNI官方已经提供了工具包，我们只需要实现cmdAdd, cmdCheck, cmdDel接口即可实现一个CNI插件。 func PluginMainWithError(cmdAdd, cmdCheck, cmdDel func(_ *CmdArgs) error, versionInfo version.PluginInfo, about string) *types.Error { return (\u0026dispatcher{ Getenv: os.Getenv, Stdin: os.Stdin, Stdout: os.Stdout, Stderr: os.Stderr, }).pluginMain(cmdAdd, cmdCheck, cmdDel, versionInfo, about) } 我们主要关注网络创建过程，实现IP分配与网桥配置： func cmdAdd(args *skel.CmdArgs) error { // 加载配置 conf, err := config.LoadCNIConfig(args.StdinData) // 存储本机IP分配列表 s, err := store.NewStore(conf.DataDir, conf.Name) defer s.Close() // ipam服务，分配ip ipam, err := ipam.NewIPAM(conf, s) gateway := ipam.Gateway() ip, err := ipam.AllocateIP(args.ContainerID, args.IfName) // 创建网桥，虚拟设备，并绑定到网桥 br, err := bridge.CreateBridge(conf.Bridge, mtu, ipam.IPNet(gateway)) bridge.SetupVeth(netns, br, mtu, args.IfName, ipam.IPNet(ip), gateway) // 返回网络配置信息 result := ¤t.Result{ CNIVersion: current.ImplementedSpecVersion, IPs: []*current.IPConfig{ { Address: net.IPNet{IP: ip, Mask: ipam.Mask()}, Gateway: gateway, }, }, } return types.PrintResult(result, conf.CNIVersion) } IPAMIPAM服务需要保证为Pod分配唯一的IP，K8s会为每个节点分配PodCIDR，只需要保证节点上所有Pod IP不冲突即可。 通过本地文件来存储已分配的IP，当新Pod创建时只需要检查已分配IP，通过CIDR取一个未使用的IP。通常做法是将IP信息存储在数据库中(etcd)，简单期间本文只使用文件存储。 首先需要保证并发请求时，IP分配不会冲突，可通过文件锁实现，存储实现如下： type data struct { IPs map[string]containerNetINfo `json:\"ips\"` // 存储IP信息 Last string `json:\"last\"`// 上一个分配IP } type Store struct { *filemutex.FileMutex // 文件锁 dir string data *data dataFile string } 分配IP代码： func (im *IPAM) AllocateIP(id, ifName string) (net.IP, error) { im.store.Lock() // 上锁，防止冲突 defer im.store.Unlock() im.store.LoadData(); // 加载存储中IP数据 // 根据容器id查询ip是否已分配 ip, _ := im.store.GetIPByID(id) if len(ip) \u003e 0 { return ip, nil } // 从上次已分配IP开始，依次检查，如果IP未使用则添加到文件中 start := make(net.IP, len(last)) for { next, err := im.NextIP(start) if err == IPOverflowError \u0026\u0026 !last.Equal(im.gateway) { start = im.gateway continue } else if err != nil { return nil, err } // 分配IP if !im.store.Contain(next) { err := im.store.Add(next, id, ifName) return next, err } start = next if start.Equal(last) { break } fmt.Printf(\"ip: %s\", next) } return nil, fmt.Errorf(\"no available ip\") } IP分配过程如下： 加文件锁，读取已分配IP 从上一个分配IP开始，遍历判断是否未分配 如果IP未使用，存储到文件中并返回 节点内通信节点内通信通过网桥实现，创建一个虚拟设备对，分别绑定到Pod所在Namespace与网桥上，绑定IPAM分配的IP, 并设置默认路由。从而实现同一个节点上，Node-\u003ePod与Pod之间的通信。 首先，如果网桥不存在则通过netlink库创建 func CreateBridge(bridge string, mtu int, gateway *net.IPNet) (netlink.Link, error) { if l, _ := netlink.LinkByName(bridge); l != nil { return l, nil } br := \u0026netlink.Bridge{ LinkAttrs: netlink.LinkAttrs{ Name: bridge, MTU: mtu, TxQLen: -1, }, } if err := netlink.LinkAdd(br); err != nil \u0026\u0026 err != syscall.EEXIST { return nil, err } dev, err := netlink.LinkByName(bridge) if err != nil { return nil, err } // 添加地址，即Pod默认网关地址 if err := netlink.AddrAdd(dev, \u0026netlink.Addr{IPNet: gateway}); err != nil { return nil, err } // 启动网桥 if err := netlink.LinkSetUp(dev); err != nil { return nil, err } return dev, nil } 为容器创建虚拟网卡 func SetupVeth(netns ns.NetNS, br netlink.Link, mtu int, ifName string, podIP *net.IPNet, gateway net.IP) error { hostIface := ¤t.Interface{} err := netns.Do(func(hostNS ns.NetNS) error { // 在容器网络空间创建虚拟网卡 hostVeth, containerVeth, err := ip.SetupVeth(ifName, mtu, \"\", hostNS) if err != nil { return err } hostIface.Name = hostVeth.Name // set ip for container veth conLink, err := netlink.LinkByName(containerVeth.Name) if err != nil { return err } // 绑定Pod IP if err := netlink.AddrAdd(conLink, \u0026netlink.Addr{IPNet: podIP}); err != nil { return err } // 启动网卡 if err := netlink.LinkSetUp(conLink); err != nil { return err } // 添加默认路径，网关即网桥的地址 if err := ip.AddDefaultRoute(gateway, conLink); err != nil { return err } return nil }) // need to lookup hostVeth again as its index has changed during ns move hostVeth, err := netlink.LinkByName(hostIface.Name) if err != nil { return fmt.Errorf(\"failed to lookup %q: %v\", hostIface.Name, err) } // 将虚拟网卡另一端绑定到网桥上 if err := netlink.LinkSetMaster(hostVeth, br); err != nil {","date":"Apr 01, 2022","objectID":"/how-to-write-k8s-cni/:2:1","series":null,"tags":["k8s","cni"],"title":"手写一个Kubernetes CNI网络插件","uri":"/how-to-write-k8s-cni/#节点内通信"},{"categories":["cloud"],"content":" Mycnidmycnid为节点上的守护进程，实现不同节点的Pod通信，主要功能包括： 监听K8s Nodes, 获取本节点的PodCIDR写入配置文件(默认位置在/run/mycni/subnet.json) 为其他节点添加路由(ip route add podCIDR via nodeip) 一些初始化配置，写入默认的Iptables规则，初始化网桥 节点间通信通过Controller监听Node资源，当前有节点Add/Delete时，调用Reconcile同步路由 func (r *Reconciler) Reconcile(ctx context.Context, req reconcile.Request) (reconcile.Result, error) { result := reconcile.Result{} nodes := \u0026corev1.NodeList{} // 获取所有节点 if err := r.client.List(ctx, nodes); err != nil { return result, err } // 获取节点的CIDR，生成路由 cidrs := make(map[string]netlink.Route) for _, node := range nodes.Items { if node.Name == r.config.nodeName { // 跳过当前节点 continue } // 生成期望路由 _, cidr, err := net.ParseCIDR(node.Spec.PodCIDR) nodeip, err := getNodeInternalIP(\u0026node) route := netlink.Route{ Dst: cidr, Gw: nodeip, ILinkIndex: r.hostLink.Attrs().Index, } cidrs[cidr.String()] = route // 与本地的路由对比，不存在则创建 if currentRoute, ok := r.routes[cidr.String()]; ok { if isRouteEqual(route, currentRoute) { continue } if err := r.ReplaceRoute(currentRoute); err != nil { return result, err } } else { if err := r.addRoute(route); err != nil { return result, err } } } // 删除多余的路由 for cidr, route := range r.routes { if _, ok := cidrs[cidr]; !ok { if err := r.delRoute(route); err != nil { return result, err } } } return result, nil } // 创建路由 func (r *Reconciler) addRoute(route netlink.Route) (err error) { defer func() { if err == nil { r.routes[route.Dst.String()] = route } }() log.Info(fmt.Sprintf(\"add route: %s\", route.String())) err = netlink.RouteAdd(\u0026route) if err != nil { log.Error(err, \"failed to add route\", \"route\", route.String()) } return } 主要步骤包括： 当前Node变化时获取集群中所有Node网络信息（PodCIDR、IP等） 与节点上路由进行比对，若缺少则添加，若宿主机上存在多余的路由则删除 为什么不是直接对变化的Node修改对应的路由？ 通过获取所有节点网络信息与宿主机进行路由比对，符合Kubernetes编程规范，声明式编程而不是过程式，这种方式不会丢事件。即使手动误删了路由，下次同步也会将其恢复。 其他配置如果使用Docker的话，Docker会禁止非Docker网桥的流量转发，需要配置iptables iptables -A FORWARD -i ${bridge} -j ACCEPT 大部分集群允许Pod访问外部网络，需要配置SNAT： sudo iptables -t nat -A POSTROUTING -s $cidr -j MASQUERADE # 另外允许主机网卡转发 iptables -A FORWARD -i $hostNetWork -j ACCEPT 代码如下，通过github.com/coreos/go-iptables/iptables可以配置iptables： func addIptables(bridgeName, hostDeviceName, nodeCIDR string) error { ipt, err := iptables.NewWithProtocol(iptables.ProtocolIPv4) if err != nil { return err } if err := ipt.AppendUnique(\"filter\", \"FORWARD\", \"-i\", bridgeName, \"-j\", \"ACCEPT\"); err != nil { return err } if err := ipt.AppendUnique(\"filter\", \"FORWARD\", \"-i\", hostDeviceName, \"-j\", \"ACCEPT\"); err != nil { return err } if err := ipt.AppendUnique(\"nat\", \"POSTROUTING\", \"-s\", nodeCIDR, \"-j\", \"MASQUERADE\"); err != nil { return err } return nil } ","date":"Apr 01, 2022","objectID":"/how-to-write-k8s-cni/:2:2","series":null,"tags":["k8s","cni"],"title":"手写一个Kubernetes CNI网络插件","uri":"/how-to-write-k8s-cni/#mycnid"},{"categories":["cloud"],"content":" Mycnidmycnid为节点上的守护进程，实现不同节点的Pod通信，主要功能包括： 监听K8s Nodes, 获取本节点的PodCIDR写入配置文件(默认位置在/run/mycni/subnet.json) 为其他节点添加路由(ip route add podCIDR via nodeip) 一些初始化配置，写入默认的Iptables规则，初始化网桥 节点间通信通过Controller监听Node资源，当前有节点Add/Delete时，调用Reconcile同步路由 func (r *Reconciler) Reconcile(ctx context.Context, req reconcile.Request) (reconcile.Result, error) { result := reconcile.Result{} nodes := \u0026corev1.NodeList{} // 获取所有节点 if err := r.client.List(ctx, nodes); err != nil { return result, err } // 获取节点的CIDR，生成路由 cidrs := make(map[string]netlink.Route) for _, node := range nodes.Items { if node.Name == r.config.nodeName { // 跳过当前节点 continue } // 生成期望路由 _, cidr, err := net.ParseCIDR(node.Spec.PodCIDR) nodeip, err := getNodeInternalIP(\u0026node) route := netlink.Route{ Dst: cidr, Gw: nodeip, ILinkIndex: r.hostLink.Attrs().Index, } cidrs[cidr.String()] = route // 与本地的路由对比，不存在则创建 if currentRoute, ok := r.routes[cidr.String()]; ok { if isRouteEqual(route, currentRoute) { continue } if err := r.ReplaceRoute(currentRoute); err != nil { return result, err } } else { if err := r.addRoute(route); err != nil { return result, err } } } // 删除多余的路由 for cidr, route := range r.routes { if _, ok := cidrs[cidr]; !ok { if err := r.delRoute(route); err != nil { return result, err } } } return result, nil } // 创建路由 func (r *Reconciler) addRoute(route netlink.Route) (err error) { defer func() { if err == nil { r.routes[route.Dst.String()] = route } }() log.Info(fmt.Sprintf(\"add route: %s\", route.String())) err = netlink.RouteAdd(\u0026route) if err != nil { log.Error(err, \"failed to add route\", \"route\", route.String()) } return } 主要步骤包括： 当前Node变化时获取集群中所有Node网络信息（PodCIDR、IP等） 与节点上路由进行比对，若缺少则添加，若宿主机上存在多余的路由则删除 为什么不是直接对变化的Node修改对应的路由？ 通过获取所有节点网络信息与宿主机进行路由比对，符合Kubernetes编程规范，声明式编程而不是过程式，这种方式不会丢事件。即使手动误删了路由，下次同步也会将其恢复。 其他配置如果使用Docker的话，Docker会禁止非Docker网桥的流量转发，需要配置iptables iptables -A FORWARD -i ${bridge} -j ACCEPT 大部分集群允许Pod访问外部网络，需要配置SNAT： sudo iptables -t nat -A POSTROUTING -s $cidr -j MASQUERADE # 另外允许主机网卡转发 iptables -A FORWARD -i $hostNetWork -j ACCEPT 代码如下，通过github.com/coreos/go-iptables/iptables可以配置iptables： func addIptables(bridgeName, hostDeviceName, nodeCIDR string) error { ipt, err := iptables.NewWithProtocol(iptables.ProtocolIPv4) if err != nil { return err } if err := ipt.AppendUnique(\"filter\", \"FORWARD\", \"-i\", bridgeName, \"-j\", \"ACCEPT\"); err != nil { return err } if err := ipt.AppendUnique(\"filter\", \"FORWARD\", \"-i\", hostDeviceName, \"-j\", \"ACCEPT\"); err != nil { return err } if err := ipt.AppendUnique(\"nat\", \"POSTROUTING\", \"-s\", nodeCIDR, \"-j\", \"MASQUERADE\"); err != nil { return err } return nil } ","date":"Apr 01, 2022","objectID":"/how-to-write-k8s-cni/:2:2","series":null,"tags":["k8s","cni"],"title":"手写一个Kubernetes CNI网络插件","uri":"/how-to-write-k8s-cni/#节点间通信"},{"categories":["cloud"],"content":" Mycnidmycnid为节点上的守护进程，实现不同节点的Pod通信，主要功能包括： 监听K8s Nodes, 获取本节点的PodCIDR写入配置文件(默认位置在/run/mycni/subnet.json) 为其他节点添加路由(ip route add podCIDR via nodeip) 一些初始化配置，写入默认的Iptables规则，初始化网桥 节点间通信通过Controller监听Node资源，当前有节点Add/Delete时，调用Reconcile同步路由 func (r *Reconciler) Reconcile(ctx context.Context, req reconcile.Request) (reconcile.Result, error) { result := reconcile.Result{} nodes := \u0026corev1.NodeList{} // 获取所有节点 if err := r.client.List(ctx, nodes); err != nil { return result, err } // 获取节点的CIDR，生成路由 cidrs := make(map[string]netlink.Route) for _, node := range nodes.Items { if node.Name == r.config.nodeName { // 跳过当前节点 continue } // 生成期望路由 _, cidr, err := net.ParseCIDR(node.Spec.PodCIDR) nodeip, err := getNodeInternalIP(\u0026node) route := netlink.Route{ Dst: cidr, Gw: nodeip, ILinkIndex: r.hostLink.Attrs().Index, } cidrs[cidr.String()] = route // 与本地的路由对比，不存在则创建 if currentRoute, ok := r.routes[cidr.String()]; ok { if isRouteEqual(route, currentRoute) { continue } if err := r.ReplaceRoute(currentRoute); err != nil { return result, err } } else { if err := r.addRoute(route); err != nil { return result, err } } } // 删除多余的路由 for cidr, route := range r.routes { if _, ok := cidrs[cidr]; !ok { if err := r.delRoute(route); err != nil { return result, err } } } return result, nil } // 创建路由 func (r *Reconciler) addRoute(route netlink.Route) (err error) { defer func() { if err == nil { r.routes[route.Dst.String()] = route } }() log.Info(fmt.Sprintf(\"add route: %s\", route.String())) err = netlink.RouteAdd(\u0026route) if err != nil { log.Error(err, \"failed to add route\", \"route\", route.String()) } return } 主要步骤包括： 当前Node变化时获取集群中所有Node网络信息（PodCIDR、IP等） 与节点上路由进行比对，若缺少则添加，若宿主机上存在多余的路由则删除 为什么不是直接对变化的Node修改对应的路由？ 通过获取所有节点网络信息与宿主机进行路由比对，符合Kubernetes编程规范，声明式编程而不是过程式，这种方式不会丢事件。即使手动误删了路由，下次同步也会将其恢复。 其他配置如果使用Docker的话，Docker会禁止非Docker网桥的流量转发，需要配置iptables iptables -A FORWARD -i ${bridge} -j ACCEPT 大部分集群允许Pod访问外部网络，需要配置SNAT： sudo iptables -t nat -A POSTROUTING -s $cidr -j MASQUERADE # 另外允许主机网卡转发 iptables -A FORWARD -i $hostNetWork -j ACCEPT 代码如下，通过github.com/coreos/go-iptables/iptables可以配置iptables： func addIptables(bridgeName, hostDeviceName, nodeCIDR string) error { ipt, err := iptables.NewWithProtocol(iptables.ProtocolIPv4) if err != nil { return err } if err := ipt.AppendUnique(\"filter\", \"FORWARD\", \"-i\", bridgeName, \"-j\", \"ACCEPT\"); err != nil { return err } if err := ipt.AppendUnique(\"filter\", \"FORWARD\", \"-i\", hostDeviceName, \"-j\", \"ACCEPT\"); err != nil { return err } if err := ipt.AppendUnique(\"nat\", \"POSTROUTING\", \"-s\", nodeCIDR, \"-j\", \"MASQUERADE\"); err != nil { return err } return nil } ","date":"Apr 01, 2022","objectID":"/how-to-write-k8s-cni/:2:2","series":null,"tags":["k8s","cni"],"title":"手写一个Kubernetes CNI网络插件","uri":"/how-to-write-k8s-cni/#其他配置"},{"categories":["cloud"],"content":" 演示通过kind模拟创建多节点k8s集群kind create cluster --config deploy/kind.yaml # cat deploy/kind.yaml kind: Cluster apiVersion: kind.x-k8s.io/v1alpha4 networking: # 禁止默认的CNI disableDefaultCNI: true nodes: # 包括一个master节点，三个worker - role: control-plane - role: worker - role: worker - role: worker 创建完成后，可以看到所有节点都是NotReady，这是因为我们没有装CNI mycni kubectl get no NAME STATUS ROLES AGE VERSION kind-control-plane NotReady control-plane,master 37s v1.23.4 kind-worker NotReady \u003cnone\u003e 18s v1.23.4 kind-worker2 NotReady \u003cnone\u003e 18s v1.23.4 kind-worker3 NotReady \u003cnone\u003e 18s v1.23.4 接下编译我们的mycni镜像，通过kind load docker-image加载到集群中。 部署CNI插件，这里参考了Flannel的部署，主要是通过Daemonset在每个节点上部署一个Pod，初始化容器将mycni检查二进制文件拷贝到/opt/cni/bin，将配置文件拷贝到/etc/cni/net.d，再启动mycnid容器。 kubectl apply -f deploy/mycni.yaml 部署完成后，可以看到所有节点状态变为Ready。 最后测试Pod的网络配置情况，部署一个多副本Alpine Deployment kubectl create deployment cni-test --image=alpine --replicas=6 -- top kubectl get po -owide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES cni-test-5df744744c-5wthb 1/1 Running 0 12s 10.244.2.3 kind-worker2 \u003cnone\u003e \u003cnone\u003e cni-test-5df744744c-7cdll 1/1 Running 0 12s 10.244.1.2 kind-worker \u003cnone\u003e \u003cnone\u003e cni-test-5df744744c-jssjk 1/1 Running 0 12s 10.244.3.2 kind-worker3 \u003cnone\u003e \u003cnone\u003e cni-test-5df744744c-jw6xv 1/1 Running 0 12s 10.244.1.3 kind-worker \u003cnone\u003e \u003cnone\u003e cni-test-5df744744c-klbr4 1/1 Running 0 12s 10.244.3.3 kind-worker3 \u003cnone\u003e \u003cnone\u003e cni-test-5df744744c-w7q9t 1/1 Running 0 12s 10.244.2.2 kind-worker2 \u003cnone\u003e \u003cnone\u003e 所有Pod可以正常启动，首先测试Node与Pod的通信, 结果如下 root@kind-worker:/# ping 10.244.1.2 PING 10.244.1.2 (10.244.1.2) 56(84) bytes of data. 64 bytes from 10.244.1.2: icmp_seq=1 ttl=64 time=0.101 ms 64 bytes from 10.244.1.2: icmp_seq=2 ttl=64 time=0.049 ms --- 10.244.1.2 ping statistics --- 同节点上的Pod通信： ~ kubectl exec cni-test-5df744744c-7cdll -- ping 10.244.1.3 -c 4 PING 10.244.1.3 (10.244.1.3): 56 data bytes 64 bytes from 10.244.1.3: seq=0 ttl=64 time=0.118 ms 64 bytes from 10.244.1.3: seq=1 ttl=64 time=0.077 ms 64 bytes from 10.244.1.3: seq=2 ttl=64 time=0.082 ms 64 bytes from 10.244.1.3: seq=3 ttl=64 time=0.085 ms --- 10.244.1.3 ping statistics --- 4 packets transmitted, 4 packets received, 0% packet loss round-trip min/avg/max = 0.077/0.090/0.118 ms 不同节点的Pod通信 ~ kubectl exec cni-test-5df744744c-7cdll -- ping 10.244.2.2 -c 4 PING 10.244.2.2 (10.244.2.2): 56 data bytes 64 bytes from 10.244.2.2: seq=0 ttl=62 time=0.298 ms 64 bytes from 10.244.2.2: seq=1 ttl=62 time=0.234 ms 64 bytes from 10.244.2.2: seq=2 ttl=62 time=0.180 ms 64 bytes from 10.244.2.2: seq=3 ttl=62 time=0.234 ms --- 10.244.2.2 ping statistics --- 4 packets transmitted, 4 packets received, 0% packet loss round-trip min/avg/max = 0.180/0.236/0.298 ms Pod访问外网 ~ kubectl exec cni-test-5df744744c-7cdll -- ping www.baidu.com -c 4 PING www.baidu.com (103.235.46.39): 56 data bytes 64 bytes from 103.235.46.39: seq=0 ttl=47 time=312.115 ms 64 bytes from 103.235.46.39: seq=1 ttl=47 time=311.126 ms 64 bytes from 103.235.46.39: seq=2 ttl=47 time=311.653 ms 64 bytes from 103.235.46.39: seq=3 ttl=47 time=311.250 ms --- www.baidu.com ping statistics --- 4 packets transmitted, 4 packets received, 0% packet loss round-trip min/avg/max = 311.126/311.536/312.115 ms 通过测试验证，我们实现的mycni满足k8s的CNI网络插件的要求，可以实现集群内所有Pod的通信，以及Node与Pod的通信，Pod也可以正常访问外网。 ","date":"Apr 01, 2022","objectID":"/how-to-write-k8s-cni/:3:0","series":null,"tags":["k8s","cni"],"title":"手写一个Kubernetes CNI网络插件","uri":"/how-to-write-k8s-cni/#演示"},{"categories":["cloud"],"content":" 总结本文首先介绍了CNI的架构，通过手动实现一个CNI网络插件，可以更加深入的了解CNI的工作原理以及Linux相关网络知识。 欢迎指正，所有代码见： https://github.com/qingwave/mycni ","date":"Apr 01, 2022","objectID":"/how-to-write-k8s-cni/:4:0","series":null,"tags":["k8s","cni"],"title":"手写一个Kubernetes CNI网络插件","uri":"/how-to-write-k8s-cni/#总结"},{"categories":["cloud"],"content":" 引用 https://www.cni.dev/docs/ https://ronaknathani.com/blog/2020/08/how-a-kubernetes-pod-gets-an-ip-address/ Explore more in https://qingwave.github.io ","date":"Apr 01, 2022","objectID":"/how-to-write-k8s-cni/:5:0","series":null,"tags":["k8s","cni"],"title":"手写一个Kubernetes CNI网络插件","uri":"/how-to-write-k8s-cni/#引用"},{"categories":["code"],"content":"Golang中通过我们使用Channel来传递信息、信号，经典的如生产者消费者、退出信号等, 那么除此之外Channel还有哪些不常见的用法。 ","date":"Feb 23, 2022","objectID":"/golang-channel-awesome/:0:0","series":null,"tags":["golang"],"title":"Golang Channel 妙用","uri":"/golang-channel-awesome/#"},{"categories":["code"],"content":" 限制并发数Golang原生提供了强大的并发原语，但如果无节制的使用大量Goroutine，并发过大会造成资源浪费，严重时会导致程序崩溃。使用带缓冲区的Channel可以解决此类问题。 在Golang的godoc/gatevfs中实现了对最大虚拟文件的并发限制。 // New returns a new FileSystem that delegates to fs. // If gateCh is non-nil and buffered, it's used as a gate // to limit concurrency on calls to fs. func New(fs vfs.FileSystem, gateCh chan bool) vfs.FileSystem { if cap(gateCh) == 0 { return fs } return gatefs{fs, gate(gateCh)} } type gate chan bool func (g gate) enter() { g \u003c- true } func (g gate) leave() { \u003c-g } 通过带缓存的Channel，每次打开文件时调用enter发生数据到Channel，当文件关闭时调用leave读取Channel数据，当前Channel满后再次读取变会阻塞，直到有资源被释放，从而达到限制并发数的目的。 func (fs gatefs) Open(p string) (vfs.ReadSeekCloser, error) { fs.enter() defer fs.leave() rsc, err := fs.fs.Open(p) if err != nil { return nil, err } return gatef{rsc, fs.gate}, nil } 可以配合WaitGroup来实现最大并发数的控制，具体代码如下: // control number type Gocontrol struct { ch chan struct{} wg sync.WaitGroup } func NewGocontrol(number int) *Gocontrol { return \u0026Gocontrol{ ch: make(chan struct{}, number), } } func (g *Gocontrol) Enter() { g.ch \u003c- struct{}{} } func (g *Gocontrol) Leave() { \u003c-g.ch } func (g *Gocontrol) Run(f func()) { g.Enter() g.wg.Add(1) go func() { defer g.Leave() defer g.wg.Done() f() }() } func (g *Gocontrol) Wait() { g.wg.Wait() } 测试运行，创建100个任务，调用Gocontrol限制最大并发为10，运行runtime.NumGoroutine来获取当前Goroutine数 func RunGocontrol() { go func() { for { fmt.Printf(\"goroutine numbers: %v\\n\", runtime.NumGoroutine()) time.Sleep(500 * time.Millisecond) } }() gctl := NewGocontrol(10) start := time.Now() for i := 0; i \u003c 100; i++ { n := i gctl.Run(func() { time.Sleep(1 * time.Second) fmt.Println(\"hello\", n) }) } gctl.Wait() dur := time.Since(start) fmt.Printf(\"run time: %v\", dur) } 运行结果显示，最大Goroutine数为12（包含1个主线程，1个监控线程，10个任务线程），符合预期 goroutine numbers: 12 run time: 10.002604769s ","date":"Feb 23, 2022","objectID":"/golang-channel-awesome/:1:0","series":null,"tags":["golang"],"title":"Golang Channel 妙用","uri":"/golang-channel-awesome/#限制并发数"},{"categories":["code"],"content":" 实现锁除了调用sync包，使用Channel也可以实现锁，以互斥锁为例： type ChLock chan struct{} func NewChLock() ChLock { return make(chan struct{}, 1) } func (l ChLock) Lock() { l \u003c- struct{}{} } func (l ChLock) Unlock() { \u003c-l } 互斥锁通过容量为1的Channel实现互斥，同样借助多个Channel可以使用读写锁，通过关闭Channel可以实现类型Once的功能。 从源码层面分析，Channel其实是一个线程安全的环形队列，Channel定义在runtime/chan.go中： type hchan struct { qcount uint // total data in the queue dataqsiz uint // size of the circular queue buf unsafe.Pointer // points to an array of dataqsiz elements elemsize uint16 closed uint32 elemtype *_type // element type sendx uint // send index recvx uint // receive index recvq waitq // list of recv waiters sendq waitq // list of send waiters lock mutex } 其中包含了lock锁结构，这里的mutex不同于sync.Mutex，只在Runtime内部使用是一种低阶的同步原语，也没有提供Lock/Unlock方法，只能通过全局的lock/unlock/initlock等函数调用。 ","date":"Feb 23, 2022","objectID":"/golang-channel-awesome/:2:0","series":null,"tags":["golang"],"title":"Golang Channel 妙用","uri":"/golang-channel-awesome/#实现锁"},{"categories":["code"],"content":" 最后本文介绍了一些Channel的妙用，限制并发数与实现锁等，通过示例及源码阐述其深层次的原因。 Explore more in https://qingwave.github.io ","date":"Feb 23, 2022","objectID":"/golang-channel-awesome/:3:0","series":null,"tags":["golang"],"title":"Golang Channel 妙用","uri":"/golang-channel-awesome/#最后"},{"categories":["code","前端"],"content":"最近疫情在家，空闲时间比较多，整理下之前写的Golang项目Weave，补充了一些功能，加了前端实现。作为一个Web应用模板，也算是功能比较齐全了，现将开发过程中遇到的一些问题、项目特性总结下。 ","date":"Jan 20, 2022","objectID":"/golang-vue-starter/:0:0","series":null,"tags":["golang","vue"],"title":"Golang+Vue轻松构建Web应用","uri":"/golang-vue-starter/#"},{"categories":["code","前端"],"content":" 介绍Weave是一个基于Go+Vue实现的Web应用模板，支持前后端，拥有完整的认证、存储、Restful API等功能。 后端基于Golang开发，主要特性如下： Restful API，通过gin实现，支持swagger MVC架构 支持Postgres存储，可以轻松替换为MySQL，使用gorm接入 Redis缓存 基于JWT认证 服务优雅终止 请求限速 Docker容器管理，Websocket支持 RBAC认证，由Casbin支持 其他支持Prometheus监控、格式化日志、PProf等 前端基于Vue开发，使用ElementPlus组件库 Vue3开发，使用组合式API 使用vite快速编译 支持WebShell，基于xtermjs 图表功能，基于echarts 支持WindiCSS，减少CSS编写 主要界面如下： 登录界面 Dashboard界面 应用界面 WebShell界面 ","date":"Jan 20, 2022","objectID":"/golang-vue-starter/:1:0","series":null,"tags":["golang","vue"],"title":"Golang+Vue轻松构建Web应用","uri":"/golang-vue-starter/#介绍"},{"categories":["code","前端"],"content":" 项目结构项目组织如下： ├── Dockerfile ├── Makefile ├── README.md ├── bin ├── config # server配置 ├── docs # swagger 生成文件 ├── document # 文档 ├── go.mod ├── go.sum ├── main.go # server入口 ├── pkg # server业务代码 ├── scripts # 脚本 ├── static # 静态文件 └── web # 前端目录 ","date":"Jan 20, 2022","objectID":"/golang-vue-starter/:2:0","series":null,"tags":["golang","vue"],"title":"Golang+Vue轻松构建Web应用","uri":"/golang-vue-starter/#项目结构"},{"categories":["code","前端"],"content":" 后端结构后端按照MVC架构实现，参考了社区一些最佳实践，具体如下： ├── pkg │ ├── common # 通用包 │ ├── config # 配置相关 │ ├── container # 容器库 │ ├── controller # 控制器层，处理HTTP请求 │ ├── database # 数据库初始化，封装 │ ├── metrics # 监控相关 │ ├── middleware # http中间件 │ ├── model # 模型层 │ ├── repository # 存储层，数据持久化 │ ├── server # server入口，创建router │ └── service # 逻辑层，处理业务 ","date":"Jan 20, 2022","objectID":"/golang-vue-starter/:2:1","series":null,"tags":["golang","vue"],"title":"Golang+Vue轻松构建Web应用","uri":"/golang-vue-starter/#后端结构"},{"categories":["code","前端"],"content":" 前端结构前端实现Vue3实现，与一般Vue项目类似 web ├── README.md ├── index.html ├── node_modules ├── package-lock.json ├── package.json ├── public │ └── favicon.ico ├── src # 所有代码位于src │ ├── App.vue # Vue项目入口 │ ├── assets # 静态文件 │ ├── axios # http请求封装 │ ├── components # Vue组件 │ ├── main.js │ ├── router # 路由 │ ├── utils # 工具包 │ └── views # 所有页面 └── vite.config.js # vite配置 ","date":"Jan 20, 2022","objectID":"/golang-vue-starter/:2:2","series":null,"tags":["golang","vue"],"title":"Golang+Vue轻松构建Web应用","uri":"/golang-vue-starter/#前端结构"},{"categories":["code","前端"],"content":" 一些细节","date":"Jan 20, 2022","objectID":"/golang-vue-starter/:3:0","series":null,"tags":["golang","vue"],"title":"Golang+Vue轻松构建Web应用","uri":"/golang-vue-starter/#一些细节"},{"categories":["code","前端"],"content":" 为什么使用JWT主要是为了方便服务横向扩展，如果基于Cookie+Session，Session只能保存在服务端，无法进行负载均衡。另外通过api访问，jwt可以放在HTTP Header的Bearer Token中。 当使用Websocket时，不支持HTTP Header，由于认证统一在中间件中进行，可以通过简单通过cookie存储，也可以单独为Websocket配置认证。 JWT不支持取消，可以通过在redis存入黑名单实现。 ","date":"Jan 20, 2022","objectID":"/golang-vue-starter/:3:1","series":null,"tags":["golang","vue"],"title":"Golang+Vue轻松构建Web应用","uri":"/golang-vue-starter/#为什么使用jwt"},{"categories":["code","前端"],"content":" 缓存实现加入了缓存便引入了数据一致性问题，经典的解决办法是先写数据库再写缓存（Cache-Aside模式），实现最终一致性，业务简单的项目可以使用这种方法。 那先写缓存行不行？如果同时有一个写请求一读请求，写请求会先删除缓存，读请求缓慢未命中会将DB中的旧数据载入，可能会造成数据不一致。先写数据库则不会有这样的问题，如果要实现先写缓存，可以使用双删的办法，即写前后分别操作一次缓存，这样处理逻辑会更复杂。如果不想侵入业务代码，可以通过监听Binlog来异步更新缓存。 ","date":"Jan 20, 2022","objectID":"/golang-vue-starter/:3:2","series":null,"tags":["golang","vue"],"title":"Golang+Vue轻松构建Web应用","uri":"/golang-vue-starter/#缓存实现"},{"categories":["code","前端"],"content":" 请求限流限流使用了golang.org/x/time/rate提供的令牌桶算法，以应对突发流量，可以对单个IP以及Server层面实现请求控制。 需要特别注意的是限流应当区别长连接与短连接，比如Weave中实现了容器exec接口，通过Websocket登录到容器，不应该影响其他正常请求。 ","date":"Jan 20, 2022","objectID":"/golang-vue-starter/:3:3","series":null,"tags":["golang","vue"],"title":"Golang+Vue轻松构建Web应用","uri":"/golang-vue-starter/#请求限流"},{"categories":["code","前端"],"content":" 从零开发前端前端而言完全是毫无经验，选用了Vue3，主要是文档比较全面适合新手。UI基于了ElementPlus，目前还是Beta版本，使用过程了也遇到了一些Bug，生产过程中不建议用，无奈的是目前Vue3好像也没有比较成熟的UI库。 Vue文档以及示例很详细，上手也挺快。主要是CCS不熟悉，调整样式上花了不少功夫，后来引入了WindiCSS, 只编写了少量的样式，其他全部依赖WindiCSS实现。其他路由、请求、图表参考对应的文档实现起来也很容易。 搭建了一个比较完整的管理平台，自己还是挺满意的，后面会不断优化，加一些其他特性。 ","date":"Jan 20, 2022","objectID":"/golang-vue-starter/:3:4","series":null,"tags":["golang","vue"],"title":"Golang+Vue轻松构建Web应用","uri":"/golang-vue-starter/#从零开发前端"},{"categories":["code","前端"],"content":" 运行后端本地运行，需要依赖Docker，Makefile文件只在Linux下有效，其他平台请自行尝试 安装数据库postgres与redis，初始化库 make init 本地运行 make run 前端使用vite编译 cd web npm i npm run dev 更多见ReadMe ","date":"Jan 20, 2022","objectID":"/golang-vue-starter/:4:0","series":null,"tags":["golang","vue"],"title":"Golang+Vue轻松构建Web应用","uri":"/golang-vue-starter/#运行"},{"categories":["code","前端"],"content":" 总结本文总结了Weave的架构与特性，以及开发过程中遇到的一些问题，从零开始实现一个完整的前后端Web应用，其他功能后面会不断优化。 项目链接见 https://github.com/qingwave/weave Explore more in https://qingwave.github.io ","date":"Jan 20, 2022","objectID":"/golang-vue-starter/:5:0","series":null,"tags":["golang","vue"],"title":"Golang+Vue轻松构建Web应用","uri":"/golang-vue-starter/#总结"},{"categories":["cloud"],"content":"熟悉client-go的同学都知道，不止有Deployment、Pod这些结构化对象，也提供了unstructured.Unstructured对象，那么为什么需要非结构对象？ ","date":"Dec 15, 2021","objectID":"/k8s-unstructured-object/:0:0","series":null,"tags":["k8s"],"title":"K8S中为什么需要Unstructured对象","uri":"/k8s-unstructured-object/#"},{"categories":["cloud"],"content":" Structured vs Unstructured结构化对象是指可以用Go Struct表示的对象，比如Deployment在k8s.io/api/apps/v1定义 type Deployment struct { metav1.TypeMeta `json:\",inline\"` // Standard object's metadata. // More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata // +optional metav1.ObjectMeta `json:\"metadata,omitempty\" protobuf:\"bytes,1,opt,name=metadata\"` ... } 我们可以直接通过appsv1.Deployment来安全地定义Deployment的各个字段，通常创建过程如下： clientset, err := kubernetes.NewForConfig(config) deployment := \u0026appsv1.Deployment{} deployment.Name = \"example\" deployment.Spec = appsv1.DeploymentSpec{ ... } clientset.AppsV1().Deployments(apiv1.NamespaceDefault).Create(deployment) 而对于Unstructured定义在k8s.io/apimachinery/pkg/apis/meta/v1/unstructured中 type Unstructured struct { // Object is a JSON compatible map with string, float, int, bool, []interface{}, or // map[string]interface{} // children. Object map[string]interface{} } 通过定义map[string]interface{}可以来表示任意的JSON/YAML对象，而不需要引用Go Struct。可以通过Dynamic client来创建非结构化对象，以下是使用Unstructured创建Deployment的样例。 client, _ := dynamic.NewForConfig(config) deploymentRes := schema.GroupVersionResource{Group: \"apps\", Version: \"v1\", Resource: \"deployments\"} deployment := \u0026unstructured.Unstructured{ Object: map[string]interface{}{ \"apiVersion\": \"apps/v1\", \"kind\": \"Deployment\", \"metadata\": map[string]interface{}{ \"name\": \"demo-deployment\", }, \"spec\": map[string]interface{}{ \"replicas\": 2, ... } } } client.Resource(deploymentRes).Namespace(namespace).Create(context.TODO(), deployment, metav1.CreateOptions{}) ","date":"Dec 15, 2021","objectID":"/k8s-unstructured-object/:1:0","series":null,"tags":["k8s"],"title":"K8S中为什么需要Unstructured对象","uri":"/k8s-unstructured-object/#structured-vs-unstructured"},{"categories":["cloud"],"content":" Why那么什么情况下需要使用到Unstructured对象呢，结构化对象有着安全地类型校验，通过clientset可以方便地增删改查。而非结构化对象只能手动设置GVR、通过map[string]interface{}设置各个字段。 假想你作为一个Paas平台的开发者，需要为每个用户传入的YAML/JSON资源添加label，比如添加user信息creator=xxx。如果用户只能创建Deployment，那么我们可以将资源解析成appsv1.Deployment{}对象，再添加label。但是通常会传入多种资源，不仅有内置的Deployment、Service等，也可能会包含自定义资源。由于不确定资源类型，我们只能通过Unstructured对象来解析。 const manifest = ` apiVersion: apps/v1 kind: Deployment metadata: name: example spec: ... ` // convert yaml to unstructured obj := \u0026unstructured.Unstructured{} dec := yaml.NewDecodingSerializer(unstructured.UnstructuredJSONScheme) dec.Decode([]byte(manifest), nil, obj) // add label labels := obj.GetLabels() labels[\"creator\"]=\"userxxx\" // set label obj.SetLabels(labels) dynamicClient.Resource().Namespace(namespace).Create(context.TODO(), obj, metav1.CreateOptions{}) 当实现对多种资源的通用处理（上面的示例），或者运行时才能确定的对象（例如根据配置监听不同对象），又或者不愿引入额外的依赖（处理大量的CRD），可以使用Unstructured对象来处理以上情况。 ","date":"Dec 15, 2021","objectID":"/k8s-unstructured-object/:2:0","series":null,"tags":["k8s"],"title":"K8S中为什么需要Unstructured对象","uri":"/k8s-unstructured-object/#why"},{"categories":["cloud"],"content":" How不管是结构化对象还是非结构化，最终会调用k8s的Rest API，例如Create Deployment时 POST /apis/apps/v1/namespaces/{namespace}/deployments/{name} K8s中GVR(GroupVersionResource)可以唯一表征资源对象，用来组成Rest API, 如上Group为apps、Version为v1、Resource是deployments；GVK(GroupVersionKind)可以来标识类型（如Deployment）。Resource与Kind的对应关系可以通过kubectl api-resources查看。 ~ kubectl api-resources --api-group apps NAME SHORTNAMES APIVERSION NAMESPACED KIND controllerrevisions apps/v1 true ControllerRevision daemonsets ds apps/v1 true DaemonSet deployments deploy apps/v1 true Deployment replicasets rs apps/v1 true ReplicaSet statefulsets sts apps/v1 true StatefulSet 对于结构化对象，使用clientset可以获取到GVR，最后调用restClient组成到Rest API clientset.AppsV1().Deployments(namespace).Create(deployment) // Create takes the representation of a deployment and creates it. Returns the server's representation of the deployment, and an error, if there is any. func (c *deployments) Create(ctx context.Context, deployment *v1.Deployment, opts metav1.CreateOptions) (result *v1.Deployment, err error) { result = \u0026v1.Deployment{} err = c.client.Post(). Namespace(c.ns). Resource(\"deployments\"). // Resource设置 VersionedParams(\u0026opts, scheme.ParameterCodec). Body(deployment). Do(ctx). Into(result) return } 对于非结构化对象，需要用户手动填充GVR，如果只知道GVK可以通过restMapping获取 deploymentRes := schema.GroupVersionResource{Group: \"apps\", Version: \"v1\", Resource: \"deployments\"} dynamicClient.Resource().Namespace(namespace).Create() // Create具体实现 func (c *dynamicResourceClient) Create(ctx context.Context, obj *unstructured.Unstructured, opts metav1.CreateOptions, subresources ...string) (*unstructured.Unstructured, error) { outBytes, err := runtime.Encode(unstructured.UnstructuredJSONScheme, obj) name := \"\" if len(subresources) \u003e 0 { accessor, err := meta.Accessor(obj) name = accessor.GetName() } // 调用restClient result := c.client.client. Post(). AbsPath(append(c.makeURLSegments(name), subresources...)...). Body(outBytes). SpecificallyVersionedParams(\u0026opts, dynamicParameterCodec, versionV1). Do(ctx) // ... } ","date":"Dec 15, 2021","objectID":"/k8s-unstructured-object/:3:0","series":null,"tags":["k8s"],"title":"K8S中为什么需要Unstructured对象","uri":"/k8s-unstructured-object/#how"},{"categories":["cloud"],"content":" 总结本文描述Unstructured对象在K8s中的使用场景、使用方式，与Structured对象的对比，以及相关代码解析。 ","date":"Dec 15, 2021","objectID":"/k8s-unstructured-object/:4:0","series":null,"tags":["k8s"],"title":"K8S中为什么需要Unstructured对象","uri":"/k8s-unstructured-object/#总结"},{"categories":["cloud"],"content":" 引用 https://kubernetes.io/zh/docs/reference/using-api/api-concepts/ ","date":"Dec 15, 2021","objectID":"/k8s-unstructured-object/:5:0","series":null,"tags":["k8s"],"title":"K8S中为什么需要Unstructured对象","uri":"/k8s-unstructured-object/#引用"},{"categories":["cloud","tool"],"content":"在k8s云环境中，我们需要在容器内抓包进行Debug, 但通常大多容器都没有安装tcpdump以及其他网络工具；在托管k8s中我们想登录node，不是没权限就是步骤太麻烦。本文的主角nsenter正是很擅长解决这些问题，nsenter可以进入指定namespace的工具，一般用来在容器环境中进行调试。 ","date":"Nov 12, 2021","objectID":"/k8s-debug-nsenter/:0:0","series":null,"tags":["k8s"],"title":"Kubernetes调试利器Nsenter","uri":"/k8s-debug-nsenter/#"},{"categories":["cloud","tool"],"content":" 调试容器网络通过nsenter可以轻松在宿主机进入容器的网络命令空间，命令如下： # 设置containerid containerid=xxx # 获取容器主进程 pid=$(docker inspect -f {{.State.Pid}} $containerid) # 进入容器networker namespace nsenter -n --target $pid 之后便可以使用宿主机各种工具tcpdump, netstat等命令 ","date":"Nov 12, 2021","objectID":"/k8s-debug-nsenter/:1:0","series":null,"tags":["k8s"],"title":"Kubernetes调试利器Nsenter","uri":"/k8s-debug-nsenter/#调试容器网络"},{"categories":["cloud","tool"],"content":" 登录k8s节点如果只有Apiserver权限，登录k8s节点也可以使用nsenter 临时登录某个节点可以使用如下脚本: 前提是需要拥有一些特殊权限privileded，hostPID等 node=xxx cmd='[ \"nsenter\", \"--target\", \"1\", \"--mount\", \"--uts\", \"--ipc\", \"--net\", \"--pid\", \"--\"]' overrides=\"$( cat \u003c\u003cEOT { \"spec\": { \"nodeName\": \"$node\", \"hostPID\": true, \"hostNetwork\": true, \"containers\": [ { \"securityContext\": { \"privileged\": true }, \"image\": \"alpine\", \"name\": \"nsenter\", \"stdin\": true, \"stdinOnce\": true, \"tty\": true, \"command\": $cmd } ], \"tolerations\": [ { \"operator\": \"Exists\" } ] } } EOT )\" pod=\"kube-nodeshell-$(env LC_ALL=C tr -dc a-z0-9 \u003c/dev/urandom | head -c 6)\" kubectl run --image=alpine --restart=Never --rm --overrides=\"$overrides\" -it $pod 原理是通过共享pid方式hostPID=true，在容器中看到宿主机的所有进程，然后使用nsenter进入宿主机1号进程（宿主机根进程）的mount、uts、ipc、net、pid等namespace，从而可以获取类似宿主机的shell。 如果需要经常使用，可以部署个DaemonSet，使用时登录对应节点的pod即可（建议只在测试环境使用，具有一定风险） apiVersion: apps/v1 kind: DaemonSet metadata: name: kube-nodehsell labels: app: kube-nodehsell spec: selector: matchLabels: app: kube-nodehsell template: metadata: labels: app: kube-nodehsell spec: tolerations: - operator: \"Exists\" containers: - name: kube-nodehsell image: alpine command: - nsenter - --target - \"1\" - --mount - --uts - --ipc - --net - --pid - -- - sleep - infinity securityContext: privileged: true hostIPC: true hostPID: true hostNetwork: true priorityClassName: system-node-critical 本文所有文件见kube-nodeshell ","date":"Nov 12, 2021","objectID":"/k8s-debug-nsenter/:2:0","series":null,"tags":["k8s"],"title":"Kubernetes调试利器Nsenter","uri":"/k8s-debug-nsenter/#登录k8s节点"},{"categories":["cloud","tool"],"content":" 临时容器kubernetes 1.18之后启用了临时容器，用户可以通过kubectl debug命令来添加临时容器到pod，也可以登录到node shell，一些简单的调试工作可以使用这种方法， 见调试运行中的Pod。 对比nsenter方法，kubectl debug通过shell登录节点时只是共享了pid、hostNetwork，nsenter则更灵活可以使用宿主机的相关工具以及执行特权操作。 ","date":"Nov 12, 2021","objectID":"/k8s-debug-nsenter/:3:0","series":null,"tags":["k8s"],"title":"Kubernetes调试利器Nsenter","uri":"/k8s-debug-nsenter/#临时容器"},{"categories":["cloud","tool"],"content":" 参考 https://man7.org/linux/man-pages/man1/nsenter.1.html https://github.com/kvaps/kubectl-node-shell https://kubernetes.io/zh/docs/tasks/debug-application-cluster/debug-running-pod/ ","date":"Nov 12, 2021","objectID":"/k8s-debug-nsenter/:4:0","series":null,"tags":["k8s"],"title":"Kubernetes调试利器Nsenter","uri":"/k8s-debug-nsenter/#参考"},{"categories":["cloud"],"content":" 背景最近在AWS k8s集群部署一个多AZ应用时，发现cluster-autoscaler无法正常scale up。 通过反复测试发现，当NodeGroup的初始容量为0时（minSize=0）无法扩容，报错信息如下，当NodeGrop初始容量为1时（minSize=1）可以正常扩容。 Normal NotTriggerScaleUp 18m cluster-autoscaler pod didn't trigger scale-up (it wouldn't fit if a new node is added): 3 Insufficient cpu, 1 node(s) didn't match node selector, 1 max node group size reached, 12 Insufficient memory, 1 node(s) didn't match pod affinity/anti-affinity, 1 node(s) didn't match pod anti-affinity rules, 3 node(s) had taint, that the pod didn't tolerate 这个现象很有意思，跟着代码查看下究竟是哪里出问题了。 ","date":"Nov 08, 2021","objectID":"/k8s-clusterautoscaler-from-zero-issue/:1:0","series":null,"tags":["k8s"],"title":"ClusterAutoScaler无法从零扩容","uri":"/k8s-clusterautoscaler-from-zero-issue/#背景"},{"categories":["cloud"],"content":" 探究ClusterAutoScaler主要扩容逻辑如下： 定期获取（默认10s）所有Pending Pod，过滤出由于资源不足调度失败的Pod 根据NodeGroup生成新Node模拟调度，如果可以调度则将新节点加入集群 根据现象猜测与生成NodeTemplate有关，否则不会造成两次情况调度结果不一致。 主要代码位于utils.go#L42 // nodes 指当前集群中所有node func GetNodeInfosForGroups(nodes []*apiv1.Node, nodeInfoCache map[string]*schedulerframework.NodeInfo, cloudProvider cloudprovider.CloudProvider, listers kube_util.ListerRegistry, daemonsets []*appsv1.DaemonSet, predicateChecker simulator.PredicateChecker, ignoredTaints taints.TaintKeySet) (map[string]*schedulerframework.NodeInfo, errors.AutoscalerError) { result := make(map[string]*schedulerframework.NodeInfo) seenGroups := make(map[string]bool) // 构建node与pod的映射 podsForNodes, err := getPodsForNodes(listers) if err != nil { return map[string]*schedulerframework.NodeInfo{}, err } // processNode returns information whether the nodeTemplate was generated and if there was an error. // processNode 函数通过提供的node生成node模板 processNode := func(node *apiv1.Node) (bool, string, errors.AutoscalerError) { nodeGroup, err := cloudProvider.NodeGroupForNode(node) if err != nil { return false, \"\", errors.ToAutoscalerError(errors.CloudProviderError, err) } if nodeGroup == nil || reflect.ValueOf(nodeGroup).IsNil() { return false, \"\", nil } id := nodeGroup.Id() // nodeGroup id不存在则将其添加到result中 if _, found := result[id]; !found { // Build nodeInfo. // 根据当前node生成模板 nodeInfo, err := simulator.BuildNodeInfoForNode(node, podsForNodes) if err != nil { return false, \"\", err } sanitizedNodeInfo, err := sanitizeNodeInfo(nodeInfo, id, ignoredTaints) if err != nil { return false, \"\", err } result[id] = sanitizedNodeInfo return true, id, nil } return false, \"\", nil } // 遍历所有node，如果通过node能获取到对应nodeGroup的模板，则添加到nodeInfoCache中 for _, node := range nodes { // Broken nodes might have some stuff missing. Skipping. if !kube_util.IsNodeReadyAndSchedulable(node) { continue } added, id, typedErr := processNode(node) if typedErr != nil { return map[string]*schedulerframework.NodeInfo{}, typedErr } if added \u0026\u0026 nodeInfoCache != nil { if nodeInfoCopy, err := deepCopyNodeInfo(result[id]); err == nil { nodeInfoCache[id] = nodeInfoCopy } } } //如果不在cahce中，则通过nodeGroup生成模板 for _, nodeGroup := range cloudProvider.NodeGroups() { id := nodeGroup.Id() seenGroups[id] = true if _, found := result[id]; found { continue } // No good template, check cache of previously running nodes. if nodeInfoCache != nil { if nodeInfo, found := nodeInfoCache[id]; found { if nodeInfoCopy, err := deepCopyNodeInfo(nodeInfo); err == nil { result[id] = nodeInfoCopy continue } } } // No good template, trying to generate one. This is called only if there are no // working nodes in the node groups. By default CA tries to use a real-world example. nodeInfo, err := GetNodeInfoFromTemplate(nodeGroup, daemonsets, predicateChecker, ignoredTaints) if err != nil { if err == cloudprovider.ErrNotImplemented { continue } else { klog.Errorf(\"Unable to build proper template node for %s: %v\", id, err) return map[string]*schedulerframework.NodeInfo{}, errors.ToAutoscalerError(errors.CloudProviderError, err) } } result[id] = nodeInfo } // Remove invalid node groups from cache for id := range nodeInfoCache { if _, ok := seenGroups[id]; !ok { delete(nodeInfoCache, id) } } // 处理unready/unschedulable的节点 for _, node := range nodes { // Allowing broken nodes if !kube_util.IsNodeReadyAndSchedulable(node) { added, _, typedErr := processNode(node) if typedErr != nil { return map[string]*schedulerframework.NodeInfo{}, typedErr } nodeGroup, err := cloudProvider.NodeGroupForNode(node) if err != nil { return map[string]*schedulerframework.NodeInfo{}, errors.ToAutoscalerError( errors.CloudProviderError, err) } if added { klog.Warningf(\"Built template for %s based on unready/unschedulable node %s\", nodeGroup.Id(), node.Name) } } } return result, nil } 方法GetNodeInfosForGroups生成模板的主要逻辑如下： 遍历集群中所有节点，节点有对应nodeGroup，则根据节点生成模板 其他nodeGroup则根据nodeGroup配置与daemonset信息生成模板 节点存在nodeGroup则根据sanitizeNodeInfo方法生成模板 func BuildNodeInfoForNode(node *apiv1.Node, podsF","date":"Nov 08, 2021","objectID":"/k8s-clusterautoscaler-from-zero-issue/:2:0","series":null,"tags":["k8s"],"title":"ClusterAutoScaler无法从零扩容","uri":"/k8s-clusterautoscaler-from-zero-issue/#探究"},{"categories":["cloud"],"content":" 总结在AWS中通过静态文件来获取节点资源信息，当实际生产中与文件不一致时，会造成ClusterAutoScaler无法按照预期工作。ClusterAutoScaler大大减少了运维工作，无需时时关心资源申请量，但只有了解其内部逻辑，才能更好的应用于生产。 ","date":"Nov 08, 2021","objectID":"/k8s-clusterautoscaler-from-zero-issue/:3:0","series":null,"tags":["k8s"],"title":"ClusterAutoScaler无法从零扩容","uri":"/k8s-clusterautoscaler-from-zero-issue/#总结"},{"categories":["cloud"],"content":"前文快速实现一个Kubernetes Operator介绍了kubebuilder工具，快速实现了一个Operator。今天我们深入水下，探寻kubebuilder究竟是如何工作的。 ","date":"Aug 23, 2021","objectID":"/k8s-kubebuilder-deep-dive/:0:0","series":null,"tags":["k8s"],"title":"深入了解kubebuilder","uri":"/k8s-kubebuilder-deep-dive/#"},{"categories":["cloud"],"content":" 普通开发流程如果不借助任何Operator脚手架，我们是如何实现Operator的？大体分为一下几步： CRD定义 Controller开发，编写逻辑 测试部署 ","date":"Aug 23, 2021","objectID":"/k8s-kubebuilder-deep-dive/:1:0","series":null,"tags":["k8s"],"title":"深入了解kubebuilder","uri":"/k8s-kubebuilder-deep-dive/#普通开发流程"},{"categories":["cloud"],"content":" API定义首先通过k8s.io/code-generator项目生成API相关代码，定义相关字段。 ","date":"Aug 23, 2021","objectID":"/k8s-kubebuilder-deep-dive/:1:1","series":null,"tags":["k8s"],"title":"深入了解kubebuilder","uri":"/k8s-kubebuilder-deep-dive/#api定义"},{"categories":["cloud"],"content":" Controller实现实现Controller以官方提供的sample-controller为例，如图所示 主要分为以下几步： 初始化client配置 //通过master/kubeconfig建立client config cfg, err := clientcmd.BuildConfigFromFlags(masterURL, kubeconfig) if err != nil { klog.Fatalf(\"Error building kubeconfig: %s\", err.Error()) } // kubernetes client kubeClient, err := kubernetes.NewForConfig(cfg) if err != nil { klog.Fatalf(\"Error building kubernetes clientset: %s\", err.Error()) } // crd client exampleClient, err := clientset.NewForConfig(cfg) if err != nil { klog.Fatalf(\"Error building example clientset: %s\", err.Error()) } 初始化Informer并启动 //k8s sharedInformer kubeInformerFactory := kubeinformers.NewSharedInformerFactory(kubeClient, time.Second*30) // crd sharedInformer exampleInformerFactory := informers.NewSharedInformerFactory(exampleClient, time.Second*30) // 初始化controller，传入informer, 注册了Deployment与Foo Informers controller := NewController(kubeClient, exampleClient, kubeInformerFactory.Apps().V1().Deployments(), exampleInformerFactory.Samplecontroller().V1alpha1().Foos()) //启动Informer kubeInformerFactory.Start(stopCh) exampleInformerFactory.Start(stopCh) 最后启动Controller if err = controller.Run(2, stopCh); err != nil { klog.Fatalf(\"Error running controller: %s\", err.Error()) } 在Controller的实现中，通过NewController来初始化： func NewController( kubeclientset kubernetes.Interface, sampleclientset clientset.Interface, deploymentInformer appsinformers.DeploymentInformer, fooInformer informers.FooInformer) *Controller { // Create event broadcaster utilruntime.Must(samplescheme.AddToScheme(scheme.Scheme)) klog.V(4).Info(\"Creating event broadcaster\") eventBroadcaster := record.NewBroadcaster() eventBroadcaster.StartStructuredLogging(0) eventBroadcaster.StartRecordingToSink(\u0026typedcorev1.EventSinkImpl{Interface: kubeclientset.CoreV1().Events(\"\")}) recorder := eventBroadcaster.NewRecorder(scheme.Scheme, corev1.EventSource{Component: controllerAgentName}) controller := \u0026Controller{ kubeclientset: kubeclientset, sampleclientset: sampleclientset, deploymentsLister: deploymentInformer.Lister(), //只读cache deploymentsSynced: deploymentInformer.Informer().HasSynced, //调用Informer()会注册informer到共享informer中 foosLister: fooInformer.Lister(), foosSynced: fooInformer.Informer().HasSynced, workqueue: workqueue.NewNamedRateLimitingQueue(workqueue.DefaultControllerRateLimiter(), \"Foos\"), // 初始化工作队列 recorder: recorder, } klog.Info(\"Setting up event handlers\") // 添加回调事件 fooInformer.Informer().AddEventHandler(cache.ResourceEventHandlerFuncs{ AddFunc: controller.enqueueFoo, UpdateFunc: func(old, new interface{}) { controller.enqueueFoo(new) }, }) deploymentInformer.Informer().AddEventHandler(cache.ResourceEventHandlerFuncs{ AddFunc: controller.handleObject, UpdateFunc: func(old, new interface{}) { newDepl := new.(*appsv1.Deployment) oldDepl := old.(*appsv1.Deployment) if newDepl.ResourceVersion == oldDepl.ResourceVersion { // Periodic resync will send update events for all known Deployments. // Two different versions of the same Deployment will always have different RVs. return } controller.handleObject(new) }, DeleteFunc: controller.handleObject, }) return controller } Controller启动则是典型的k8s工作流，通过控制循环不断从工作队列获取对象进行处理，使其达到期望状态 func (c *Controller) Run(workers int, stopCh \u003c-chan struct{}) error { defer utilruntime.HandleCrash() defer c.workqueue.ShutDown() // 等待cache同步 klog.Info(\"Waiting for informer caches to sync\") if ok := cache.WaitForCacheSync(stopCh, c.deploymentsSynced, c.foosSynced); !ok { return fmt.Errorf(\"failed to wait for caches to sync\") } // 启动worker,每个worker一个goroutine for i := 0; i \u003c workers; i++ { go wait.Until(c.runWorker, time.Second, stopCh) } // 等待退出信号 \u003c-stopCh return nil } // worker就是一个循环不断调用processNextWorkItem func (c *Controller) runWorker() { for c.processNextWorkItem() { } } func (c *Controller) processNextWorkItem() bool { // 从工作队列获取对象 obj, shutdown := c.workqueue.Get() if shutdown { return false } // We wrap this block in a func so we can defer c.workqueue.Done. err := func(obj interface{}) error { defer c.workque","date":"Aug 23, 2021","objectID":"/k8s-kubebuilder-deep-dive/:1:2","series":null,"tags":["k8s"],"title":"深入了解kubebuilder","uri":"/k8s-kubebuilder-deep-dive/#controller实现"},{"categories":["cloud"],"content":" Operator模式在Operator模式下，用户只需要实现Reconcile(调谐)即sample-controller中的syncHandler，其他步骤kubebuilder已经帮我们实现了。那我们来一探究竟，kubebuilder是怎么一步步触发Reconcile逻辑。 以mygame为例，通常使用kubebuilder生成的主文件如下： var ( // 用来解析kubernetes对象 scheme = runtime.NewScheme() setupLog = ctrl.Log.WithName(\"setup\") ) func init() { utilruntime.Must(clientgoscheme.AddToScheme(scheme)) // 添加自定义对象到scheme utilruntime.Must(myappv1.AddToScheme(scheme)) //+kubebuilder:scaffold:scheme } func main() { // ... ctrl.SetLogger(zap.New(zap.UseFlagOptions(\u0026opts))) // 初始化controller manager mgr, err := ctrl.NewManager(ctrl.GetConfigOrDie(), ctrl.Options{ Scheme: scheme, MetricsBindAddress: metricsAddr, Port: 9443, HealthProbeBindAddress: probeAddr, LeaderElection: enableLeaderElection, LeaderElectionID: \"7bc453ad.qingwave.github.io\", }) if err != nil { setupLog.Error(err, \"unable to start manager\") os.Exit(1) } // 初始化Reconciler if err = (\u0026controllers.GameReconciler{ Client: mgr.GetClient(), Scheme: mgr.GetScheme(), }).SetupWithManager(mgr); err != nil { setupLog.Error(err, \"unable to create controller\", \"controller\", \"Game\") os.Exit(1) } // 初始化Webhook if enableWebhook { if err = (\u0026myappv1.Game{}).SetupWebhookWithManager(mgr); err != nil { setupLog.Error(err, \"unable to create webhook\", \"webhook\", \"Game\") os.Exit(1) } } //+kubebuilder:scaffold:builder // 启动manager if err := mgr.Start(ctrl.SetupSignalHandler()); err != nil { setupLog.Error(err, \"problem running manager\") os.Exit(1) } } kubebuilder封装了controller-runtime，在主文件中主要初始了controller-manager,以及我们填充的Reconciler与Webhook，最后启动manager。 分别来看下每个流程。 ","date":"Aug 23, 2021","objectID":"/k8s-kubebuilder-deep-dive/:2:0","series":null,"tags":["k8s"],"title":"深入了解kubebuilder","uri":"/k8s-kubebuilder-deep-dive/#operator模式"},{"categories":["cloud"],"content":" Manager初始化代码如下： func New(config *rest.Config, options Options) (Manager, error) { // 设置默认配置 options = setOptionsDefaults(options) // cluster初始化 cluster, err := cluster.New(config, func(clusterOptions *cluster.Options) { clusterOptions.Scheme = options.Scheme clusterOptions.MapperProvider = options.MapperProvider clusterOptions.Logger = options.Logger clusterOptions.SyncPeriod = options.SyncPeriod clusterOptions.Namespace = options.Namespace clusterOptions.NewCache = options.NewCache clusterOptions.ClientBuilder = options.ClientBuilder clusterOptions.ClientDisableCacheFor = options.ClientDisableCacheFor clusterOptions.DryRunClient = options.DryRunClient clusterOptions.EventBroadcaster = options.EventBroadcaster }) if err != nil { return nil, err } // event recorder初始化 recorderProvider, err := options.newRecorderProvider(config, cluster.GetScheme(), options.Logger.WithName(\"events\"), options.makeBroadcaster) if err != nil { return nil, err } // 选主的资源锁配置 leaderConfig := options.LeaderElectionConfig if leaderConfig == nil { leaderConfig = rest.CopyConfig(config) } resourceLock, err := options.newResourceLock(leaderConfig, recorderProvider, leaderelection.Options{ LeaderElection: options.LeaderElection, LeaderElectionResourceLock: options.LeaderElectionResourceLock, LeaderElectionID: options.LeaderElectionID, LeaderElectionNamespace: options.LeaderElectionNamespace, }) if err != nil { return nil, err } // ... return \u0026controllerManager{ cluster: cluster, recorderProvider: recorderProvider, resourceLock: resourceLock, metricsListener: metricsListener, metricsExtraHandlers: metricsExtraHandlers, logger: options.Logger, elected: make(chan struct{}), port: options.Port, host: options.Host, certDir: options.CertDir, leaseDuration: *options.LeaseDuration, renewDeadline: *options.RenewDeadline, retryPeriod: *options.RetryPeriod, healthProbeListener: healthProbeListener, readinessEndpointName: options.ReadinessEndpointName, livenessEndpointName: options.LivenessEndpointName, gracefulShutdownTimeout: *options.GracefulShutdownTimeout, internalProceduresStop: make(chan struct{}), leaderElectionStopped: make(chan struct{}), }, nil 在New中主要初始化了各种配置端口、选主信息、eventRecorder，最重要的是初始了Cluster。Cluster用来访问k8s，初始化代码如下： // New constructs a brand new cluster func New(config *rest.Config, opts ...Option) (Cluster, error) { if config == nil { return nil, errors.New(\"must specify Config\") } options := Options{} for _, opt := range opts { opt(\u0026options) } options = setOptionsDefaults(options) // Create the mapper provider mapper, err := options.MapperProvider(config) if err != nil { options.Logger.Error(err, \"Failed to get API Group-Resources\") return nil, err } // Create the cache for the cached read client and registering informers cache, err := options.NewCache(config, cache.Options{Scheme: options.Scheme, Mapper: mapper, Resync: options.SyncPeriod, Namespace: options.Namespace}) if err != nil { return nil, err } clientOptions := client.Options{Scheme: options.Scheme, Mapper: mapper} apiReader, err := client.New(config, clientOptions) if err != nil { return nil, err } writeObj, err := options.ClientBuilder. WithUncached(options.ClientDisableCacheFor...). Build(cache, config, clientOptions) if err != nil { return nil, err } if options.DryRunClient { writeObj = client.NewDryRunClient(writeObj) } recorderProvider, err := options.newRecorderProvider(config, options.Scheme, options.Logger.WithName(\"events\"), options.makeBroadcaster) if err != nil { return nil, err } return \u0026cluster{ config: config, scheme: options.Scheme, cache: cache, fieldIndexes: cache, client: writeObj, apiReader: apiReader, recorderProvider: recorderProvider, mapper: mapper, logger: options.Logger, }, nil } 这里主要创建了cache与读写client ","date":"Aug 23, 2021","objectID":"/k8s-kubebuilder-deep-dive/:2:1","series":null,"tags":["k8s"],"title":"深入了解kubebuilder","uri":"/k8s-kubebuilder-deep-dive/#manager初始化"},{"categories":["cloud"],"content":" Cache初始化创建cache代码： // New initializes and returns a new Cache. func New(config *rest.Config, opts Options) (Cache, error) { opts, err := defaultOpts(config, opts) if err != nil { return nil, err } im := internal.NewInformersMap(config, opts.Scheme, opts.Mapper, *opts.Resync, opts.Namespace) return \u0026informerCache{InformersMap: im}, nil } New中调用了NewInformersMap来创建infermer map，分为structured、unstructured与metadata func NewInformersMap(config *rest.Config, scheme *runtime.Scheme, mapper meta.RESTMapper, resync time.Duration, namespace string) *InformersMap { return \u0026InformersMap{ structured: newStructuredInformersMap(config, scheme, mapper, resync, namespace), unstructured: newUnstructuredInformersMap(config, scheme, mapper, resync, namespace), metadata: newMetadataInformersMap(config, scheme, mapper, resync, namespace), Scheme: scheme, } } 最终都是调用newSpecificInformersMap // newStructuredInformersMap creates a new InformersMap for structured objects. func newStructuredInformersMap(config *rest.Config, scheme *runtime.Scheme, mapper meta.RESTMapper, resync time.Duration, namespace string) *specificInformersMap { return newSpecificInformersMap(config, scheme, mapper, resync, namespace, createStructuredListWatch) } func newSpecificInformersMap(config *rest.Config, scheme *runtime.Scheme, mapper meta.RESTMapper, resync time.Duration, namespace string, createListWatcher createListWatcherFunc) *specificInformersMap { ip := \u0026specificInformersMap{ config: config, Scheme: scheme, mapper: mapper, informersByGVK: make(map[schema.GroupVersionKind]*MapEntry), codecs: serializer.NewCodecFactory(scheme), paramCodec: runtime.NewParameterCodec(scheme), resync: resync, startWait: make(chan struct{}), createListWatcher: createListWatcher, namespace: namespace, } return ip } func createStructuredListWatch(gvk schema.GroupVersionKind, ip *specificInformersMap) (*cache.ListWatch, error) { // Kubernetes APIs work against Resources, not GroupVersionKinds. Map the // groupVersionKind to the Resource API we will use. mapping, err := ip.mapper.RESTMapping(gvk.GroupKind(), gvk.Version) if err != nil { return nil, err } client, err := apiutil.RESTClientForGVK(gvk, false, ip.config, ip.codecs) if err != nil { return nil, err } listGVK := gvk.GroupVersion().WithKind(gvk.Kind + \"List\") listObj, err := ip.Scheme.New(listGVK) if err != nil { return nil, err } // TODO: the functions that make use of this ListWatch should be adapted to // pass in their own contexts instead of relying on this fixed one here. ctx := context.TODO() // Create a new ListWatch for the obj return \u0026cache.ListWatch{ ListFunc: func(opts metav1.ListOptions) (runtime.Object, error) { res := listObj.DeepCopyObject() isNamespaceScoped := ip.namespace != \"\" \u0026\u0026 mapping.Scope.Name() != meta.RESTScopeNameRoot err := client.Get().NamespaceIfScoped(ip.namespace, isNamespaceScoped).Resource(mapping.Resource.Resource).VersionedParams(\u0026opts, ip.paramCodec).Do(ctx).Into(res) return res, err }, // Setup the watch function WatchFunc: func(opts metav1.ListOptions) (watch.Interface, error) { // Watch needs to be set to true separately opts.Watch = true isNamespaceScoped := ip.namespace != \"\" \u0026\u0026 mapping.Scope.Name() != meta.RESTScopeNameRoot return client.Get().NamespaceIfScoped(ip.namespace, isNamespaceScoped).Resource(mapping.Resource.Resource).VersionedParams(\u0026opts, ip.paramCodec).Watch(ctx) }, }, nil } 在newSpecificInformersMap中通过informersByGVK来记录schema中每个GVK对象与informer的对应关系，使用时可根据GVK得到informer再去List/Get。 newSpecificInformersMap中的createListWatcher来初始化ListWatch对象。 ","date":"Aug 23, 2021","objectID":"/k8s-kubebuilder-deep-dive/:2:2","series":null,"tags":["k8s"],"title":"深入了解kubebuilder","uri":"/k8s-kubebuilder-deep-dive/#cache初始化"},{"categories":["cloud"],"content":" Client初始化client这里有多种类型，apiReader直接从apiserver读取对象，writeObj可以从apiserver或者cache中读取数据。 apiReader, err := client.New(config, clientOptions) if err != nil { return nil, err } func New(config *rest.Config, options Options) (Client, error) { if config == nil { return nil, fmt.Errorf(\"must provide non-nil rest.Config to client.New\") } // Init a scheme if none provided if options.Scheme == nil { options.Scheme = scheme.Scheme } // Init a Mapper if none provided if options.Mapper == nil { var err error options.Mapper, err = apiutil.NewDynamicRESTMapper(config) if err != nil { return nil, err } } // 从cache中读取 clientcache := \u0026clientCache{ config: config, scheme: options.Scheme, mapper: options.Mapper, codecs: serializer.NewCodecFactory(options.Scheme), structuredResourceByType: make(map[schema.GroupVersionKind]*resourceMeta), unstructuredResourceByType: make(map[schema.GroupVersionKind]*resourceMeta), } rawMetaClient, err := metadata.NewForConfig(config) if err != nil { return nil, fmt.Errorf(\"unable to construct metadata-only client for use as part of client: %w\", err) } c := \u0026client{ typedClient: typedClient{ cache: clientcache, paramCodec: runtime.NewParameterCodec(options.Scheme), }, unstructuredClient: unstructuredClient{ cache: clientcache, paramCodec: noConversionParamCodec{}, }, metadataClient: metadataClient{ client: rawMetaClient, restMapper: options.Mapper, }, scheme: options.Scheme, mapper: options.Mapper, } return c, nil } writeObj实现了读写分离的Client，写直连apiserver，读获取在cache中则直接读取cache，否则通过clientset。 writeObj, err := options.ClientBuilder. WithUncached(options.ClientDisableCacheFor...). Build(cache, config, clientOptions) if err != nil { return nil, err } func (n *newClientBuilder) Build(cache cache.Cache, config *rest.Config, options client.Options) (client.Client, error) { // Create the Client for Write operations. c, err := client.New(config, options) if err != nil { return nil, err } return client.NewDelegatingClient(client.NewDelegatingClientInput{ CacheReader: cache, Client: c, UncachedObjects: n.uncached, }) } // 读写分离client func NewDelegatingClient(in NewDelegatingClientInput) (Client, error) { uncachedGVKs := map[schema.GroupVersionKind]struct{}{} for _, obj := range in.UncachedObjects { gvk, err := apiutil.GVKForObject(obj, in.Client.Scheme()) if err != nil { return nil, err } uncachedGVKs[gvk] = struct{}{} } return \u0026delegatingClient{ scheme: in.Client.Scheme(), mapper: in.Client.RESTMapper(), Reader: \u0026delegatingReader{ CacheReader: in.CacheReader, ClientReader: in.Client, scheme: in.Client.Scheme(), uncachedGVKs: uncachedGVKs, cacheUnstructured: in.CacheUnstructured, }, Writer: in.Client, StatusClient: in.Client, }, nil } // Get retrieves an obj for a given object key from the Kubernetes Cluster. func (d *delegatingReader) Get(ctx context.Context, key ObjectKey, obj Object) error { //根据是否cached选择client if isUncached, err := d.shouldBypassCache(obj); err != nil { return err } else if isUncached { return d.ClientReader.Get(ctx, key, obj) } return d.CacheReader.Get(ctx, key, obj) } ","date":"Aug 23, 2021","objectID":"/k8s-kubebuilder-deep-dive/:2:3","series":null,"tags":["k8s"],"title":"深入了解kubebuilder","uri":"/k8s-kubebuilder-deep-dive/#client初始化"},{"categories":["cloud"],"content":" Controller初始化Controller初始化代码如下： func (r *GameReconciler) SetupWithManager(mgr ctrl.Manager) error { ctrl.NewControllerManagedBy(mgr). WithOptions(controller.Options{ MaxConcurrentReconciles: 3, }). For(\u0026myappv1.Game{}). // Reconcile资源 Owns(\u0026appsv1.Deployment{}). // 监听Owner是当前资源的Deployment Complete(r) return nil } // Complete builds the Application ControllerManagedBy. func (blder *Builder) Complete(r reconcile.Reconciler) error { _, err := blder.Build(r) return err } // Build builds the Application ControllerManagedBy and returns the Controller it created. func (blder *Builder) Build(r reconcile.Reconciler) (controller.Controller, error) { if r == nil { return nil, fmt.Errorf(\"must provide a non-nil Reconciler\") } if blder.mgr == nil { return nil, fmt.Errorf(\"must provide a non-nil Manager\") } if blder.forInput.err != nil { return nil, blder.forInput.err } // Checking the reconcile type exist or not if blder.forInput.object == nil { return nil, fmt.Errorf(\"must provide an object for reconciliation\") } // Set the Config blder.loadRestConfig() // Set the ControllerManagedBy if err := blder.doController(r); err != nil { return nil, err } // Set the Watch if err := blder.doWatch(); err != nil { return nil, err } return blder.ctrl, nil } 初始化Controller调用ctrl.NewControllerManagedBy来创建Builder，填充配置，最后通过Build方法完成初始化，主要做了三件事 设置配置 doController来创建controller doWatch来设置需要监听的资源 先看controller初始化 func (blder *Builder) doController(r reconcile.Reconciler) error { ctrlOptions := blder.ctrlOptions if ctrlOptions.Reconciler == nil { ctrlOptions.Reconciler = r } gvk, err := getGvk(blder.forInput.object, blder.mgr.GetScheme()) if err != nil { return err } // Setup the logger. if ctrlOptions.Log == nil { ctrlOptions.Log = blder.mgr.GetLogger() } ctrlOptions.Log = ctrlOptions.Log.WithValues(\"reconciler group\", gvk.Group, \"reconciler kind\", gvk.Kind) // Build the controller and return. blder.ctrl, err = newController(blder.getControllerName(gvk), blder.mgr, ctrlOptions) return err } func New(name string, mgr manager.Manager, options Options) (Controller, error) { c, err := NewUnmanaged(name, mgr, options) if err != nil { return nil, err } // Add the controller as a Manager components return c, mgr.Add(c) } func NewUnmanaged(name string, mgr manager.Manager, options Options) (Controller, error) { if options.Reconciler == nil { return nil, fmt.Errorf(\"must specify Reconciler\") } if len(name) == 0 { return nil, fmt.Errorf(\"must specify Name for Controller\") } if options.Log == nil { options.Log = mgr.GetLogger() } if options.MaxConcurrentReconciles \u003c= 0 { options.MaxConcurrentReconciles = 1 } if options.CacheSyncTimeout == 0 { options.CacheSyncTimeout = 2 * time.Minute } if options.RateLimiter == nil { options.RateLimiter = workqueue.DefaultControllerRateLimiter() } // Inject dependencies into Reconciler if err := mgr.SetFields(options.Reconciler); err != nil { return nil, err } // Create controller with dependencies set return \u0026controller.Controller{ Do: options.Reconciler, MakeQueue: func() workqueue.RateLimitingInterface { return workqueue.NewNamedRateLimitingQueue(options.RateLimiter, name) }, MaxConcurrentReconciles: options.MaxConcurrentReconciles, CacheSyncTimeout: options.CacheSyncTimeout, SetFields: mgr.SetFields, Name: name, Log: options.Log.WithName(\"controller\").WithName(name), }, nil } doController调用controller.New来创建controller并添加到manager，在NewUnmanaged可以看到我们熟悉的配置，与上文sample-controller类似这里也设置了工作队列、最大Worker数等。 doWatch代码如下 func (blder *Builder) doWatch() error { // Reconcile type typeForSrc, err := blder.project(blder.forInput.object, blder.forInput.objectProjection) if err != nil { return err } src := \u0026source.Kind{Type: typeForSrc} hdler := \u0026handler.EnqueueRequestForObject{} allPredicates := append(blder.globalPredicates, blder.forInput.predicates...) if err := blder.ctrl.Watch(src, hdler, allPredicates...); err != nil { return err } // Watches the managed types for _, own := range blder.ownsInput { typeForSrc, err := blder.project(own.object, own","date":"Aug 23, 2021","objectID":"/k8s-kubebuilder-deep-dive/:2:4","series":null,"tags":["k8s"],"title":"深入了解kubebuilder","uri":"/k8s-kubebuilder-deep-dive/#controller初始化"},{"categories":["cloud"],"content":" Manager启动最后来看Manager启动流程 func (cm *controllerManager) Start(ctx context.Context) (err error) { if err := cm.Add(cm.cluster); err != nil { return fmt.Errorf(\"failed to add cluster to runnables: %w\", err) } cm.internalCtx, cm.internalCancel = context.WithCancel(ctx) stopComplete := make(chan struct{}) defer close(stopComplete) defer func() { stopErr := cm.engageStopProcedure(stopComplete) }() cm.errChan = make(chan error) if cm.metricsListener != nil { go cm.serveMetrics() } // Serve health probes if cm.healthProbeListener != nil { go cm.serveHealthProbes() } go cm.startNonLeaderElectionRunnables() go func() { if cm.resourceLock != nil { err := cm.startLeaderElection() if err != nil { cm.errChan \u003c- err } } else { // Treat not having leader election enabled the same as being elected. cm.startLeaderElectionRunnables() close(cm.elected) } }() select { case \u003c-ctx.Done(): // We are done return nil case err := \u003c-cm.errChan: // Error starting or running a runnable return err } } 主要流程包括： 启动监控服务 启动健康检查服务 启动非选主服务 启动选主服务 对于非选主服务，代码如下 func (cm *controllerManager) startNonLeaderElectionRunnables() { cm.mu.Lock() defer cm.mu.Unlock() cm.waitForCache(cm.internalCtx) // Start the non-leaderelection Runnables after the cache has synced for _, c := range cm.nonLeaderElectionRunnables { cm.startRunnable(c) } } func (cm *controllerManager) waitForCache(ctx context.Context) { if cm.started { return } for _, cache := range cm.caches { cm.startRunnable(cache) } for _, cache := range cm.caches { cache.GetCache().WaitForCacheSync(ctx) } cm.started = true } 启动cache，启动其他服务，对于选主服务也类似，初始化controller时会加入到选主服务队列，即最后启动Controller func (c *Controller) Start(ctx context.Context) error { ... c.Queue = c.MakeQueue() defer c.Queue.ShutDown() // needs to be outside the iife so that we shutdown after the stop channel is closed err := func() error { defer c.mu.Unlock() defer utilruntime.HandleCrash() for _, watch := range c.startWatches { c.Log.Info(\"Starting EventSource\", \"source\", watch.src) if err := watch.src.Start(ctx, watch.handler, c.Queue, watch.predicates...); err != nil { return err } } for _, watch := range c.startWatches { syncingSource, ok := watch.src.(source.SyncingSource) if !ok { continue } if err := func() error { // use a context with timeout for launching sources and syncing caches. sourceStartCtx, cancel := context.WithTimeout(ctx, c.CacheSyncTimeout) defer cancel() if err := syncingSource.WaitForSync(sourceStartCtx); err != nil { err := fmt.Errorf(\"failed to wait for %s caches to sync: %w\", c.Name, err) c.Log.Error(err, \"Could not wait for Cache to sync\") return err } return nil }(); err != nil { return err } } ... for i := 0; i \u003c c.MaxConcurrentReconciles; i++ { go wait.UntilWithContext(ctx, func(ctx context.Context) { for c.processNextWorkItem(ctx) { } }, c.JitterPeriod) } c.Started = true return nil }() if err != nil { return err } \u003c-ctx.Done() c.Log.Info(\"Stopping workers\") return nil } func (c *Controller) processNextWorkItem(ctx context.Context) bool { obj, shutdown := c.Queue.Get() ... c.reconcileHandler(ctx, obj) return true } func (c *Controller) reconcileHandler(ctx context.Context, obj interface{}) { // Make sure that the the object is a valid request. req, ok := obj.(reconcile.Request) ... if result, err := c.Do.Reconcile(ctx, req); err != nil { ... } Controller启动主要包括 等待cache同步 启动多个processNextWorkItem 每个Worker调用c.Do.Reconcile来进行数据处理 与sample-controller工作流程一致，不断获取工作队列中的数据调用Reconcile进行调谐。 ","date":"Aug 23, 2021","objectID":"/k8s-kubebuilder-deep-dive/:2:5","series":null,"tags":["k8s"],"title":"深入了解kubebuilder","uri":"/k8s-kubebuilder-deep-dive/#manager启动"},{"categories":["cloud"],"content":" 流程归纳至此，通过kubebuilder生成代码的主要逻辑已经明朗，对比sample-controller其实整体流程类似，只是kubebuilder通过controller-runtime已经帮我们做了很多工作，如client、cache的初始化，controller的运行框架，我们只需要关心Reconcile逻辑即可。 初始化manager，创建client与cache 创建controller，对于监听资源会创建对应informer并添加回调函数 启动manager，启动cache与controller ","date":"Aug 23, 2021","objectID":"/k8s-kubebuilder-deep-dive/:2:6","series":null,"tags":["k8s"],"title":"深入了解kubebuilder","uri":"/k8s-kubebuilder-deep-dive/#流程归纳"},{"categories":["cloud"],"content":" 总结kubebuilder大大简化了开发Operator的流程，了解其背后的原理有利于我们对Operator进行调优，能更好地应用于生产。 ","date":"Aug 23, 2021","objectID":"/k8s-kubebuilder-deep-dive/:3:0","series":null,"tags":["k8s"],"title":"深入了解kubebuilder","uri":"/k8s-kubebuilder-deep-dive/#总结"},{"categories":["cloud"],"content":" 引用 https://github.com/kubernetes/sample-controller https://book.kubebuilder.io/architecture.html https://developer.aliyun.com/article/719215 ","date":"Aug 23, 2021","objectID":"/k8s-kubebuilder-deep-dive/:4:0","series":null,"tags":["k8s"],"title":"深入了解kubebuilder","uri":"/k8s-kubebuilder-deep-dive/#引用"},{"categories":["cloud"],"content":"Kubernetes提供了众多的扩展功能，比如CRD、CRI、CSI等等，强大的扩展功能让k8s迅速占领市场。Operator模式可以实现CRD并管理自定义资源的生命周期，本文基于kubebuilder快速实现一个Operator，示例源码见mygame。 ","date":"Aug 12, 2021","objectID":"/how-to-write-a-k8s-operator/:0:0","series":null,"tags":["k8s"],"title":"快速实现一个Kubernetes Operator","uri":"/how-to-write-a-k8s-operator/#"},{"categories":["cloud"],"content":" Kubebuilderkubebuilder是一个官方提供快速实现Operator的工具包，可快速生成k8s的CRD、Controller、Webhook，用户只需要实现业务逻辑。 类似工具还有operader-sdk，目前正在与Kubebuilder融合 kubebuilder封装了controller-runtime与controller-tools，通过controller-gen来生产代码，简化了用户创建Operator的步骤。 一般创建Operator流程如下： 创建工作目录，初始化项目 创建API，填充字段 创建Controller，编写核心协调逻辑(Reconcile) 创建Webhook，实现接口，可选 验证测试 发布到集群中 ","date":"Aug 12, 2021","objectID":"/how-to-write-a-k8s-operator/:1:0","series":null,"tags":["k8s"],"title":"快速实现一个Kubernetes Operator","uri":"/how-to-write-a-k8s-operator/#kubebuilder"},{"categories":["cloud"],"content":" 示例我们准备创建一个2048的游戏，对外可以提供服务，也能方便地扩缩容。 ","date":"Aug 12, 2021","objectID":"/how-to-write-a-k8s-operator/:2:0","series":null,"tags":["k8s"],"title":"快速实现一个Kubernetes Operator","uri":"/how-to-write-a-k8s-operator/#示例"},{"categories":["cloud"],"content":" 准备环境首先你需要有Kubernetes、Docker、Golang相关环境。 Linux下安装kubebuilder curl -L -o kubebuilder https://go.kubebuilder.io/dl/latest/$(go env GOOS)/$(go env GOARCH) chmod +x kubebuilder \u0026\u0026 mv kubebuilder /usr/local/bin/ ","date":"Aug 12, 2021","objectID":"/how-to-write-a-k8s-operator/:2:1","series":null,"tags":["k8s"],"title":"快速实现一个Kubernetes Operator","uri":"/how-to-write-a-k8s-operator/#准备环境"},{"categories":["cloud"],"content":" 创建项目mkdir -p ~/work/mygame \u0026\u0026 cd $_ kubebuilder init --domain qingwave.github.io --repo qingwave.github.io/mygame ","date":"Aug 12, 2021","objectID":"/how-to-write-a-k8s-operator/:2:2","series":null,"tags":["k8s"],"title":"快速实现一个Kubernetes Operator","uri":"/how-to-write-a-k8s-operator/#创建项目"},{"categories":["cloud"],"content":" 创建APIkubebuilder create api --group myapp --version v1 --kind Game Create Resource [y/n] y #生成CR Create Controller [y/n] y #生成Controller 目录结构如下： ├── api │ └── v1 # CRD定义 ├── bin │ └── controller-gen ├── config │ ├── crd # crd配置 │ ├── default │ ├── manager # operator部署文件 │ ├── prometheus │ ├── rbac │ └── samples # cr示例 ├── controllers │ ├── game_controller.go # controller逻辑 │ └── suite_test.go ├── Dockerfile ├── go.mod ├── go.sum ├── hack │ └── boilerplate.go.txt # 头文件模板 ├── main.go # 项目主函数 ├── Makefile └── PROJECT #项目元数据 ","date":"Aug 12, 2021","objectID":"/how-to-write-a-k8s-operator/:2:3","series":null,"tags":["k8s"],"title":"快速实现一个Kubernetes Operator","uri":"/how-to-write-a-k8s-operator/#创建api"},{"categories":["cloud"],"content":" 编写API在mygame/api/v1/game_types.go定义我们需要的字段 Spec配置如下 type GameSpec struct { // Number of desired pods. This is a pointer to distinguish between explicit // zero and not specified. Defaults to 1. // +optional //+kubebuilder:default:=1 //+kubebuilder:validation:Minimum:=1 Replicas *int32 `json:\"replicas,omitempty\" protobuf:\"varint,1,opt,name=replicas\"` // Docker image name // +optional Image string `json:\"image,omitempty\"` // Ingress Host name Host string `json:\"host,omitempty\"` } kubebuilder:default可以设置默认值 Status定义如下 const ( Running = \"Running\" Pending = \"Pending\" NotReady = \"NotReady\" Failed = \"Failed\" ) type GameStatus struct { // Phase is the phase of guestbook Phase string `json:\"phase,omitempty\"` // replicas is the number of Pods created by the StatefulSet controller. Replicas int32 `json:\"replicas\"` // readyReplicas is the number of Pods created by the StatefulSet controller that have a Ready Condition. ReadyReplicas int32 `json:\"readyReplicas\"` // LabelSelector is label selectors for query over pods that should match the replica count used by HPA. LabelSelector string `json:\"labelSelector,omitempty\"` } 另外需要添加scale接口 //+kubebuilder:subresource:scale:specpath=.spec.replicas,statuspath=.status.replicas,selectorpath=.status.labelSelector 添加kubectl展示参数 //+kubebuilder:printcolumn:name=\"Phase\",type=\"string\",JSONPath=\".status.phase\",description=\"The phase of game.\" //+kubebuilder:printcolumn:name=\"Host\",type=\"string\",JSONPath=\".spec.host\",description=\"The Host Address.\" //+kubebuilder:printcolumn:name=\"DESIRED\",type=\"integer\",JSONPath=\".spec.replicas\",description=\"The desired number of pods.\" //+kubebuilder:printcolumn:name=\"CURRENT\",type=\"integer\",JSONPath=\".status.replicas\",description=\"The number of currently all pods.\" //+kubebuilder:printcolumn:name=\"READY\",type=\"integer\",JSONPath=\".status.readyReplicas\",description=\"The number of pods ready.\" //+kubebuilder:printcolumn:name=\"AGE\",type=\"date\",JSONPath=\".metadata.creationTimestamp\",description=\"CreationTimestamp is a timestamp representing the server time when this object was created. It is not guaranteed to be set in happens-before order across separate operations. Clients may not set this value. It is represented in RFC3339 form and is in UTC.\" ","date":"Aug 12, 2021","objectID":"/how-to-write-a-k8s-operator/:2:4","series":null,"tags":["k8s"],"title":"快速实现一个Kubernetes Operator","uri":"/how-to-write-a-k8s-operator/#编写api"},{"categories":["cloud"],"content":" 编写Controller逻辑Controller的核心逻辑在Reconcile中，我们只需要填充自己的业务逻辑 func (r *GameReconciler) Reconcile(ctx context.Context, req ctrl.Request) (ctrl.Result, error) { logger := log.FromContext(ctx) logger.Info(\"revice reconcile event\", \"name\", req.String()) // 获取game对象 game := \u0026myappv1.Game{} if err := r.Get(ctx, req.NamespacedName, game); err != nil { return ctrl.Result{}, client.IgnoreNotFound(err) } // 如果处在删除中直接跳过 if game.DeletionTimestamp != nil { logger.Info(\"game in deleting\", \"name\", req.String()) return ctrl.Result{}, nil } // 同步资源，如果资源不存在创建deployment、ingress、service，并更新status if err := r.syncGame(ctx, game); err != nil { logger.Error(err, \"failed to sync game\", \"name\", req.String()) return ctrl.Result{}, nil } return ctrl.Result{}, nil } 添加rbac配置 //+kubebuilder:rbac:groups=apps,resources=deployments,verbs=get;list;watch;create;update;patch;delete //+kubebuilder:rbac:groups=apps,resources=deployments/status,verbs=get;update;patch //+kubebuilder:rbac:groups=core,resources=services,verbs=get;list;watch;create;update;patch;delete //+kubebuilder:rbac:groups=networking,resources=ingresses,verbs=get;list;watch;create;update;patch;delete 具体syncGame逻辑如下 func (r *GameReconciler) syncGame(ctx context.Context, obj *myappv1.Game) error { logger := log.FromContext(ctx) game := obj.DeepCopy() name := types.NamespacedName{ Namespace: game.Namespace, Name: game.Name, } // 构造owner owner := []metav1.OwnerReference{ { APIVersion: game.APIVersion, Kind: game.Kind, Name: game.Name, Controller: pointer.BoolPtr(true), BlockOwnerDeletion: pointer.BoolPtr(true), UID: game.UID, }, } labels := game.Labels labels[gameLabelName] = game.Name meta := metav1.ObjectMeta{ Name: game.Name, Namespace: game.Namespace, Labels: labels, OwnerReferences: owner, } // 获取对应deployment, 如不存在则创建 deploy := \u0026appsv1.Deployment{} if err := r.Get(ctx, name, deploy); err != nil { if !errors.IsNotFound(err) { return err } deploy = \u0026appsv1.Deployment{ ObjectMeta: meta, Spec: getDeploymentSpec(game, labels), } if err := r.Create(ctx, deploy); err != nil { return err } logger.Info(\"create deployment success\", \"name\", name.String()) } else { // 如果存在对比和game生成的deployment是否一致，不一致则更新 want := getDeploymentSpec(game, labels) get := getSpecFromDeployment(deploy) if !reflect.DeepEqual(want, get) { deploy = \u0026appsv1.Deployment{ ObjectMeta: meta, Spec: want, } if err := r.Update(ctx, deploy); err != nil { return err } logger.Info(\"update deployment success\", \"name\", name.String()) } } //service创建 svc := \u0026corev1.Service{} if err := r.Get(ctx, name, svc); err != nil { ... } // ingress创建 ing := \u0026networkingv1.Ingress{} if err := r.Get(ctx, name, ing); err != nil { ... } newStatus := myappv1.GameStatus{ Replicas: *game.Spec.Replicas, ReadyReplicas: deploy.Status.ReadyReplicas, } if newStatus.Replicas == newStatus.ReadyReplicas { newStatus.Phase = myappv1.Running } else { newStatus.Phase = myappv1.NotReady } // 更新状态 if !reflect.DeepEqual(game.Status, newStatus) { game.Status = newStatus logger.Info(\"update game status\", \"name\", name.String()) return r.Client.Status().Update(ctx, game) } return nil } 默认情况下生成的controller只监听自定义资源，在示例中我们也需要监听game的子资源，如监听deployment是否符合预期 // SetupWithManager sets up the controller with the Manager. func (r *GameReconciler) SetupWithManager(mgr ctrl.Manager) error { // 创建controller c, err := controller.New(\"game-controller\", mgr, controller.Options{ Reconciler: r, MaxConcurrentReconciles: 3, //controller运行的worker数 }) if err != nil { return err } //监听自定义资源 if err := c.Watch(\u0026source.Kind{Type: \u0026myappv1.Game{}}, \u0026handler.EnqueueRequestForObject{}); err != nil { return err } //监听deployment,将owner信息即game namespace/name添加到队列 if err := c.Watch(\u0026source.Kind{Type: \u0026appsv1.Deployment{}}, \u0026handler.EnqueueRequestForOwner{ OwnerType: \u0026myappv1.Game{}, IsController: true, }); err != nil { return err } return nil } ","date":"Aug 12, 2021","objectID":"/how-to-write-a-k8s-operator/:2:5","series":null,"tags":["k8s"],"title":"快速实现一个Kubernetes Operator","uri":"/how-to-write-a-k8s-operator/#编写controller逻辑"},{"categories":["cloud"],"content":" 部署验证安装CRD make install 本地运行operator make run 修改sample文件config/samples/myapp_v1_game.yaml apiVersion: myapp.qingwave.github.io/v1 kind: Game metadata: name: game-sample spec: replicas: 1 image: alexwhen/docker-2048 host: mygame.io 部署game-sample kubectl apply -f config/samples/myapp_v1_game.yaml 查看game自定义资源状态 # 查看game kubectl get game NAME PHASE HOST DESIRED CURRENT READY AGE game-sample Running mygame.io 1 1 1 6m # 查看deploy kubectl get deploy game-sample NAME READY UP-TO-DATE AVAILABLE AGE game-sample 1/1 1 1 6m # 查看ingress kubectl get ing game-sample NAME CLASS HOSTS ADDRESS PORTS AGE game-sample \u003cnone\u003e mygame.io 192.168.49.2 80 7m 验证应用，在/etc/hosts中添加\u003cIngress ADDRESS Ip\u003e mygame.io，访问浏览器如下图所示 验证扩容 kubectl scale games.myapp.qingwave.github.io game-sample --replicas 2 game.myapp.qingwave.github.io/game-sample scaled # 扩容后 kubectl get games.myapp.qingwave.github.io NAME PHASE HOST DESIRED CURRENT READY AGE game-sample Running mygame.io 2 2 2 7m 如需部署Operator到集群中，可参考官方文档，制作镜像并上传，运行make deploy ","date":"Aug 12, 2021","objectID":"/how-to-write-a-k8s-operator/:2:6","series":null,"tags":["k8s"],"title":"快速实现一个Kubernetes Operator","uri":"/how-to-write-a-k8s-operator/#部署验证"},{"categories":["cloud"],"content":" Webhook通常我们需要与CR自定义资源设置部分字段的默认值，或者验证字段是否合法，这就需要自己实现Webhook，Kubebuilder也提供了Webhook的功能。 通过设置--defaulting可创建mutatingadmissionwebhook类型准入控制器，用来修改传入资源；参数--programmatic-validation可创建validatingadmissionwebhook，用来验证传入资源 在资源创建、修改时apiserver会通过http调用webhook提供的接口，所以会带来额外开销，简单的验证工作可通过//+kubebuilder:validation注解，直接通过openapi验证，性能更好 kubebuilder create webhook --group myapp --version v1 --kind Game --defaulting --programmatic-validation 生成文件在api/v1/game_webhook.go Default接口可实现修改资源，根据kubebuilder注释,当game资源create与update时，调用这个接口 //+kubebuilder:webhook:path=/mutate-myapp-qingwave-github-io-v1-game,mutating=true,failurePolicy=fail,sideEffects=None,groups=myapp.qingwave.github.io,resources=games,verbs=create;update,versions=v1,name=mgame.kb.io,admissionReviewVersions={v1,v1beta1} const ( defaultImage = `alexwhen/docker-2048` ) // Default implements webhook.Defaulter so a webhook will be registered for the type func (r *Game) Default() { gamelog.Info(\"default\", \"name\", r.Name) // 设置默认镜像 if r.Spec.Image == \"\" { r.Spec.Image = defaultImage } // 设置默认Host if r.Spec.Host == \"\" { r.Spec.Host = fmt.Sprintf(\"%s.%s.mygame.io\", r.Name, r.Namespace) } } 同样的通过ValidateCreate、ValidateUpdate可实现validating webhook func (r *Game) ValidateCreate() error { gamelog.Info(\"validate create\", \"name\", r.Name) // Host不能包括通配符 if strings.Contains(r.Spec.Host, \"*\") { return errors.New(\"host should not contain *\") } return nil } 本地验证webhook需要配置证书，在集群中测试更方便点，可参考官方文档。 ","date":"Aug 12, 2021","objectID":"/how-to-write-a-k8s-operator/:2:7","series":null,"tags":["k8s"],"title":"快速实现一个Kubernetes Operator","uri":"/how-to-write-a-k8s-operator/#webhook"},{"categories":["cloud"],"content":" 总结至此，我们已经实现了一个功能完全的game-operator，可以管理game资源的生命周期，创建/更新game时会自动创建deployment、service、ingress等资源，当deployment被误删或者误修改时也可以自动回复到期望状态，也实现了scale接口。 通过kubebuiler大大简化了开发operator的成本，我们只需要关心业务逻辑即可，不需要再手动去创建client/controller等，但同时kubebuilder生成的代码中屏蔽了很多细节，比如controller的最大worker数、同步时间、队列类型等参数设置，只有了解operator的原理才更好应用于生产。 ","date":"Aug 12, 2021","objectID":"/how-to-write-a-k8s-operator/:3:0","series":null,"tags":["k8s"],"title":"快速实现一个Kubernetes Operator","uri":"/how-to-write-a-k8s-operator/#总结"},{"categories":["cloud"],"content":" 引用 https://book.kubebuilder.io/ ","date":"Aug 12, 2021","objectID":"/how-to-write-a-k8s-operator/:4:0","series":null,"tags":["k8s"],"title":"快速实现一个Kubernetes Operator","uri":"/how-to-write-a-k8s-operator/#引用"},{"categories":["cloud"],"content":"Pod水平自动扩缩（Horizontal Pod Autoscaler, 简称HPA）可以基于 CPU/MEM 利用率自动扩缩Deployment、StatefulSet 中的 Pod 数量，同时也可以基于其他应程序提供的自定义度量指标来执行自动扩缩。默认HPA可以满足一些简单场景，对于生产环境并不一定适合，本文主要分析HPA的不足与优化方式。 ","date":"Apr 02, 2021","objectID":"/k8s-hpa-enchance/:0:0","series":null,"tags":["k8s","hpa"],"title":"优化Kubernetes横向扩缩HPA","uri":"/k8s-hpa-enchance/#"},{"categories":["cloud"],"content":" HPA Resource类型不足默认HPA提供了Resource类型，通过CPU/MEM使用率指标（由metrics-server提供原始指标）来扩缩应用。 ","date":"Apr 02, 2021","objectID":"/k8s-hpa-enchance/:1:0","series":null,"tags":["k8s","hpa"],"title":"优化Kubernetes横向扩缩HPA","uri":"/k8s-hpa-enchance/#hpa-resource类型不足"},{"categories":["cloud"],"content":" 使用率计算方式在Resource类型中，使用率计算是通过request而不是limit，源码如下： // 获取Pod resource request func calculatePodRequests(pods []*v1.Pod, resource v1.ResourceName) (map[string]int64, error) { requests := make(map[string]int64, len(pods)) for _, pod := range pods { podSum := int64(0) for _, container := range pod.Spec.Containers { if containerRequest, ok := container.Resources.Requests[resource]; ok { podSum += containerRequest.MilliValue() } else { return nil, fmt.Errorf(\"missing request for %s\", resource) } } requests[pod.Name] = podSum } return requests, nil } // 计算使用率 func GetResourceUtilizationRatio(metrics PodMetricsInfo, requests map[string]int64, targetUtilization int32) (utilizationRatio float64, currentUtilization int32, rawAverageValue int64, err error) { metricsTotal := int64(0) requestsTotal := int64(0) numEntries := 0 for podName, metric := range metrics { request, hasRequest := requests[podName] if !hasRequest { // we check for missing requests elsewhere, so assuming missing requests == extraneous metrics continue } metricsTotal += metric.Value requestsTotal += request numEntries++ } currentUtilization = int32((metricsTotal * 100) / requestsTotal) return float64(currentUtilization) / float64(targetUtilization), currentUtilization, metricsTotal / int64(numEntries), nil } 通常在Paas平台中会对资源进行超配，limit即用户请求资源，request即实际分配资源，如果按照request来计算使用率（会超过100%）是不符合预期的。相关issue见72811，目前还存在争论。可以修改源码，或者使用自定义指标来代替。 ","date":"Apr 02, 2021","objectID":"/k8s-hpa-enchance/:1:1","series":null,"tags":["k8s","hpa"],"title":"优化Kubernetes横向扩缩HPA","uri":"/k8s-hpa-enchance/#使用率计算方式"},{"categories":["cloud"],"content":" 多容器Pod使用率问题默认提供的Resource类型的HPA，通过上述方式计算资源使用率，核心方式如下： metricsTotal = sum(pod.container.metricValue) requestsTotal = sum(pod.container.Request) currentUtilization = int32((metricsTotal * 100) / requestsTotal) 计算出所有container的资源使用量再比总的申请量，对于单容器Pod这没影响。但对于多容器Pod，比如Pod包含多个容器con1、con2(request都为1cpu)，con1使用率10%，con2使用率100%，HPA目标使用率60%，按照目前方式得到使用率为55%不会进行扩容，但实际con2已经达到资源瓶颈，势必会影响服务质量。当前系统中，多容器Pod通常都是1个主容器与多个sidecar，依赖主容器的指标更合适点。 好在1.20版本中已经支持了ContainerResource可以配置基于某个容器的资源使用率来进行扩缩，如果是之前的版本建议使用自定义指标替换。 ","date":"Apr 02, 2021","objectID":"/k8s-hpa-enchance/:1:2","series":null,"tags":["k8s","hpa"],"title":"优化Kubernetes横向扩缩HPA","uri":"/k8s-hpa-enchance/#多容器pod使用率问题"},{"categories":["cloud"],"content":" 性能问题","date":"Apr 02, 2021","objectID":"/k8s-hpa-enchance/:2:0","series":null,"tags":["k8s","hpa"],"title":"优化Kubernetes横向扩缩HPA","uri":"/k8s-hpa-enchance/#性能问题"},{"categories":["cloud"],"content":" 单线程架构默认的hpa-controller是单个Goroutine执行的，随着集群规模的增多，势必会成为性能瓶颈，目前默认hpa资源同步周期会15s，假设每个metric请求延时为100ms，当前架构只能支持150个HPA资源（保证在15s内同步一次） func (a *HorizontalController) Run(stopCh \u003c-chan struct{}) { // ... // start a single worker (we may wish to start more in the future) go wait.Until(a.worker, time.Second, stopCh) \u003c-stopCh } 可以通过调整worker数量来横向扩展，已提交PR。 ","date":"Apr 02, 2021","objectID":"/k8s-hpa-enchance/:2:1","series":null,"tags":["k8s","hpa"],"title":"优化Kubernetes横向扩缩HPA","uri":"/k8s-hpa-enchance/#单线程架构"},{"categories":["cloud"],"content":" 调用链路在hpa controller中一次hpa资源同步，需要调用多次apiserver接口，主要链路如下 通过scaleForResourceMappings得到scale资源 调用computeReplicasForMetrics获取metrics value 调用Scales().Update更新计算出的副本数 尤其在获取metrics value时，需要先调用apiserver，apiserver调用metrics-server/custom-metrics-server，当集群内存在大量hpa时可能会对apiserver性能产生一定影响。 ","date":"Apr 02, 2021","objectID":"/k8s-hpa-enchance/:2:2","series":null,"tags":["k8s","hpa"],"title":"优化Kubernetes横向扩缩HPA","uri":"/k8s-hpa-enchance/#调用链路"},{"categories":["cloud"],"content":" 其他对于自定义指标用户需要实现custom.metrics.k8s.io或external.metrics.k8s.io，目前已经有部分开源实现见custom-metrics-api。 另外，hpa核心的扩缩算法根据当前指标和期望指标来计算扩缩比例，并不适合所有场景，只使用线性增长的指标。 期望副本数 = ceil[当前副本数 * (当前指标 / 期望指标)] watermarkpodautoscaler提供了更灵活的扩缩算法，比如平均值、水位线等，可以作为参考。 ","date":"Apr 02, 2021","objectID":"/k8s-hpa-enchance/:3:0","series":null,"tags":["k8s","hpa"],"title":"优化Kubernetes横向扩缩HPA","uri":"/k8s-hpa-enchance/#其他"},{"categories":["cloud"],"content":" 总结Kubernetes提供原生的HPA只能满足一部分场景，如果要上生产环境，必须对其做一些优化，本文总结了当前HPA存在的不足，例如在性能、使用率计算方面，并提供了解决思路。 ","date":"Apr 02, 2021","objectID":"/k8s-hpa-enchance/:4:0","series":null,"tags":["k8s","hpa"],"title":"优化Kubernetes横向扩缩HPA","uri":"/k8s-hpa-enchance/#总结"},{"categories":["cloud"],"content":"在Prometheus分区实践中我们介绍了使用集群联邦与远程存储来扩展Prometheus以及监控数据持久化，但之前的分区方案存在一定不足，如分区配置较难维护，全局Prometheus存在性能瓶颈等，本文通过Thanos+Kvass实现更优雅的Prometheus扩展方案。 ","date":"Mar 30, 2021","objectID":"/prometheus-auto-federation/:0:0","series":null,"tags":["prometheus"],"title":"Prometheus高可用自动分区方案","uri":"/prometheus-auto-federation/#"},{"categories":["cloud"],"content":" 自动分区之前分区方案依赖Prometheus提供的hashmod方法，通过在配置中指定hash对象与modules进行散列（md5），每个分片只抓取相同job命中的对象，例如我们可以通过对node散列从而对cadvisor、node-exporter等job做分片。 通过这种方式可以简单的扩展Prometheus，降低其抓取压力，但是显而易见hashmod需要指定散列对象，每个job可能需要配置不同的对象如node、pod、ip等，随着采集对象增多，配置难以维护。直到看见了Kvass，Kvass是一个Prometheus横向扩展方案，可以不依赖hashmod动态调整target，支持数千万series规模。 Kvass核心架构如下： Kvass-Coordinator: 加载配置文件并进行服务发现，获取所有target，周期性分配target到kvass-sidecar，以及管理分片负载与扩缩容 Kvass-Sidecar: 根据Coordinator分发的target生成配置，以及代理Prometheus请求 通过Kvass可实现Prometheus动态横向扩展，而不依赖hashmod，灵活性更高。 ","date":"Mar 30, 2021","objectID":"/prometheus-auto-federation/:1:0","series":null,"tags":["prometheus"],"title":"Prometheus高可用自动分区方案","uri":"/prometheus-auto-federation/#自动分区"},{"categories":["cloud"],"content":" 全局查询另一个问题是在集群联邦中我们需要一个全局的Prometheus来聚合分区Prometheus的数据，依赖原生的/federate接口，随着数据量增多，全局Prometheus必然会达到性能瓶颈。高可用Prometheus集群解决方案Thanos中提供了全局查询功能，通过Thanos-Query与Thanos-Sidecar可实现查询多个Prometheus的数据，并支持了去重。 Thanos组件较多，核心架构如下： Thanos Query: 实现了Prometheus API，将来自下游组件提供的数据进行聚合最终返回给查询数据的client (如 grafana)，类似数据库中间件 Thanos Sidecar: 连接Prometheus，将其数据提供给Thanos Query查询，并且可将其上传到对象存储，以供长期存储 Thanos Store Gateway: 将对象存储的数据暴露给Thanos Query去查询 Thanos Ruler: 对监控数据进行评估和告警，还可以计算出新的监控数据，将这些新数据提供给Thanos Query查询并且可上传到对象存储，以供长期存储 Thanos Compact: 将对象存储中的数据进行压缩和降低采样率，加速大时间区间监控数据查询的速度 借助于Thanos提供的Query与Ruler我们可以实现全局查询与聚合。 ","date":"Mar 30, 2021","objectID":"/prometheus-auto-federation/:2:0","series":null,"tags":["prometheus"],"title":"Prometheus高可用自动分区方案","uri":"/prometheus-auto-federation/#全局查询"},{"categories":["cloud"],"content":" 最终方案Kvass+Thanos可实现Prometheus自动扩展、全局查询，再配合Remote Wirite实现数据支持化，通过Grafana展示监控数据 ","date":"Mar 30, 2021","objectID":"/prometheus-auto-federation/:3:0","series":null,"tags":["prometheus"],"title":"Prometheus高可用自动分区方案","uri":"/prometheus-auto-federation/#最终方案"},{"categories":["cloud"],"content":" 测试验证所有部署文件见prometheus-kvass git clone https://github.com/qingwave/kube-monitor.git kubectl apply -f kube-monitor/prometheus-kvass 结果如下： $ kubectl get po NAME READY STATUS RESTARTS AGE kvass-coordinator-7f65c546d9-vxgxr 2/2 Running 2 29h metrics-774949d94d-4btzh 1/1 Running 0 10s metrics-774949d94d-558gn 1/1 Running 1 29h metrics-774949d94d-gs8kc 1/1 Running 1 29h metrics-774949d94d-r85rc 1/1 Running 1 29h metrics-774949d94d-xhbk9 1/1 Running 0 10s metrics-774949d94d-z5mwk 1/1 Running 1 29h prometheus-rep-0-0 3/3 Running 0 49s prometheus-rep-0-1 3/3 Running 0 48s prometheus-rep-0-2 3/3 Running 0 19s thanos-query-b469b648f-ltxth 1/1 Running 0 60s thanos-rule-0 1/1 Running 2 25h Deployment metrics有6个副本，每个生成10045 series，kvass-coordinator配置每个分区最大series为30000，以及Prometheus默认的指标，需要3个Prometheus分片。 每个分片包含2个target prometheus_tsdb_head_chunks{instance=\"127.0.0.1:9090\",job=\"prometheus_shards\",replicate=\"prometheus-rep-0-0\",shard=\"0\"} 20557 通过Thanos Query可以查询到多个Prometheus分片的数据，以及聚合规则metrics_count ","date":"Mar 30, 2021","objectID":"/prometheus-auto-federation/:3:1","series":null,"tags":["prometheus"],"title":"Prometheus高可用自动分区方案","uri":"/prometheus-auto-federation/#测试验证"},{"categories":["cloud"],"content":" 待优化问题此方案可满足绝大部分场景，用户可通过自己的实际环境配合不同的组件，但也存在一些需要优化确认的问题 Thanos Ruler不支持远程写接口，只能存储于Thanos提供的对象存储中 Thanos Query全局查询依赖多个下游组件，可能只返回部分结果挺好使 Coordinator性能需要压测验证 ","date":"Mar 30, 2021","objectID":"/prometheus-auto-federation/:3:2","series":null,"tags":["prometheus"],"title":"Prometheus高可用自动分区方案","uri":"/prometheus-auto-federation/#待优化问题"},{"categories":["cloud"],"content":" 总结Kvass+Thanos+Remote-write可以实现Prometheus集群的自动分区、全局查询、数据持久化等功能，满足绝大部分场景。虽然有一些问题需要验证优化，但瑕不掩瑜，能够解决原生Prometheus扩展性问题。 ","date":"Mar 30, 2021","objectID":"/prometheus-auto-federation/:4:0","series":null,"tags":["prometheus"],"title":"Prometheus高可用自动分区方案","uri":"/prometheus-auto-federation/#总结"},{"categories":["cloud"],"content":" 引用 https://qingwave.github.io/prometheus-federation/ https://github.com/tkestack/kvass https://github.com/thanos-io/thanos ","date":"Mar 30, 2021","objectID":"/prometheus-auto-federation/:5:0","series":null,"tags":["prometheus"],"title":"Prometheus高可用自动分区方案","uri":"/prometheus-auto-federation/#引用"},{"categories":["code"],"content":"某些情况下我们希望程序通过自定义Nameserver去查询域名，而不希望通过操作系统给定的Nameserver，本文介绍如何在Golang中实现自定义Nameserver。 ","date":"Mar 29, 2021","objectID":"/golang-special-dns-nameserver/:0:0","series":null,"tags":["dns","golang"],"title":"Golang自定义DNS Nameserver","uri":"/golang-special-dns-nameserver/#"},{"categories":["code"],"content":" DNS解析过程Golang中一般通过net.Resolver的LookupHost(ctx context.Context, host string) (addrs []string, err error)去实现域名解析，解析过程如下： 检查本地hosts文件是否存在解析记录，存在即返回解析地址 不存在即根据resolv.conf中读取的nameserver发起递归查询 nameserver不断的向上级nameserver发起迭代查询 nameserver最终返回查询结果给请求者 用户可以通过修改/etc/resolv.conf来添加特定的nameserver，但某些场景下我们不希望更改系统配置。比如在kubernetes中，作为sidecar服务需要通过service去访问其他集群内服务，必须更改dnsPolicy为ClusterFirst，但这可能会影响其他容器的DNS查询效率。 ","date":"Mar 29, 2021","objectID":"/golang-special-dns-nameserver/:1:0","series":null,"tags":["dns","golang"],"title":"Golang自定义DNS Nameserver","uri":"/golang-special-dns-nameserver/#dns解析过程"},{"categories":["code"],"content":" 自定义Nameserver在Golang中自定义Nameserver，需要我们自己实现一个Resolver，如果是httpClient需要自定义DialContext() Resolver实现如下： // 默认dialer dialer := \u0026net.Dialer{ Timeout: 1 * time.Second, } // 定义resolver resolver := \u0026net.Resolver{ Dial: func(ctx context.Context, network, address string) (net.Conn, error) { return dialer.DialContext(ctx, \"tcp\", nameserver) // 通过tcp请求nameserver解析域名 }, } 自定义Dialer如下： type Dialer struct { dialer *net.Dialer resolver *net.Resolver nameserver string } // NewDialer create a Dialer with user's nameserver. func NewDialer(dialer *net.Dialer, nameserver string) (*Dialer, error) { conn, err := dialer.Dial(\"tcp\", nameserver) if err != nil { return nil, err } defer conn.Close() return \u0026Dialer{ dialer: dialer, resolver: \u0026net.Resolver{ Dial: func(ctx context.Context, network, address string) (net.Conn, error) { return dialer.DialContext(ctx, \"tcp\", nameserver) }, }, nameserver: nameserver, // 用户设置的nameserver }, nil } // DialContext connects to the address on the named network using // the provided context. func (d *Dialer) DialContext(ctx context.Context, network, address string) (net.Conn, error) { host, port, err := net.SplitHostPort(address) if err != nil { return nil, err } ips, err := d.resolver.LookupHost(ctx, host) // 通过自定义nameserver查询域名 for _, ip := range ips { // 创建链接 conn, err := d.dialer.DialContext(ctx, network, ip+\":\"+port) if err == nil { return conn, nil } } return d.dialer.DialContext(ctx, network, address) } httpClient中自定义DialContext()如下： ndialer, _ := NewDialer(dialer, nameserver) client := \u0026http.Client{ Transport: \u0026http.Transport{ DialContext: ndialer.DialContext, TLSHandshakeTimeout: 10 * time.Second, }, Timeout: timeout, } ","date":"Mar 29, 2021","objectID":"/golang-special-dns-nameserver/:2:0","series":null,"tags":["dns","golang"],"title":"Golang自定义DNS Nameserver","uri":"/golang-special-dns-nameserver/#自定义nameserver"},{"categories":["code"],"content":" 总结通过以上实现可解决自定义Nameserver，也可以在Dailer中添加缓存，实现DNS缓存。 ","date":"Mar 29, 2021","objectID":"/golang-special-dns-nameserver/:3:0","series":null,"tags":["dns","golang"],"title":"Golang自定义DNS Nameserver","uri":"/golang-special-dns-nameserver/#总结"},{"categories":["杂记"],"content":" 但少闲人如吾两人尔李一冰的《苏东坡新传》在微信读书里躺了一年有余，偶尔想起翻开几页，不过大多时候是想不起的。年纪越大越发的浮躁，少了上学时的心平气和，工作之后阅读的功利心重了许多，总想通过几段文字掌握多少知识，开阔些许眼界，对于这些“不务正业”的书难有闲心了。 ","date":"Mar 25, 2021","objectID":"/sudongpo/:1:0","series":null,"tags":["随笔"],"title":"也谈苏东坡","uri":"/sudongpo/#但少闲人如吾两人尔"},{"categories":["杂记"],"content":" 春江水暖鸭先知对于苏东坡，上学前就会背他的《赤壁怀古》，记得小学时老师问谁会背词，便大东江东去了一番，引得不少目光。但对东坡其人知之甚少，后来开始慢慢接触他的诗与杂文。高中时有摘抄作业，读了林语堂的《苏东坡传》，将其中的句子摘写下来，很是欣赏，有时也会引入作文中。从《水调歌头》到《前后赤壁赋》，最喜欢的还是《定风波》，欣赏“谁怕，一蓑烟雨任平生”的豪情，羡慕“归去，也无风雨也无晴”的洒脱，虽然据说他淋雨后便生了一场病，但激励我渡过一些苦闷的日子。 ","date":"Mar 25, 2021","objectID":"/sudongpo/:2:0","series":null,"tags":["随笔"],"title":"也谈苏东坡","uri":"/sudongpo/#春江水暖鸭先知"},{"categories":["杂记"],"content":" 人生到处知何似人生的际遇谁又能意料的到，苏轼走出眉山，从凤翔到京城，本想有所作为，可与王安石相左。后面乌台诗案爆发，便是“黄州惠州儋州”。每每低落时他总能找到慰藉的方法，“长江绕郭知鱼美”，“日啖荔枝三百颗”，也曾想过“小舟从此逝”，但总有“何妨吟啸且徐行”的潇洒。失之东隅收之桑榆，黄州的日子也就成苏轼，有了雪堂，有了苏东坡。 自初中起便开始在外求学，老爸对我说你要走出家乡出人头地了，当时还不明白什么意思。现在过去好多年了，高考大学工作都还算顺利，按照王小波同志的说法，一切都在不可避免的走向庸俗。你我也有着自己的际遇，不似东坡那般跌宕，但也想在平淡的日子里开出一些“清风明月”的小花，迷茫时能历经自己的“料峭春风”，踏出不一样的“飞鸿雪泥”。 ","date":"Mar 25, 2021","objectID":"/sudongpo/:3:0","series":null,"tags":["随笔"],"title":"也谈苏东坡","uri":"/sudongpo/#人生到处知何似"},{"categories":["code"],"content":" 背景在容器平台上我们提供了zk做白名单功能，Pod启动时 sidecar会自动注册zk。昨天遇到zk server抖动，sidecar容器输出大量StateDisconnected事件，zk正常后仍无法恢复，由于大量日志造成sidecar容器 cpu占用过高，进而引发dockerdcpu占用过高，严重时影响dockerd正常调用。 ","date":"Mar 02, 2021","objectID":"/golang-zk-statedisconnected/:1:0","series":null,"tags":["golang","zk"],"title":"golang zk大量disconnected event","uri":"/golang-zk-statedisconnected/#背景"},{"categories":["code"],"content":" 问题分析","date":"Mar 02, 2021","objectID":"/golang-zk-statedisconnected/:2:0","series":null,"tags":["golang","zk"],"title":"golang zk大量disconnected event","uri":"/golang-zk-statedisconnected/#问题分析"},{"categories":["code"],"content":" 问题复现正常情况下，sidecar启动后会去注册zk： # docker logs -f 01a1a4a74785 I0302 15:04:05.476463 1 manager.go:116] start run plugin zk 2021/03/02 15:04:05 Connected to 10.38.161.60:11000 I0302 15:04:05.488006 1 zk.go:152] zookeeper connect succeed: zk.srv:11000 2021/03/02 15:04:05 authenticated: id=33746806328105493, timeout=30000 2021/03/02 15:04:05 re-submitting `0` credentials after reconnect I0302 15:04:05.516446 1 zk.go:220] watching zk node:[/tasks/cluster.xxx_default_deployment.htool/10.46.12.72] in cluster[xxx] #注册成功，开始watch 通过iptables来模拟异常，首先进入到容器network namesapce pod=htool-6875bcb898-w7llc containerid=$(docker ps |grep $pod|awk '{print $1}'|head -n 1) pid=$(docker inspect -f {{.State.Pid}} $containerid) nsenter -n --target $pid 使用iptables drop掉发往zk的请求(11000为zk server端口) iptables -A OUTPUT -p tcp -m tcp --dport 11000 -j DROP zk client自动重试（1s一次），日志显示Failed to connect to 10.38.161.54:11000: dial tcp 10.38.161.54:11000: i/o timeout I0302 15:04:05.516446 1 zk.go:220] watching zk node:[/tasks/cluster.xxx_default_deployment.htool/10.46.12.72] in cluster[xxx] 2021/03/02 15:08:55 recv loop terminated: err=failed to read from connection: read tcp 10.46.12.72:36884-\u003e10.38.161.60:11000: i/o timeout 2021/03/02 15:08:55 send loop terminated: err=\u003cnil\u003e 2021/03/02 15:08:56 Failed to connect to 10.38.161.54:11000: dial tcp 10.38.161.54:11000: i/o timeout 网络恢复，删除iptables iptables -D OUTPUT -p tcp -m tcp --dport 11000 -j DROP 出现大量StateDisconnected日志 I0302 15:09:50.951897 1 zk.go:232] Unknown zk event[StateDisconnected] for znode:[/tasks/cluster.xxx_default_deployment.htool/10.46.12.72] I0302 15:09:50.951893 1 zk.go:232] Unknown zk event[StateDisconnected] for znode:[/tasks/cluster.xxx_default_deployment.htool/10.46.12.72] ... ","date":"Mar 02, 2021","objectID":"/golang-zk-statedisconnected/:2:1","series":null,"tags":["golang","zk"],"title":"golang zk大量disconnected event","uri":"/golang-zk-statedisconnected/#问题复现"},{"categories":["code"],"content":" 问题分析sidecar中zk watch代码如下： exist, _, eventCh, err := conn.ExistsW(node) //监听zk事件 watcher: for { select { case e := \u003c-eventCh: switch e.State { case zk.StateExpired: return fmt.Errorf(\"node[%v] expired\", node) case zk.StateConnected, zk.StateHasSession: return fmt.Errorf(\"Get zk event: %v \", e.State) default: klog.Infof(\"Get zk event[%v] for znode:[%v]\", e.State, node) // 出错位置 } case \u003c-ctx.Done(): // we close the conn in caller break watcher } } ExistsW函数由github.com/samuel/go-zookeeper/zk库提供，监听zk给定目录的事件 func (c *Conn) ExistsW(path string) (bool, *Stat, \u003c-chan Event, error) { var ech \u003c-chan Event ... ech = c.addWatcher(path, watchTypeData) return exists, \u0026res.Stat, ech, err } 当zk异常恢复后，c.addWatcher中的channel被close，即sidecar中eventCh关闭，进入死循环。 ","date":"Mar 02, 2021","objectID":"/golang-zk-statedisconnected/:2:2","series":null,"tags":["golang","zk"],"title":"golang zk大量disconnected event","uri":"/golang-zk-statedisconnected/#问题分析-1"},{"categories":["code"],"content":" 修复验证知道了原因，修复很简单，判断下eventCh状态即可 for { select { case e, ok := \u003c-eventCh: if !ok { return fmt.Errorf(\"event channel closed\") } if e.Err != nil { return fmt.Errorf(\"Get zk event: %v, err: %v\", e.State, e.Err) } switch e.State { case zk.StateExpired: return fmt.Errorf(\"node[%v] expired\", node) case zk.StateConnected, zk.StateHasSession: return fmt.Errorf(\"Get zk event: %v \", e.State) default: klog.Infof(\"Get zk event[%v] for znode:[%v]\", e.State, node) } } 在修复代码后，再次验证可正常注册 2021/03/02 15:13:40 Failed to connect to 10.38.161.60:11000: dial tcp 10.38.161.60:11000: i/o timeout 2021/03/02 15:13:40 Connected to 10.38.161.55:11000 2021/03/02 15:13:40 authentication failed: zk: session has been expired by the server W0302 15:13:40.222923 1 zk.go:300] meet error when watching node path: Get zk event: StateDisconnected, err: zk: session has been expired by the server 2021/03/02 15:13:40 Connected to 10.38.161.54:11000 2021/03/02 15:13:40 authenticated: id=177861994644216038, timeout=30000 2021/03/02 15:13:40 re-submitting `1` credentials after reconnect I0302 15:13:41.238524 1 zk.go:220] watching zk node:[/tasks/cluster.xxx_default_deployment.htool/10.46.12.72] in cluster[xxx] ","date":"Mar 02, 2021","objectID":"/golang-zk-statedisconnected/:2:3","series":null,"tags":["golang","zk"],"title":"golang zk大量disconnected event","uri":"/golang-zk-statedisconnected/#修复验证"},{"categories":["code"],"content":" 总结这个问题其实与zk没关系，是由于没有判断channel状态，陷入死循环。通常情况下大部分应用只有退出时才会关闭channel，不需要特殊处理。 ","date":"Mar 02, 2021","objectID":"/golang-zk-statedisconnected/:3:0","series":null,"tags":["golang","zk"],"title":"golang zk大量disconnected event","uri":"/golang-zk-statedisconnected/#总结"},{"categories":["cloud"],"content":" 背景在k8s或docker中，有时候我们需要通过shell来启动程序，但是默认shell不会传递信号（sigterm）给子进程，当在pod终止时应用无法优雅退出，直到最大时间时间后强制退出（kill -9）。 ","date":"Feb 03, 2021","objectID":"/docker-shell-signal/:1:0","series":null,"tags":["k8s","docker"],"title":"k8s中shell脚本启动如何传递信号","uri":"/docker-shell-signal/#背景"},{"categories":["cloud"],"content":" 分析普通情况下，大多业务的启动命令如下 command: [\"binary\", \"-flags\", ...] 主进程做为1号进程会收到sigterm信号，优雅退出(需要程序捕获信号); 而通过脚本启动时，shell作为1号进程，不会显示传递信号给子进程，造成子进程无法优雅退出，直到最大退出时间后强制终止。 ","date":"Feb 03, 2021","objectID":"/docker-shell-signal/:2:0","series":null,"tags":["k8s","docker"],"title":"k8s中shell脚本启动如何传递信号","uri":"/docker-shell-signal/#分析"},{"categories":["cloud"],"content":" 解决方案","date":"Feb 03, 2021","objectID":"/docker-shell-signal/:3:0","series":null,"tags":["k8s","docker"],"title":"k8s中shell脚本启动如何传递信号","uri":"/docker-shell-signal/#解决方案"},{"categories":["cloud"],"content":" exec如何只需一个进程收到信号，可通过exec，exec会替换当前shell进程，即pid不变 #! /bin/bash # do something exec binay -flags ... 正常情况测试命令如下，使用sleep来模拟应用sh -c 'echo \"start\"; sleep 100'： pstree展示如下，sleep进程会生成一个子进程 bash(28701)───sh(24588)───sleep(24589) 通过exec运行后，命令sh -c 'echo \"start\"; exec sleep 100' bash(28701)───sleep(24664) 加入exec后，sleep进程替换了shell进程，没有生成子进程 此种方式可以收到信号，但只适用于一个子进程的情况 ","date":"Feb 03, 2021","objectID":"/docker-shell-signal/:3:1","series":null,"tags":["k8s","docker"],"title":"k8s中shell脚本启动如何传递信号","uri":"/docker-shell-signal/#exec"},{"categories":["cloud"],"content":" trap在shell中可以显示通过trap捕捉信号传递给子进程 #!/bin/bash echo \"start\" binary -flags... \u0026 pid=\"$!\" _kill() { echo \"receive sigterm\" kill $pid #传递给子进程 wait $pid exit 0 } trap _kill SIGTERM #捕获信号 wait #等待子进程退出 此种方式需要改动启动脚本，显示传递信号给子进程 ","date":"Feb 03, 2021","objectID":"/docker-shell-signal/:3:2","series":null,"tags":["k8s","docker"],"title":"k8s中shell脚本启动如何传递信号","uri":"/docker-shell-signal/#trap"},{"categories":["cloud"],"content":" docker-initdocker-init即在docker启动时加入--init参数，docker-int会作为一号进程，会向子进程传递信号并且会回收僵尸进程。 遗憾的是k8s并不支持--init参数，用户可在镜像中声明init进程，更多可参考container-init RUN wget -O /usr/bin/dumb-init https://github.com/Yelp/dumb-init/releases/download/v1.2.2/dumb-init_1.2.2_amd64 RUN chmod +x /usr/bin/dumb-init ENTRYPOINT [\"/usr/bin/dumb-init\", \"-v\", \"--\"] CMD [\"nginx\", \"-g\", \"daemon off;\"] ","date":"Feb 03, 2021","objectID":"/docker-shell-signal/:4:0","series":null,"tags":["k8s","docker"],"title":"k8s中shell脚本启动如何传递信号","uri":"/docker-shell-signal/#docker-init"},{"categories":["cloud"],"content":"kubernetes集群内置的dns插件kubedns/coredns在高并发情况下可能遇到性能瓶颈，以下从配置与本地缓存方面说明如何减少dns查询失败率，提高性能。 ","date":"Feb 01, 2021","objectID":"/k8s-dns-optimize/:0:0","series":null,"tags":["k8s","dns"],"title":"优化Kubernetes集群内DNS","uri":"/k8s-dns-optimize/#"},{"categories":["cloud"],"content":" 配置优化","date":"Feb 01, 2021","objectID":"/k8s-dns-optimize/:1:0","series":null,"tags":["k8s","dns"],"title":"优化Kubernetes集群内DNS","uri":"/k8s-dns-optimize/#配置优化"},{"categories":["cloud"],"content":" dnsPolicyk8s 默认的 dnsPolicy 是ClusterFirst，因为 ndots 和 serach domain 在访问外部 dns 会有额外的查询次数。 / # cat /etc/resolv.conf nameserver 10.254.0.2 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5 / # / # / # host -v mi.com Trying \"mi.com.default.svc.cluster.local\" Trying \"mi.com.svc.cluster.local\" Trying \"mi.com.cluster.local\" Trying \"mi.com\" ;; -\u003e\u003eHEADER\u003c\u003c- opcode: QUERY, status: NOERROR, id: 38967 ;; flags: qr rd ra; QUERY: 1, ANSWER: 1, AUTHORITY: 0, ADDITIONAL: 0 ;; QUESTION SECTION: ;mi.com. IN A ;; ANSWER SECTION: mi.com. 30 IN A 58.83.160.156 如果不访问service，调整dnsPolicy为Default，直接走宿主机的dns ","date":"Feb 01, 2021","objectID":"/k8s-dns-optimize/:1:1","series":null,"tags":["k8s","dns"],"title":"优化Kubernetes集群内DNS","uri":"/k8s-dns-optimize/#dnspolicy"},{"categories":["cloud"],"content":" ndots如需访问service，尽量减少ndots（默认5）即域名中点的个数小于ndots会按照search域（mi.com.default.svc.cluster.local）依次查询，若查询不到再查询原始域名，总共进行8次dns查询（4次ipv4, 4次ipv6） 设置ndots为1后，只有两次查询（1次ipv4, ipv6） / # host -v mi.com Trying \"mi.com\" ;; -\u003e\u003eHEADER\u003c\u003c- opcode: QUERY, status: NOERROR, id: 23894 ;; flags: qr rd ra; QUERY: 1, ANSWER: 1, AUTHORITY: 0, ADDITIONAL: 0 ;; QUESTION SECTION: ;mi.com. IN A ;; ANSWER SECTION: mi.com. 30 IN A 58.83.160.156 但此种方式service域名分割大于等于ndots，则解析不到，需要业务自行判断合适的ndots值 / # host -v prometheus.kube-system Trying \"prometheus.kube-system\" Host prometheus.kube-system not found: 3(NXDOMAIN) Received 115 bytes from 10.254.0.2#53 in 8 ms Received 115 bytes from 10.254.0.2#53 in 8 ms ","date":"Feb 01, 2021","objectID":"/k8s-dns-optimize/:1:2","series":null,"tags":["k8s","dns"],"title":"优化Kubernetes集群内DNS","uri":"/k8s-dns-optimize/#ndots"},{"categories":["cloud"],"content":" coredns优化调整合理的副本数，阿里建议coredns:node=1:8，启动AutoPath插件减少查询次数，见DNS性能优化 ","date":"Feb 01, 2021","objectID":"/k8s-dns-optimize/:1:3","series":null,"tags":["k8s","dns"],"title":"优化Kubernetes集群内DNS","uri":"/k8s-dns-optimize/#coredns优化"},{"categories":["cloud"],"content":" DNS缓存","date":"Feb 01, 2021","objectID":"/k8s-dns-optimize/:2:0","series":null,"tags":["k8s","dns"],"title":"优化Kubernetes集群内DNS","uri":"/k8s-dns-optimize/#dns缓存"},{"categories":["cloud"],"content":" NodeLocalDNSNodeLocal DNSCache 通过在集群节点上作为 DaemonSet 运行 dns 缓存代理来提高集群 DNS 性能， 借助这种新架构，Pods 将可以访问在同一节点上运行的 dns 缓存代理，从而避免了 iptables DNAT 规则和连接跟踪。 架构如下: NodeLocalDNS的设计提案见（nodelocal-dns-cache） 验证官方安装方式见nodelocaldns，需要自行替换变量 可通过如下脚本，一键安装（注意设置kubedns svc ClusterIP） #!/bin/bash wget https://raw.githubusercontent.com/kubernetes/kubernetes/master/cluster/addons/dns/nodelocaldns/nodelocaldns.yaml # registery docker_registery=k8s.gcr.io/dns/k8s-dns-node-cache # kube-dns svc clusterip kubedns_svc=10.254.0.2 # nodelocaldns ip nodelocaldns_ip=169.254.20.10 # kube-proxy mode, iptables or ipvs kubeproxy_mode=iptables result=result.yaml if [ ${kubeproxy_mode} == \"ipvs\" ]; then sed -e \"s|k8s.gcr.io/dns/k8s-dns-node-cache|$docker_registery|g\" \\ -e \"s/__PILLAR__CLUSTER__DNS__/$kubedns_svc/g\" \\ -e \"s/__PILLAR__LOCAL__DNS__/$nodelocaldns_ip/g\" \\ -e 's/[ |,]__PILLAR__DNS__SERVER__//g' \\ -e \"s/__PILLAR__DNS__DOMAIN__/cluster.local/g\" nodelocaldns.yaml \u003e$result else sed -e \"s|k8s.gcr.io/dns/k8s-dns-node-cache|$docker_registery|g\" \\ -e \"s/__PILLAR__DNS__SERVER__/$kubedns_svc/g\" \\ -e \"s/__PILLAR__LOCAL__DNS__/$nodelocaldns_ip/g\" \\ -e \"s/__PILLAR__DNS__DOMAIN__/cluster.local/g\" nodelocaldns.yaml \u003e$result fi kubectl apply -f $result 创建完成后，每个节点运行一个pod，查看pod(个别节点ingress-nginx占用8080端口，导致nodelocaldns启动失败) # kubectl get po -n kube-system -l k8s-app=node-local-dns -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES node-local-dns-2fvxb 0/1 CrashLoopBackOff 4 103s 10.38.200.195 node04 \u003cnone\u003e \u003cnone\u003e node-local-dns-4zmcd 1/1 Running 0 54d 10.38.201.55 node06 \u003cnone\u003e \u003cnone\u003e node-local-dns-55tzg 1/1 Running 0 60d 10.38.200.186 node02 \u003cnone\u003e \u003cnone\u003e node-local-dns-cctg7 1/1 Running 0 54d 10.38.200.242 node07 \u003cnone\u003e \u003cnone\u003e node-local-dns-khgmm 1/1 Running 0 54d 10.38.201.36 node08 \u003cnone\u003e \u003cnone\u003e node-local-dns-mbr64 1/1 Running 0 60d 10.38.200.187 node05 \u003cnone\u003e \u003cnone\u003e node-local-dns-t67vw 1/1 Running 0 60d 10.38.200.188 node03 \u003cnone\u003e \u003cnone\u003e node-local-dns-tmm92 1/1 Running 14 54d 10.38.200.57 node09 \u003cnone\u003e \u003cnone\u003e 默认配置如下： cluster.local:53 { errors cache { success 9984 30 # 默认成功缓存30s denial 9984 5 #失败缓存5s } reload loop bind 169.254.20.10 10.254.0.2 #本地监听ip forward . 10.254.132.95 { #转发到kubedns-upstream force_tcp } prometheus :9253 #监控接口 health 169.254.20.10:8080 #健康检测端口 } in-addr.arpa:53 { errors cache 30 reload loop bind 169.254.20.10 10.254.0.2 forward . 10.254.132.95 { force_tcp } prometheus :9253 } ip6.arpa:53 { errors cache 30 reload loop bind 169.254.20.10 10.254.0.2 forward . 10.254.132.95 { force_tcp } prometheus :9253 } .:53 { errors cache 30 reload loop bind 169.254.20.10 10.254.0.2 forward . /etc/resolv.conf prometheus :9253 } 节点上查看localdns的网卡，本地将监听169.254.20.10与10.254.0.2两个地址，拦截kubedns((默认10.254.0.2)的请求，命中后直接返回，若未命中转发到kubedns(对应service kube-dns-upstream，kube-dns-upstream由localdns创建绑定kubedns pod) # ip addr show nodelocaldns 182232: nodelocaldns: \u003cBROADCAST,NOARP\u003e mtu 1500 qdisc noop state DOWN link/ether 4e:62:1c:fd:56:12 brd ff:ff:ff:ff:ff:ff inet 169.254.20.10/32 brd 169.254.20.10 scope global nodelocaldns valid_lft forever preferred_lft forever inet 10.254.0.2/32 brd 10.254.0.2 scope global nodelocaldns valid_lft forever preferred_lft forever iptables规则，使用NOTRACK跳过其它表处理 iptables-save | egrep \"10.254.0.2|169.254.20.10\" -A PREROUTING -d 10.254.0.2/32 -p udp -m udp --dport 53 -j NOTRACK -A PREROUTING -d 10.254.0.2/32 -p tcp -m tcp --dport 53 -j NOTRACK -A PREROUTING -d 169.254.20.10/32 -p udp -m udp --dport 53 -j NOTRACK -A PREROUTING -d 169.254.20.10/32 -p tcp -m tcp --dport 53 -j NOTRACK -A OUTPUT -d 10.254.0.2/32 -p udp -m udp --dport 53 -j NOTRACK -A OUTPUT -d 10.254.0.2/32 -p tcp -m tcp --dport 53 -j NOTRACK -A INPUT -d 10.254.0.2/32 -p udp -m udp --dport 53 -j ACCEPT -A INPUT -d 10.254.0.2/32 -p tcp -m tcp --dport 53 -j ACCEPT -A OUTPUT -s 10.254.0.2/32 -p udp -m udp --sport 53 -j ACCEPT -A OUTPUT -s 10.254.0.2/32 -p tcp -m tcp --sport 53 -j ACCEPT ... -A KUBE-SERVICES -d 10.254.0.2/32 -p tcp -m comment --comment \"kube-system/kub","date":"Feb 01, 2021","objectID":"/k8s-dns-optimize/:2:1","series":null,"tags":["k8s","dns"],"title":"优化Kubernetes集群内DNS","uri":"/k8s-dns-optimize/#nodelocaldns"},{"categories":["cloud"],"content":" NodeLocalDNSNodeLocal DNSCache 通过在集群节点上作为 DaemonSet 运行 dns 缓存代理来提高集群 DNS 性能， 借助这种新架构，Pods 将可以访问在同一节点上运行的 dns 缓存代理，从而避免了 iptables DNAT 规则和连接跟踪。 架构如下: NodeLocalDNS的设计提案见（nodelocal-dns-cache） 验证官方安装方式见nodelocaldns，需要自行替换变量 可通过如下脚本，一键安装（注意设置kubedns svc ClusterIP） #!/bin/bash wget https://raw.githubusercontent.com/kubernetes/kubernetes/master/cluster/addons/dns/nodelocaldns/nodelocaldns.yaml # registery docker_registery=k8s.gcr.io/dns/k8s-dns-node-cache # kube-dns svc clusterip kubedns_svc=10.254.0.2 # nodelocaldns ip nodelocaldns_ip=169.254.20.10 # kube-proxy mode, iptables or ipvs kubeproxy_mode=iptables result=result.yaml if [ ${kubeproxy_mode} == \"ipvs\" ]; then sed -e \"s|k8s.gcr.io/dns/k8s-dns-node-cache|$docker_registery|g\" \\ -e \"s/__PILLAR__CLUSTER__DNS__/$kubedns_svc/g\" \\ -e \"s/__PILLAR__LOCAL__DNS__/$nodelocaldns_ip/g\" \\ -e 's/[ |,]__PILLAR__DNS__SERVER__//g' \\ -e \"s/__PILLAR__DNS__DOMAIN__/cluster.local/g\" nodelocaldns.yaml \u003e$result else sed -e \"s|k8s.gcr.io/dns/k8s-dns-node-cache|$docker_registery|g\" \\ -e \"s/__PILLAR__DNS__SERVER__/$kubedns_svc/g\" \\ -e \"s/__PILLAR__LOCAL__DNS__/$nodelocaldns_ip/g\" \\ -e \"s/__PILLAR__DNS__DOMAIN__/cluster.local/g\" nodelocaldns.yaml \u003e$result fi kubectl apply -f $result 创建完成后，每个节点运行一个pod，查看pod(个别节点ingress-nginx占用8080端口，导致nodelocaldns启动失败) # kubectl get po -n kube-system -l k8s-app=node-local-dns -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES node-local-dns-2fvxb 0/1 CrashLoopBackOff 4 103s 10.38.200.195 node04 node-local-dns-4zmcd 1/1 Running 0 54d 10.38.201.55 node06 node-local-dns-55tzg 1/1 Running 0 60d 10.38.200.186 node02 node-local-dns-cctg7 1/1 Running 0 54d 10.38.200.242 node07 node-local-dns-khgmm 1/1 Running 0 54d 10.38.201.36 node08 node-local-dns-mbr64 1/1 Running 0 60d 10.38.200.187 node05 node-local-dns-t67vw 1/1 Running 0 60d 10.38.200.188 node03 node-local-dns-tmm92 1/1 Running 14 54d 10.38.200.57 node09 默认配置如下： cluster.local:53 { errors cache { success 9984 30 # 默认成功缓存30s denial 9984 5 #失败缓存5s } reload loop bind 169.254.20.10 10.254.0.2 #本地监听ip forward . 10.254.132.95 { #转发到kubedns-upstream force_tcp } prometheus :9253 #监控接口 health 169.254.20.10:8080 #健康检测端口 } in-addr.arpa:53 { errors cache 30 reload loop bind 169.254.20.10 10.254.0.2 forward . 10.254.132.95 { force_tcp } prometheus :9253 } ip6.arpa:53 { errors cache 30 reload loop bind 169.254.20.10 10.254.0.2 forward . 10.254.132.95 { force_tcp } prometheus :9253 } .:53 { errors cache 30 reload loop bind 169.254.20.10 10.254.0.2 forward . /etc/resolv.conf prometheus :9253 } 节点上查看localdns的网卡，本地将监听169.254.20.10与10.254.0.2两个地址，拦截kubedns((默认10.254.0.2)的请求，命中后直接返回，若未命中转发到kubedns(对应service kube-dns-upstream，kube-dns-upstream由localdns创建绑定kubedns pod) # ip addr show nodelocaldns 182232: nodelocaldns: mtu 1500 qdisc noop state DOWN link/ether 4e:62:1c:fd:56:12 brd ff:ff:ff:ff:ff:ff inet 169.254.20.10/32 brd 169.254.20.10 scope global nodelocaldns valid_lft forever preferred_lft forever inet 10.254.0.2/32 brd 10.254.0.2 scope global nodelocaldns valid_lft forever preferred_lft forever iptables规则，使用NOTRACK跳过其它表处理 iptables-save | egrep \"10.254.0.2|169.254.20.10\" -A PREROUTING -d 10.254.0.2/32 -p udp -m udp --dport 53 -j NOTRACK -A PREROUTING -d 10.254.0.2/32 -p tcp -m tcp --dport 53 -j NOTRACK -A PREROUTING -d 169.254.20.10/32 -p udp -m udp --dport 53 -j NOTRACK -A PREROUTING -d 169.254.20.10/32 -p tcp -m tcp --dport 53 -j NOTRACK -A OUTPUT -d 10.254.0.2/32 -p udp -m udp --dport 53 -j NOTRACK -A OUTPUT -d 10.254.0.2/32 -p tcp -m tcp --dport 53 -j NOTRACK -A INPUT -d 10.254.0.2/32 -p udp -m udp --dport 53 -j ACCEPT -A INPUT -d 10.254.0.2/32 -p tcp -m tcp --dport 53 -j ACCEPT -A OUTPUT -s 10.254.0.2/32 -p udp -m udp --sport 53 -j ACCEPT -A OUTPUT -s 10.254.0.2/32 -p tcp -m tcp --sport 53 -j ACCEPT ... -A KUBE-SERVICES -d 10.254.0.2/32 -p tcp -m comment --comment \"kube-system/kub","date":"Feb 01, 2021","objectID":"/k8s-dns-optimize/:2:1","series":null,"tags":["k8s","dns"],"title":"优化Kubernetes集群内DNS","uri":"/k8s-dns-optimize/#验证"},{"categories":["cloud"],"content":" NodeLocalDNSNodeLocal DNSCache 通过在集群节点上作为 DaemonSet 运行 dns 缓存代理来提高集群 DNS 性能， 借助这种新架构，Pods 将可以访问在同一节点上运行的 dns 缓存代理，从而避免了 iptables DNAT 规则和连接跟踪。 架构如下: NodeLocalDNS的设计提案见（nodelocal-dns-cache） 验证官方安装方式见nodelocaldns，需要自行替换变量 可通过如下脚本，一键安装（注意设置kubedns svc ClusterIP） #!/bin/bash wget https://raw.githubusercontent.com/kubernetes/kubernetes/master/cluster/addons/dns/nodelocaldns/nodelocaldns.yaml # registery docker_registery=k8s.gcr.io/dns/k8s-dns-node-cache # kube-dns svc clusterip kubedns_svc=10.254.0.2 # nodelocaldns ip nodelocaldns_ip=169.254.20.10 # kube-proxy mode, iptables or ipvs kubeproxy_mode=iptables result=result.yaml if [ ${kubeproxy_mode} == \"ipvs\" ]; then sed -e \"s|k8s.gcr.io/dns/k8s-dns-node-cache|$docker_registery|g\" \\ -e \"s/__PILLAR__CLUSTER__DNS__/$kubedns_svc/g\" \\ -e \"s/__PILLAR__LOCAL__DNS__/$nodelocaldns_ip/g\" \\ -e 's/[ |,]__PILLAR__DNS__SERVER__//g' \\ -e \"s/__PILLAR__DNS__DOMAIN__/cluster.local/g\" nodelocaldns.yaml \u003e$result else sed -e \"s|k8s.gcr.io/dns/k8s-dns-node-cache|$docker_registery|g\" \\ -e \"s/__PILLAR__DNS__SERVER__/$kubedns_svc/g\" \\ -e \"s/__PILLAR__LOCAL__DNS__/$nodelocaldns_ip/g\" \\ -e \"s/__PILLAR__DNS__DOMAIN__/cluster.local/g\" nodelocaldns.yaml \u003e$result fi kubectl apply -f $result 创建完成后，每个节点运行一个pod，查看pod(个别节点ingress-nginx占用8080端口，导致nodelocaldns启动失败) # kubectl get po -n kube-system -l k8s-app=node-local-dns -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES node-local-dns-2fvxb 0/1 CrashLoopBackOff 4 103s 10.38.200.195 node04 node-local-dns-4zmcd 1/1 Running 0 54d 10.38.201.55 node06 node-local-dns-55tzg 1/1 Running 0 60d 10.38.200.186 node02 node-local-dns-cctg7 1/1 Running 0 54d 10.38.200.242 node07 node-local-dns-khgmm 1/1 Running 0 54d 10.38.201.36 node08 node-local-dns-mbr64 1/1 Running 0 60d 10.38.200.187 node05 node-local-dns-t67vw 1/1 Running 0 60d 10.38.200.188 node03 node-local-dns-tmm92 1/1 Running 14 54d 10.38.200.57 node09 默认配置如下： cluster.local:53 { errors cache { success 9984 30 # 默认成功缓存30s denial 9984 5 #失败缓存5s } reload loop bind 169.254.20.10 10.254.0.2 #本地监听ip forward . 10.254.132.95 { #转发到kubedns-upstream force_tcp } prometheus :9253 #监控接口 health 169.254.20.10:8080 #健康检测端口 } in-addr.arpa:53 { errors cache 30 reload loop bind 169.254.20.10 10.254.0.2 forward . 10.254.132.95 { force_tcp } prometheus :9253 } ip6.arpa:53 { errors cache 30 reload loop bind 169.254.20.10 10.254.0.2 forward . 10.254.132.95 { force_tcp } prometheus :9253 } .:53 { errors cache 30 reload loop bind 169.254.20.10 10.254.0.2 forward . /etc/resolv.conf prometheus :9253 } 节点上查看localdns的网卡，本地将监听169.254.20.10与10.254.0.2两个地址，拦截kubedns((默认10.254.0.2)的请求，命中后直接返回，若未命中转发到kubedns(对应service kube-dns-upstream，kube-dns-upstream由localdns创建绑定kubedns pod) # ip addr show nodelocaldns 182232: nodelocaldns: mtu 1500 qdisc noop state DOWN link/ether 4e:62:1c:fd:56:12 brd ff:ff:ff:ff:ff:ff inet 169.254.20.10/32 brd 169.254.20.10 scope global nodelocaldns valid_lft forever preferred_lft forever inet 10.254.0.2/32 brd 10.254.0.2 scope global nodelocaldns valid_lft forever preferred_lft forever iptables规则，使用NOTRACK跳过其它表处理 iptables-save | egrep \"10.254.0.2|169.254.20.10\" -A PREROUTING -d 10.254.0.2/32 -p udp -m udp --dport 53 -j NOTRACK -A PREROUTING -d 10.254.0.2/32 -p tcp -m tcp --dport 53 -j NOTRACK -A PREROUTING -d 169.254.20.10/32 -p udp -m udp --dport 53 -j NOTRACK -A PREROUTING -d 169.254.20.10/32 -p tcp -m tcp --dport 53 -j NOTRACK -A OUTPUT -d 10.254.0.2/32 -p udp -m udp --dport 53 -j NOTRACK -A OUTPUT -d 10.254.0.2/32 -p tcp -m tcp --dport 53 -j NOTRACK -A INPUT -d 10.254.0.2/32 -p udp -m udp --dport 53 -j ACCEPT -A INPUT -d 10.254.0.2/32 -p tcp -m tcp --dport 53 -j ACCEPT -A OUTPUT -s 10.254.0.2/32 -p udp -m udp --sport 53 -j ACCEPT -A OUTPUT -s 10.254.0.2/32 -p tcp -m tcp --sport 53 -j ACCEPT ... -A KUBE-SERVICES -d 10.254.0.2/32 -p tcp -m comment --comment \"kube-system/kub","date":"Feb 01, 2021","objectID":"/k8s-dns-optimize/:2:1","series":null,"tags":["k8s","dns"],"title":"优化Kubernetes集群内DNS","uri":"/k8s-dns-optimize/#压测"},{"categories":["cloud"],"content":" NodeLocalDNSNodeLocal DNSCache 通过在集群节点上作为 DaemonSet 运行 dns 缓存代理来提高集群 DNS 性能， 借助这种新架构，Pods 将可以访问在同一节点上运行的 dns 缓存代理，从而避免了 iptables DNAT 规则和连接跟踪。 架构如下: NodeLocalDNS的设计提案见（nodelocal-dns-cache） 验证官方安装方式见nodelocaldns，需要自行替换变量 可通过如下脚本，一键安装（注意设置kubedns svc ClusterIP） #!/bin/bash wget https://raw.githubusercontent.com/kubernetes/kubernetes/master/cluster/addons/dns/nodelocaldns/nodelocaldns.yaml # registery docker_registery=k8s.gcr.io/dns/k8s-dns-node-cache # kube-dns svc clusterip kubedns_svc=10.254.0.2 # nodelocaldns ip nodelocaldns_ip=169.254.20.10 # kube-proxy mode, iptables or ipvs kubeproxy_mode=iptables result=result.yaml if [ ${kubeproxy_mode} == \"ipvs\" ]; then sed -e \"s|k8s.gcr.io/dns/k8s-dns-node-cache|$docker_registery|g\" \\ -e \"s/__PILLAR__CLUSTER__DNS__/$kubedns_svc/g\" \\ -e \"s/__PILLAR__LOCAL__DNS__/$nodelocaldns_ip/g\" \\ -e 's/[ |,]__PILLAR__DNS__SERVER__//g' \\ -e \"s/__PILLAR__DNS__DOMAIN__/cluster.local/g\" nodelocaldns.yaml \u003e$result else sed -e \"s|k8s.gcr.io/dns/k8s-dns-node-cache|$docker_registery|g\" \\ -e \"s/__PILLAR__DNS__SERVER__/$kubedns_svc/g\" \\ -e \"s/__PILLAR__LOCAL__DNS__/$nodelocaldns_ip/g\" \\ -e \"s/__PILLAR__DNS__DOMAIN__/cluster.local/g\" nodelocaldns.yaml \u003e$result fi kubectl apply -f $result 创建完成后，每个节点运行一个pod，查看pod(个别节点ingress-nginx占用8080端口，导致nodelocaldns启动失败) # kubectl get po -n kube-system -l k8s-app=node-local-dns -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES node-local-dns-2fvxb 0/1 CrashLoopBackOff 4 103s 10.38.200.195 node04 node-local-dns-4zmcd 1/1 Running 0 54d 10.38.201.55 node06 node-local-dns-55tzg 1/1 Running 0 60d 10.38.200.186 node02 node-local-dns-cctg7 1/1 Running 0 54d 10.38.200.242 node07 node-local-dns-khgmm 1/1 Running 0 54d 10.38.201.36 node08 node-local-dns-mbr64 1/1 Running 0 60d 10.38.200.187 node05 node-local-dns-t67vw 1/1 Running 0 60d 10.38.200.188 node03 node-local-dns-tmm92 1/1 Running 14 54d 10.38.200.57 node09 默认配置如下： cluster.local:53 { errors cache { success 9984 30 # 默认成功缓存30s denial 9984 5 #失败缓存5s } reload loop bind 169.254.20.10 10.254.0.2 #本地监听ip forward . 10.254.132.95 { #转发到kubedns-upstream force_tcp } prometheus :9253 #监控接口 health 169.254.20.10:8080 #健康检测端口 } in-addr.arpa:53 { errors cache 30 reload loop bind 169.254.20.10 10.254.0.2 forward . 10.254.132.95 { force_tcp } prometheus :9253 } ip6.arpa:53 { errors cache 30 reload loop bind 169.254.20.10 10.254.0.2 forward . 10.254.132.95 { force_tcp } prometheus :9253 } .:53 { errors cache 30 reload loop bind 169.254.20.10 10.254.0.2 forward . /etc/resolv.conf prometheus :9253 } 节点上查看localdns的网卡，本地将监听169.254.20.10与10.254.0.2两个地址，拦截kubedns((默认10.254.0.2)的请求，命中后直接返回，若未命中转发到kubedns(对应service kube-dns-upstream，kube-dns-upstream由localdns创建绑定kubedns pod) # ip addr show nodelocaldns 182232: nodelocaldns: mtu 1500 qdisc noop state DOWN link/ether 4e:62:1c:fd:56:12 brd ff:ff:ff:ff:ff:ff inet 169.254.20.10/32 brd 169.254.20.10 scope global nodelocaldns valid_lft forever preferred_lft forever inet 10.254.0.2/32 brd 10.254.0.2 scope global nodelocaldns valid_lft forever preferred_lft forever iptables规则，使用NOTRACK跳过其它表处理 iptables-save | egrep \"10.254.0.2|169.254.20.10\" -A PREROUTING -d 10.254.0.2/32 -p udp -m udp --dport 53 -j NOTRACK -A PREROUTING -d 10.254.0.2/32 -p tcp -m tcp --dport 53 -j NOTRACK -A PREROUTING -d 169.254.20.10/32 -p udp -m udp --dport 53 -j NOTRACK -A PREROUTING -d 169.254.20.10/32 -p tcp -m tcp --dport 53 -j NOTRACK -A OUTPUT -d 10.254.0.2/32 -p udp -m udp --dport 53 -j NOTRACK -A OUTPUT -d 10.254.0.2/32 -p tcp -m tcp --dport 53 -j NOTRACK -A INPUT -d 10.254.0.2/32 -p udp -m udp --dport 53 -j ACCEPT -A INPUT -d 10.254.0.2/32 -p tcp -m tcp --dport 53 -j ACCEPT -A OUTPUT -s 10.254.0.2/32 -p udp -m udp --sport 53 -j ACCEPT -A OUTPUT -s 10.254.0.2/32 -p tcp -m tcp --sport 53 -j ACCEPT ... -A KUBE-SERVICES -d 10.254.0.2/32 -p tcp -m comment --comment \"kube-system/kub","date":"Feb 01, 2021","objectID":"/k8s-dns-optimize/:2:1","series":null,"tags":["k8s","dns"],"title":"优化Kubernetes集群内DNS","uri":"/k8s-dns-optimize/#优缺点"},{"categories":["cloud"],"content":" NodeLocalDNSNodeLocal DNSCache 通过在集群节点上作为 DaemonSet 运行 dns 缓存代理来提高集群 DNS 性能， 借助这种新架构，Pods 将可以访问在同一节点上运行的 dns 缓存代理，从而避免了 iptables DNAT 规则和连接跟踪。 架构如下: NodeLocalDNS的设计提案见（nodelocal-dns-cache） 验证官方安装方式见nodelocaldns，需要自行替换变量 可通过如下脚本，一键安装（注意设置kubedns svc ClusterIP） #!/bin/bash wget https://raw.githubusercontent.com/kubernetes/kubernetes/master/cluster/addons/dns/nodelocaldns/nodelocaldns.yaml # registery docker_registery=k8s.gcr.io/dns/k8s-dns-node-cache # kube-dns svc clusterip kubedns_svc=10.254.0.2 # nodelocaldns ip nodelocaldns_ip=169.254.20.10 # kube-proxy mode, iptables or ipvs kubeproxy_mode=iptables result=result.yaml if [ ${kubeproxy_mode} == \"ipvs\" ]; then sed -e \"s|k8s.gcr.io/dns/k8s-dns-node-cache|$docker_registery|g\" \\ -e \"s/__PILLAR__CLUSTER__DNS__/$kubedns_svc/g\" \\ -e \"s/__PILLAR__LOCAL__DNS__/$nodelocaldns_ip/g\" \\ -e 's/[ |,]__PILLAR__DNS__SERVER__//g' \\ -e \"s/__PILLAR__DNS__DOMAIN__/cluster.local/g\" nodelocaldns.yaml \u003e$result else sed -e \"s|k8s.gcr.io/dns/k8s-dns-node-cache|$docker_registery|g\" \\ -e \"s/__PILLAR__DNS__SERVER__/$kubedns_svc/g\" \\ -e \"s/__PILLAR__LOCAL__DNS__/$nodelocaldns_ip/g\" \\ -e \"s/__PILLAR__DNS__DOMAIN__/cluster.local/g\" nodelocaldns.yaml \u003e$result fi kubectl apply -f $result 创建完成后，每个节点运行一个pod，查看pod(个别节点ingress-nginx占用8080端口，导致nodelocaldns启动失败) # kubectl get po -n kube-system -l k8s-app=node-local-dns -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES node-local-dns-2fvxb 0/1 CrashLoopBackOff 4 103s 10.38.200.195 node04 node-local-dns-4zmcd 1/1 Running 0 54d 10.38.201.55 node06 node-local-dns-55tzg 1/1 Running 0 60d 10.38.200.186 node02 node-local-dns-cctg7 1/1 Running 0 54d 10.38.200.242 node07 node-local-dns-khgmm 1/1 Running 0 54d 10.38.201.36 node08 node-local-dns-mbr64 1/1 Running 0 60d 10.38.200.187 node05 node-local-dns-t67vw 1/1 Running 0 60d 10.38.200.188 node03 node-local-dns-tmm92 1/1 Running 14 54d 10.38.200.57 node09 默认配置如下： cluster.local:53 { errors cache { success 9984 30 # 默认成功缓存30s denial 9984 5 #失败缓存5s } reload loop bind 169.254.20.10 10.254.0.2 #本地监听ip forward . 10.254.132.95 { #转发到kubedns-upstream force_tcp } prometheus :9253 #监控接口 health 169.254.20.10:8080 #健康检测端口 } in-addr.arpa:53 { errors cache 30 reload loop bind 169.254.20.10 10.254.0.2 forward . 10.254.132.95 { force_tcp } prometheus :9253 } ip6.arpa:53 { errors cache 30 reload loop bind 169.254.20.10 10.254.0.2 forward . 10.254.132.95 { force_tcp } prometheus :9253 } .:53 { errors cache 30 reload loop bind 169.254.20.10 10.254.0.2 forward . /etc/resolv.conf prometheus :9253 } 节点上查看localdns的网卡，本地将监听169.254.20.10与10.254.0.2两个地址，拦截kubedns((默认10.254.0.2)的请求，命中后直接返回，若未命中转发到kubedns(对应service kube-dns-upstream，kube-dns-upstream由localdns创建绑定kubedns pod) # ip addr show nodelocaldns 182232: nodelocaldns: mtu 1500 qdisc noop state DOWN link/ether 4e:62:1c:fd:56:12 brd ff:ff:ff:ff:ff:ff inet 169.254.20.10/32 brd 169.254.20.10 scope global nodelocaldns valid_lft forever preferred_lft forever inet 10.254.0.2/32 brd 10.254.0.2 scope global nodelocaldns valid_lft forever preferred_lft forever iptables规则，使用NOTRACK跳过其它表处理 iptables-save | egrep \"10.254.0.2|169.254.20.10\" -A PREROUTING -d 10.254.0.2/32 -p udp -m udp --dport 53 -j NOTRACK -A PREROUTING -d 10.254.0.2/32 -p tcp -m tcp --dport 53 -j NOTRACK -A PREROUTING -d 169.254.20.10/32 -p udp -m udp --dport 53 -j NOTRACK -A PREROUTING -d 169.254.20.10/32 -p tcp -m tcp --dport 53 -j NOTRACK -A OUTPUT -d 10.254.0.2/32 -p udp -m udp --dport 53 -j NOTRACK -A OUTPUT -d 10.254.0.2/32 -p tcp -m tcp --dport 53 -j NOTRACK -A INPUT -d 10.254.0.2/32 -p udp -m udp --dport 53 -j ACCEPT -A INPUT -d 10.254.0.2/32 -p tcp -m tcp --dport 53 -j ACCEPT -A OUTPUT -s 10.254.0.2/32 -p udp -m udp --sport 53 -j ACCEPT -A OUTPUT -s 10.254.0.2/32 -p tcp -m tcp --sport 53 -j ACCEPT ... -A KUBE-SERVICES -d 10.254.0.2/32 -p tcp -m comment --comment \"kube-system/kub","date":"Feb 01, 2021","objectID":"/k8s-dns-optimize/:2:1","series":null,"tags":["k8s","dns"],"title":"优化Kubernetes集群内DNS","uri":"/k8s-dns-optimize/#ha"},{"categories":["cloud"],"content":" NodeLocalDNSNodeLocal DNSCache 通过在集群节点上作为 DaemonSet 运行 dns 缓存代理来提高集群 DNS 性能， 借助这种新架构，Pods 将可以访问在同一节点上运行的 dns 缓存代理，从而避免了 iptables DNAT 规则和连接跟踪。 架构如下: NodeLocalDNS的设计提案见（nodelocal-dns-cache） 验证官方安装方式见nodelocaldns，需要自行替换变量 可通过如下脚本，一键安装（注意设置kubedns svc ClusterIP） #!/bin/bash wget https://raw.githubusercontent.com/kubernetes/kubernetes/master/cluster/addons/dns/nodelocaldns/nodelocaldns.yaml # registery docker_registery=k8s.gcr.io/dns/k8s-dns-node-cache # kube-dns svc clusterip kubedns_svc=10.254.0.2 # nodelocaldns ip nodelocaldns_ip=169.254.20.10 # kube-proxy mode, iptables or ipvs kubeproxy_mode=iptables result=result.yaml if [ ${kubeproxy_mode} == \"ipvs\" ]; then sed -e \"s|k8s.gcr.io/dns/k8s-dns-node-cache|$docker_registery|g\" \\ -e \"s/__PILLAR__CLUSTER__DNS__/$kubedns_svc/g\" \\ -e \"s/__PILLAR__LOCAL__DNS__/$nodelocaldns_ip/g\" \\ -e 's/[ |,]__PILLAR__DNS__SERVER__//g' \\ -e \"s/__PILLAR__DNS__DOMAIN__/cluster.local/g\" nodelocaldns.yaml \u003e$result else sed -e \"s|k8s.gcr.io/dns/k8s-dns-node-cache|$docker_registery|g\" \\ -e \"s/__PILLAR__DNS__SERVER__/$kubedns_svc/g\" \\ -e \"s/__PILLAR__LOCAL__DNS__/$nodelocaldns_ip/g\" \\ -e \"s/__PILLAR__DNS__DOMAIN__/cluster.local/g\" nodelocaldns.yaml \u003e$result fi kubectl apply -f $result 创建完成后，每个节点运行一个pod，查看pod(个别节点ingress-nginx占用8080端口，导致nodelocaldns启动失败) # kubectl get po -n kube-system -l k8s-app=node-local-dns -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES node-local-dns-2fvxb 0/1 CrashLoopBackOff 4 103s 10.38.200.195 node04 node-local-dns-4zmcd 1/1 Running 0 54d 10.38.201.55 node06 node-local-dns-55tzg 1/1 Running 0 60d 10.38.200.186 node02 node-local-dns-cctg7 1/1 Running 0 54d 10.38.200.242 node07 node-local-dns-khgmm 1/1 Running 0 54d 10.38.201.36 node08 node-local-dns-mbr64 1/1 Running 0 60d 10.38.200.187 node05 node-local-dns-t67vw 1/1 Running 0 60d 10.38.200.188 node03 node-local-dns-tmm92 1/1 Running 14 54d 10.38.200.57 node09 默认配置如下： cluster.local:53 { errors cache { success 9984 30 # 默认成功缓存30s denial 9984 5 #失败缓存5s } reload loop bind 169.254.20.10 10.254.0.2 #本地监听ip forward . 10.254.132.95 { #转发到kubedns-upstream force_tcp } prometheus :9253 #监控接口 health 169.254.20.10:8080 #健康检测端口 } in-addr.arpa:53 { errors cache 30 reload loop bind 169.254.20.10 10.254.0.2 forward . 10.254.132.95 { force_tcp } prometheus :9253 } ip6.arpa:53 { errors cache 30 reload loop bind 169.254.20.10 10.254.0.2 forward . 10.254.132.95 { force_tcp } prometheus :9253 } .:53 { errors cache 30 reload loop bind 169.254.20.10 10.254.0.2 forward . /etc/resolv.conf prometheus :9253 } 节点上查看localdns的网卡，本地将监听169.254.20.10与10.254.0.2两个地址，拦截kubedns((默认10.254.0.2)的请求，命中后直接返回，若未命中转发到kubedns(对应service kube-dns-upstream，kube-dns-upstream由localdns创建绑定kubedns pod) # ip addr show nodelocaldns 182232: nodelocaldns: mtu 1500 qdisc noop state DOWN link/ether 4e:62:1c:fd:56:12 brd ff:ff:ff:ff:ff:ff inet 169.254.20.10/32 brd 169.254.20.10 scope global nodelocaldns valid_lft forever preferred_lft forever inet 10.254.0.2/32 brd 10.254.0.2 scope global nodelocaldns valid_lft forever preferred_lft forever iptables规则，使用NOTRACK跳过其它表处理 iptables-save | egrep \"10.254.0.2|169.254.20.10\" -A PREROUTING -d 10.254.0.2/32 -p udp -m udp --dport 53 -j NOTRACK -A PREROUTING -d 10.254.0.2/32 -p tcp -m tcp --dport 53 -j NOTRACK -A PREROUTING -d 169.254.20.10/32 -p udp -m udp --dport 53 -j NOTRACK -A PREROUTING -d 169.254.20.10/32 -p tcp -m tcp --dport 53 -j NOTRACK -A OUTPUT -d 10.254.0.2/32 -p udp -m udp --dport 53 -j NOTRACK -A OUTPUT -d 10.254.0.2/32 -p tcp -m tcp --dport 53 -j NOTRACK -A INPUT -d 10.254.0.2/32 -p udp -m udp --dport 53 -j ACCEPT -A INPUT -d 10.254.0.2/32 -p tcp -m tcp --dport 53 -j ACCEPT -A OUTPUT -s 10.254.0.2/32 -p udp -m udp --sport 53 -j ACCEPT -A OUTPUT -s 10.254.0.2/32 -p tcp -m tcp --sport 53 -j ACCEPT ... -A KUBE-SERVICES -d 10.254.0.2/32 -p tcp -m comment --comment \"kube-system/kub","date":"Feb 01, 2021","objectID":"/k8s-dns-optimize/:2:1","series":null,"tags":["k8s","dns"],"title":"优化Kubernetes集群内DNS","uri":"/k8s-dns-optimize/#灰度方式"},{"categories":["cloud"],"content":" 本地DNS缓存除了nodelocaldns，用户还可以在容器内或者添加sidecar来启用dns缓存 通过在镜像中加入nscd进程，缓存dns，如下： FROM ubuntu RUN apt-get update \u0026\u0026 apt-get install -y nscd \u0026\u0026 rm -rf /var/lib/apt/lists/* CMD service nscd start; bash -c \"sleep 3600\" 此种方式需要用户改动镜像，或者加入额外脚本配置nscd 另外可以配置可配置dns缓存 sidecar（如coredns, dnsmasq）来提高性能，此种方式灵活性高，但需要改动pod配置，而且较nodelocaldns浪费资源 ","date":"Feb 01, 2021","objectID":"/k8s-dns-optimize/:2:2","series":null,"tags":["k8s","dns"],"title":"优化Kubernetes集群内DNS","uri":"/k8s-dns-optimize/#本地dns缓存"},{"categories":["cloud"],"content":" 参考 https://kubernetes.io/zh/docs/tasks/administer-cluster/nodelocaldns/ https://help.aliyun.com/document_detail/172339.html https://github.com/kubernetes/enhancements/blob/master/keps/sig-network/0030-nodelocal-dns-cache.md https://github.com/kubernetes/enhancements/blob/master/keps/sig-network/1024-nodelocal-cache-dns/README.md https://lework.github.io/2020/11/09/node-local-dns/ ","date":"Feb 01, 2021","objectID":"/k8s-dns-optimize/:3:0","series":null,"tags":["k8s","dns"],"title":"优化Kubernetes集群内DNS","uri":"/k8s-dns-optimize/#参考"},{"categories":["cloud"],"content":" 背景为了防止突发流量影响apiserver可用性，k8s支持多种限流配置，包括： MaxInFlightLimit，server级别整体限流 Client限流 EventRateLimit, 限制event APF，更细力度的限制配置 ","date":"Nov 11, 2020","objectID":"/k8s-rate-limit/:1:0","series":null,"tags":["k8s","apiserver"],"title":"kubernetes apiserver限流方案","uri":"/k8s-rate-limit/#背景"},{"categories":["cloud"],"content":" MaxInFlightLimitMaxInFlightLimit限流，apiserver默认可设置最大并发量（集群级别，区分只读与修改操作），通过参数--max-requests-inflight和 --max-mutating-requests-inflight， 可以简单实现限流。 ","date":"Nov 11, 2020","objectID":"/k8s-rate-limit/:1:1","series":null,"tags":["k8s","apiserver"],"title":"kubernetes apiserver限流方案","uri":"/k8s-rate-limit/#maxinflightlimit"},{"categories":["cloud"],"content":" Client限流例如client-go默认的qps为5，但是只支持客户端限流，集群管理员无法控制用户行为。 ","date":"Nov 11, 2020","objectID":"/k8s-rate-limit/:1:2","series":null,"tags":["k8s","apiserver"],"title":"kubernetes apiserver限流方案","uri":"/k8s-rate-limit/#client限流"},{"categories":["cloud"],"content":" EventRateLimitEventRateLimit在1.13之后支持，只限制event请求，集成在apiserver内部webhoook中，可配置某个用户、namespace、server等event操作限制，通过webhook形式实现。 具体原理可以参考提案，每个eventratelimit 配置使用一个单独的令牌桶限速器，每次event操作，遍历每个匹配的限速器检查是否能获取令牌，如果可以允许请求，否则返回429。 优点 实现简单，允许一定量的并发 可支持server/namespace/user等级别的限流 缺点 仅支持event，通过webhook实现只能拦截修改类请求 所有namespace的限流相同，没有优先级 ","date":"Nov 11, 2020","objectID":"/k8s-rate-limit/:1:3","series":null,"tags":["k8s","apiserver"],"title":"kubernetes apiserver限流方案","uri":"/k8s-rate-limit/#eventratelimit"},{"categories":["cloud"],"content":" API 优先级和公平性apiserver默认的限流方式太过简单，一个错误的客户端发送大量请求可能造成其他客户端请求异常，也不支持突发流量。 API 优先级和公平性（APF）是MaxInFlightLimit限流的一种替代方案，设计文档见提案。 API 优先级和公平性（1.15以上，alpha版本）， 以更细粒度（byUser，byNamespace）对请求进行分类和隔离。 支持突发流量，通过使用公平排队技术从队列中分发请求从而避免饥饿。 APF限流通过两种资源，PriorityLevelConfigurations定义隔离类型和可处理的并发预算量，还可以调整排队行为。 FlowSchemas用于对每个入站请求进行分类，并与一个 PriorityLevelConfigurations相匹配。 可对用户或用户组或全局进行某些资源某些请求的限制，如限制default namespace写services put/patch请求。 优点 考虑情况较全面，支持优先级，白名单等 可支持server/namespace/user/resource等细粒度级别的限流 缺点 配置复杂，不直观，需要对APF原理深入了解 功能较新，缺少生产环境验证 APF测试 开启APF，需要在apiserver配置--feature-gates=APIPriorityAndFairness=true --runtime-config=flowcontrol.apiserver.k8s.io/v1alpha1=true 开启后，获取默认的FlowSchemas $ kubectl get flowschemas.flowcontrol.apiserver.k8s.io NAME PRIORITYLEVEL MATCHINGPRECEDENCE DISTINGUISHERMETHOD AGE MISSINGPL system-leader-election leader-election 100 ByUser 152m False workload-leader-election leader-election 200 ByUser 152m False system-nodes system 500 ByUser 152m False kube-controller-manager workload-high 800 ByNamespace 152m False kube-scheduler workload-high 800 ByNamespace 152m False kube-system-service-accounts workload-high 900 ByNamespace 152m False health-for-strangers exempt 1000 \u003cnone\u003e 151m False service-accounts workload-low 9000 ByUser 152m False global-default global-default 9900 ByUser 152m False catch-all catch-all 10000 ByUser 152m False FlowShema配置 apiVersion: flowcontrol.apiserver.k8s.io/v1alpha1 kind: FlowSchema metadata: name: health-for-strangers spec: matchingPrecedence: 1000 #匹配优先级，1~1000，越小优先级越高 priorityLevelConfiguration: #关联的PriorityLevelConfigurations name: exempt #排除rules，即不限制当前flowshema的rules rules: #请求规则 - nonResourceRules: #非资源 - nonResourceURLs: - \"/healthz\" - \"/livez\" - \"/readyz\" verbs: - \"*\" subjects: #对应的用户或用户组 - kind: Group group: name: system:unauthenticated PriorityLevelConfiguration配置 apiVersion: flowcontrol.apiserver.k8s.io/v1alpha1 kind: PriorityLevelConfiguration metadata: name: leader-election spec: limited: #限制策略 assuredConcurrencyShares: 10 limitResponse: #如何处理被限制的请求 queuing: #类型为Queue时，列队的设置 handSize: 4 #队列 queueLengthLimit: 50 #队列长度 queues: 16 #队列数 type: Queue #Queue或者Reject，Reject直接返回429，Queue将请求加入队列 type: Limited #类型，Limited或Exempt， Exempt即不限制 ","date":"Nov 11, 2020","objectID":"/k8s-rate-limit/:1:4","series":null,"tags":["k8s","apiserver"],"title":"kubernetes apiserver限流方案","uri":"/k8s-rate-limit/#api-优先级和公平性"},{"categories":["cloud"],"content":" 总结以上是k8s相关的限流策略，通过多种策略来保证集群的稳定性。 目前MaxInFlightLimit可以轻松开启，但是限制策略不精细，而APF功能较新，实现较复杂，在充分验证后，可通过APF对全集群进行限流。 ","date":"Nov 11, 2020","objectID":"/k8s-rate-limit/:2:0","series":null,"tags":["k8s","apiserver"],"title":"kubernetes apiserver限流方案","uri":"/k8s-rate-limit/#总结"},{"categories":["cloud"],"content":" 背景在多个容器的Pod中，通常业务容器需要依赖sidecar。启动时sidecar需要先启动，退出时sidecar需要在业务容器退出后再退出。k8s目前对于sidecar的生命周期比较有争议，见issue、sidecarcontainers。 Kubernetes Pod 内有两种容器: 初始化容器(init container)和应用容器(app container)。 其中初始化容器的执行先于应用容器，按顺序启动，执行成功启动下一个： if container := podContainerChanges.NextInitContainerToStart; container != nil { // Start the next init container. if err := start(\"init container\", containerStartSpec(container)); err != nil { return } // Successfully started the container; clear the entry in the failure klog.V(4).Infof(\"Completed init container %q for pod %q\", container.Name, format.Pod(pod)) } 而对于应用容器，无法保证容器ready顺序，启动代码如下: // Step 7: start containers in podContainerChanges.ContainersToStart. for _, idx := range podContainerChanges.ContainersToStart { // start函数向docker发请求启动容器，这里没有检测函数返回而且不确定ENTRYPOINT是否成功 start(\"container\", containerStartSpec(\u0026pod.Spec.Containers[idx])) } 在删除时，同样无法保证删除顺序，代码如下 for _, container := range runningPod.Containers { go func(container *kubecontainer.Container) { killContainerResult := kubecontainer.NewSyncResult(kubecontainer.KillContainer, container.Name) // 每一个容器起goroutine执行删除 if err := m.killContainer(pod, container.ID, container.Name, \"\", gracePeriodOverride); err != nil { ... } containerResults \u003c- killContainerResult }(container) } ","date":"Sep 21, 2020","objectID":"/k8s-sideccar-lifecycle/:1:0","series":null,"tags":["k8s","container"],"title":"Kubernetes中Sidecar生命周期管理","uri":"/k8s-sideccar-lifecycle/#背景"},{"categories":["cloud"],"content":" 启动顺序k8s原生方式，对于pod中一个容器依赖另一个容器，目前需要业务进程判断依赖服务是否启动或者sleep 10s，这种方式可以工作，但不太优雅。需要业务更改启动脚本。 那么，有没有其他的解决办法？ ","date":"Sep 21, 2020","objectID":"/k8s-sideccar-lifecycle/:2:0","series":null,"tags":["k8s","container"],"title":"Kubernetes中Sidecar生命周期管理","uri":"/k8s-sideccar-lifecycle/#启动顺序"},{"categories":["cloud"],"content":" 源码分析在启动时，start函数调用startContainer来创建容器，主要代码如下： func (m *kubeGenericRuntimeManager) startContainer(podSandboxID string, podSandboxConfig *runtimeapi.PodSandboxConfig, spec *startSpec, pod *v1.Pod, podStatus *kubecontainer.PodStatus, pullSecrets []v1.Secret, podIP string, podIPs []string) (string, error) { container := spec.container // Step 1: 拉镜像. imageRef, msg, err := m.imagePuller.EnsureImageExists(pod, container, pullSecrets, podSandboxConfig) if err != nil { ... } // Step 2: 调用cri创建容器 // For a new container, the RestartCount should be 0 containerID, err := m.runtimeService.CreateContainer(podSandboxID, containerConfig, podSandboxConfig) ... // Step 3: 启动容器 err = m.runtimeService.StartContainer(containerID) // Step 4: 执行 post start hook. if container.Lifecycle != nil \u0026\u0026 container.Lifecycle.PostStart != nil { kubeContainerID := kubecontainer.ContainerID{ Type: m.runtimeName, ID: containerID, } // 调用Run来执行hook msg, handlerErr := m.runner.Run(kubeContainerID, pod, container, container.Lifecycle.PostStart) ... } return \"\", nil } 步骤如下： 拉取镜像 创建容器 启动容器 执行hook 一个Pod中容器的启动是有顺序的，排在前面容器的先启动。同时第一个容器执行完ENTRYPOINT和PostStart之后（异步执行，无法确定顺序），k8s才会创建第二个容器（这样的话就可以保证第一个容器创建多长时间后再启动第二个容器） 如果我们PostStart阶段去检测容器是否ready，那么只有在ready后才去执行下一个容器。 ","date":"Sep 21, 2020","objectID":"/k8s-sideccar-lifecycle/:2:1","series":null,"tags":["k8s","container"],"title":"Kubernetes中Sidecar生命周期管理","uri":"/k8s-sideccar-lifecycle/#源码分析"},{"categories":["cloud"],"content":" 测试配置如下，sidecar模拟需要依赖的容器，main为业务容器 apiVersion: v1 kind: Pod metadata: name: test-start spec: containers: - name: sidecar image: busybox command: [\"/bin/sh\", \"-c\", \"sleep 3600\"] lifecycle: postStart: exec: command: [\"/bin/sh\", \"-c\", \"sleep 20\"] - name: main image: busybox command: [\"/bin/sh\", \"-c\", \"sleep 3600\"] 得到结果如下，可以看到sidecar启动21s后才开始启动main容器，满足需求 Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Scheduled 54s default-scheduler Successfully assigned default/test-start to tj1-staging-k8s-slave95-202008.kscn Normal Pulling 53s kubelet, tj1-staging-k8s-slave95-202008.kscn Pulling image \"busybox\" Normal Pulled 44s kubelet, tj1-staging-k8s-slave95-202008.kscn Successfully pulled image \"busybox\" Normal Created 44s kubelet, tj1-staging-k8s-slave95-202008.kscn Created container sidecar Normal Started 44s kubelet, tj1-staging-k8s-slave95-202008.kscn Started container sidecar Normal Pulling 23s kubelet, tj1-staging-k8s-slave95-202008.kscn Pulling image \"busybox\" Normal Pulled 19s kubelet, tj1-staging-k8s-slave95-202008.kscn Successfully pulled image \"busybox\" Normal Created 18s kubelet, tj1-staging-k8s-slave95-202008.kscn Created container main Normal Started 18s kubelet, tj1-staging-k8s-slave95-202008.kscn Started container main 此方案可能存在的缺点： 如果sidecar启动失败或者hook失败，其他容器会立即启动 ","date":"Sep 21, 2020","objectID":"/k8s-sideccar-lifecycle/:2:2","series":null,"tags":["k8s","container"],"title":"Kubernetes中Sidecar生命周期管理","uri":"/k8s-sideccar-lifecycle/#测试"},{"categories":["cloud"],"content":" 退出顺序容器启动顺序比较好解决，退出顺序则是按照相反的顺序，业务容器先退出，之后sidecar再退出。 目前，在kubelet删除pod步骤如下; 遍历容器，每个容器起一个goroutine删除 删除时，先执行pre stop hook，得到gracePeriod=DeletionGracePeriodSeconds-period(stophook) 再调用cri删除接口m.runtimeService.StopContainer(containerID.ID, gracePeriod) 如果在sidecar的pre stop hook检测业务容器状态，那么可以延迟退出。 ","date":"Sep 21, 2020","objectID":"/k8s-sideccar-lifecycle/:3:0","series":null,"tags":["k8s","container"],"title":"Kubernetes中Sidecar生命周期管理","uri":"/k8s-sideccar-lifecycle/#退出顺序"},{"categories":["cloud"],"content":" 测试业务容器main退出时，创建文件；sidecar通过post-stop检测到文件后，执行退出 apiVersion: v1 kind: Pod metadata: name: test-stop spec: containers: - name: sidecar image: busybox command: - \"/bin/sh\" - \"-c\" - | trap \"touch /lifecycle/sidecar-terminated\" 15 until [ -f \"/lifecycle/sidecar-terminated\" ];do date sleep 1 done sleep 5 cat /lifecycle/main-terminated t=$(date) echo \"sidecar exit at $t\" lifecycle: preStop: exec: command: - \"/bin/sh\" - \"-c\" - | until [ -f \"/lifecycle/main-terminated\" ];do sleep 1 done t=$(date) echo \"main exit at $t\" \u003e /lifecycle/main-terminated volumeMounts: - name: lifecycle mountPath: /lifecycle - name: main image: busybox command: - \"/bin/sh\" - \"-c\" - | trap \"touch /lifecycle/main-terminated\" 15 until [ -f \"/lifecycle/main-terminated\" ];do date sleep 1 done volumeMounts: - name: lifecycle mountPath: /lifecycle volumes: - name: lifecycle emptyDir: {} 在日志中看到，main容器先结束，sidecar检测到main-terminated文件后，执行完post-stop-hook，sidecar主进程开始退出 $ kubectl logs -f test-stop main ... Tue Sep 8 03:14:20 UTC 2020 Tue Sep 8 03:14:21 UTC 2020 Tue Sep 8 03:14:22 UTC 2020 $ kubectl logs -f test-stop sidecar Tue Sep 8 03:14:22 UTC 2020 Tue Sep 8 03:14:23 UTC 2020 # post stop hook 检测到main容器退出，记录日志 main exit at Tue Sep 8 03:14:23 UTC 2020 # sidecar主进程退出 sidecar exit at Tue Sep 8 03:14:29 UTC 2020 通过测试，使用postStopHook可以达到sidecar延迟退出的目的，但这种方式也有一些缺点 配置复杂，多个sidecar都需要配置postStop监听业务容器状态 业务容器需要有可观察性（提供特定形式的健康检测） poststop执行异常，会等到最大优雅退出时间（默认30s）后才终止 ","date":"Sep 21, 2020","objectID":"/k8s-sideccar-lifecycle/:3:1","series":null,"tags":["k8s","container"],"title":"Kubernetes中Sidecar生命周期管理","uri":"/k8s-sideccar-lifecycle/#测试-1"},{"categories":["cloud"],"content":" 总结目前对于sidecar生命周期的支持方案对比如下： 方案 启动顺序 退出顺序 job sidecar 是否需要用户修改代码 是否需要修改k8s代码 缺点 备注 用户控制 支持 不支持 不支持 需要 不需要 需要用户更改启动脚本;退出支持难度大，需要同时修改业务容器与sidecar启动脚本；大部分情况不支持 启动时需要检测sidecar服务状态 Lifecycle Hooks 支持 支持 不支持 不需要 不需要 配置hook复杂度高;在hook执行异常情况下不能确保顺序 富容器 支持 部分支持 部分支持 不需要 需要（更改镜像或启动命令） 所有功能集成在一个容器中，对于外部sidecar如istio envoy等，不可控; 修改源码 支持 支持 支持 不需要 需要 需要满足各种情况，实现难度较大 社区有计划支持 在k8s提供此类功能前，目前没有完善的方案。Lifecycle Hooks不需要更改用户启动代码以及k8s相关代码，相对于其他方式不失为一种解决思路。 ","date":"Sep 21, 2020","objectID":"/k8s-sideccar-lifecycle/:4:0","series":null,"tags":["k8s","container"],"title":"Kubernetes中Sidecar生命周期管理","uri":"/k8s-sideccar-lifecycle/#总结"},{"categories":["cloud"],"content":" 背景目前k8s不支持容器启动顺序，部分业务通过开启shareProcessNamespace监控某些进程状态。当开启共享pid后，有用户反馈某个容器主进程退出，但是容器并没有重启，执行exec会卡住，现象参考issue ","date":"Jul 28, 2020","objectID":"/cotainer-init/:1:0","series":null,"tags":["k8s","docker"],"title":"开启shareProcessNamespace后容器异常","uri":"/cotainer-init/#背景"},{"categories":["cloud"],"content":" 复现 创建deployment apiVersion: apps/v1 kind: Deployment metadata: labels: app: nginx name: nginx spec: selector: matchLabels: app: nginx template: metadata: labels: app: nginx name: nginx spec: shareProcessNamespace: true containers: - image: nginx:alpine name: nginx 查看进程信息 由于开启了shareProcessNamespace, pause变为pid 1, nginx daemonpid为6, ppid为containerd-shim # 查看容器内进程 / # ps -efo \"pid,ppid,comm,args\" PID PPID COMMAND COMMAND 1 0 pause /pause 6 0 nginx nginx: master process nginx -g daemon off; 11 6 nginx nginx: worker process 12 6 nginx nginx: worker process 13 6 nginx nginx: worker process 14 6 nginx nginx: worker process 15 0 sh sh 47 15 ps ps -efo pid,ppid,comm,args 删除主进程 子进程被pid 1回收, 有时也会被containerd-shim回收 / # kill -9 6 / # / # ps -efo \"pid,ppid,comm,args\" PID PPID COMMAND COMMAND 1 0 pause /pause 11 1 nginx nginx: worker process 12 1 nginx nginx: worker process 13 1 nginx nginx: worker process 14 1 nginx nginx: worker process 15 0 sh sh 48 15 ps ps -efo pid,ppid,comm,args docker hang 此时对此容器执行docker命令(inspect, logs, exec)将卡住， 同样通过kubectl执行会超时。 ","date":"Jul 28, 2020","objectID":"/cotainer-init/:2:0","series":null,"tags":["k8s","docker"],"title":"开启shareProcessNamespace后容器异常","uri":"/cotainer-init/#复现"},{"categories":["cloud"],"content":" 分析在未开启shareProcessNamespace的容器中，主进程退出pid 1, 此pid namespace销毁，系统会kill其下的所有进程。开启后，pid 1为pause进程，容器主进程退出，由于共享pid namespace，其他进程没有退出变成孤儿进程。此时调用docker相关接口去操作容器，docker首先去找主进程，但主进程已经不存在了，导致异常(待确认)。 清理掉这些孤儿进程容器便会正常退出，可以kill掉这些进程或者killpause进程，即可恢复。 ","date":"Jul 28, 2020","objectID":"/cotainer-init/:3:0","series":null,"tags":["k8s","docker"],"title":"开启shareProcessNamespace后容器异常","uri":"/cotainer-init/#分析"},{"categories":["cloud"],"content":" 方案有没有优雅的方式解决此种问题，如果主进程退出子进程也一起退出便符合预期，这就需要进程管理工具来实现，在宿主机中有systemd、god，容器中也有类似的工具即init进程(传递信息，回收子进程)，常见的有 docker init, docker自带的init进程(即tini) tini, 可回收孤儿进程/僵尸进程，kill进程组等 dumb-init, 可管理进程，重写信号等 经过测试，tini进程只能回收前台程序，对于后台程序则无能为力(例如nohup, \u0026启动的程序)，dumb-init在主进程退出时，会传递信号给子进程，符合预期。 开启dumb-init进程的dockerfile如下，tini也类似 FROM nginx:alpine # tini # RUN apk add --no-cache tini # ENTRYPOINT [\"/sbin/tini\", \"-s\", \"-g\", \"--\"] # dumb-init RUN wget -O /usr/bin/dumb-init https://github.com/Yelp/dumb-init/releases/download/v1.2.2/dumb-init_1.2.2_amd64 RUN chmod +x /usr/bin/dumb-init ENTRYPOINT [\"/usr/bin/dumb-init\", \"-v\", \"--\"] CMD [\"nginx\", \"-g\", \"daemon off;\"] init方式对于此问题是一种临时的解决方案，需要docker从根本上解决此种情况。容器推荐单进程运行，但某些情况必须要运行多进程，如果不想处理处理传递回收进程等，可以通过init进程，无需更改代码即可实现。 ","date":"Jul 28, 2020","objectID":"/cotainer-init/:4:0","series":null,"tags":["k8s","docker"],"title":"开启shareProcessNamespace后容器异常","uri":"/cotainer-init/#方案"},{"categories":["cloud"],"content":" 参考 https://github.com/Yelp/dumb-init https://github.com/krallin/tini https://github.com/kubernetes/kubernetes/issues/92214 ","date":"Jul 28, 2020","objectID":"/cotainer-init/:5:0","series":null,"tags":["k8s","docker"],"title":"开启shareProcessNamespace后容器异常","uri":"/cotainer-init/#参考"},{"categories":["cloud"],"content":" rateprometheus中rate只能用于counter类型，对于需要聚合的数据需要先rate再sum，而不是rate(sum) ","date":"Jul 16, 2020","objectID":"/prometheus-best-practice-operation/:1:0","series":null,"tags":["prometheus","monitor"],"title":"Prometheus最佳实践-聚合函数","uri":"/prometheus-best-practice-operation/#rate"},{"categories":["cloud"],"content":" 数据准确性rate/increase/delta等操作对于原始值进行了外推（类似线性插件），得到的不是准确值 如rate(http_requests_total[2m])指两分钟内每秒平均请求量，通过2m内首尾两个数据外推得到差值，比120s得到； 同理increase(http_requests_total[2m])指的不是首尾两个值的增长量，而是外推后计算出2m内的增长量。 ","date":"Jul 16, 2020","objectID":"/prometheus-best-practice-operation/:2:0","series":null,"tags":["prometheus","monitor"],"title":"Prometheus最佳实践-聚合函数","uri":"/prometheus-best-practice-operation/#数据准确性"},{"categories":["cloud"],"content":" absent通常报警中，我们需要对某个对象是不是有数据进行监控（即nodata监控），absent用来验证指标是不是有数据很有用 ","date":"Jul 16, 2020","objectID":"/prometheus-best-practice-operation/:3:0","series":null,"tags":["prometheus","monitor"],"title":"Prometheus最佳实践-聚合函数","uri":"/prometheus-best-practice-operation/#absent"},{"categories":["cloud"],"content":" predict_linear线性回归预测，适合线性数据的预测，如预测etcd的未来4小时文件描述符使用量 predict_linear(cluster:etcd:fd_utilization[1h], 3600 * 4) ","date":"Jul 16, 2020","objectID":"/prometheus-best-practice-operation/:4:0","series":null,"tags":["prometheus","monitor"],"title":"Prometheus最佳实践-聚合函数","uri":"/prometheus-best-practice-operation/#predict_linear"},{"categories":["cloud"],"content":" quantile_over_time一段时间内统计分位数 quantile_over_time(0.9, http_requests_total[1d]) # 一天内请求量的90分位 ","date":"Jul 16, 2020","objectID":"/prometheus-best-practice-operation/:5:0","series":null,"tags":["prometheus","monitor"],"title":"Prometheus最佳实践-聚合函数","uri":"/prometheus-best-practice-operation/#quantile_over_time"},{"categories":["cloud"],"content":" bool某些情况的需要比较两个标量（通常用来报警），可以使用bool http_requests_total \u003e bool 100 ","date":"Jul 16, 2020","objectID":"/prometheus-best-practice-operation/:6:0","series":null,"tags":["prometheus","monitor"],"title":"Prometheus最佳实践-聚合函数","uri":"/prometheus-best-practice-operation/#bool"},{"categories":["cloud"],"content":"在k8s中通常用户通过ingress接入流量，转发到后端实例(ingress → pod)，在后端应用更新过程中，ingress是否能做到优雅升级，本文将通过分析升级流程与实验验证，说明在k8s中如何实现优化升级。 ","date":"Jun 19, 2020","objectID":"/k8s-graceful-update-app/:0:0","series":null,"tags":["k8s","ingress","nginx"],"title":"k8s如何优雅升级应用","uri":"/k8s-graceful-update-app/#"},{"categories":["cloud"],"content":" Ingress原理用户创建ingress资源后，ingress-nginx通过service获取到对应的endpoint，监听到endpoint变化后将动态更新upstream。 endpoint每次变化后会通过selector匹配的pod列表中ready pod（不包括待删除的pod, 及DeletionTimestamp不为空） pod ready = 所有container ready(启动成功, 健康检查通过) + 所有rediness gateway执行成功 那么endpoint在什么状况下会发生变化： service变化（一般不会） 扩缩容 升级 删除pod 不管是什么操作，可归结于启动、删除、退出 启动，只要确保pod ready时能服务能正常接受流量，不会影响影响服务 退出, 如果是应用异常退出，不能处理已接受的流量，此种状况是应用本身行为，不在讨论范围 删除, 由于k8s所有组件都采用监听机制，无法保证pod删除时ingress-nginx的后端已经更新 # 大约在2s内 ingress-nginx 生效时间 = endpoint 生效时间 + upstream更新时间 如果要保证pod删除时不丢流量，需要做到 已接受的请求需要处理完，可监听TERM信号，处理完再退出， 可参考https://kubernetes.io/docs/concepts/workloads/pods/pod/#termination-of-pods 删除时不接受新的请求，这部分无法保证，只能保证#1 ","date":"Jun 19, 2020","objectID":"/k8s-graceful-update-app/:1:0","series":null,"tags":["k8s","ingress","nginx"],"title":"k8s如何优雅升级应用","uri":"/k8s-graceful-update-app/#ingress原理"},{"categories":["cloud"],"content":" ingress-nginx 重试机制ingress-nginx默认开启了proxy_next_upstream，配置如下 # In case of errors try the next upstream server before returning an error proxy_next_upstream error timeout; proxy_next_upstream_timeout 0; proxy_next_upstream_tries 3; 如果一次请求中，upstream server 出错或超时将通过rr算法重试下一个server，最多尝试三次。如果后端大于三个实例，一个实例异常不会影响服务。 ","date":"Jun 19, 2020","objectID":"/k8s-graceful-update-app/:2:0","series":null,"tags":["k8s","ingress","nginx"],"title":"k8s如何优雅升级应用","uri":"/k8s-graceful-update-app/#ingress-nginx-重试机制"},{"categories":["cloud"],"content":" 升级策略对于Deployment有两种升级策略， Recreate与RollingUpdate Recreate, 先将旧版缩到0再将新版扩到期望值，不建议使用 RollingUpdate，默认策略，滚动更新 在滚动升级时主要依据maxSurge与maxUnavailable对新旧版本进行扩缩 maxSurge， 升级中最多有多少pod超过期望值 maxUnavailable， 此值用来计算升级中最小可用的实例数，最大不可用的实例数表示不准确 举个例子，比如10个副本的Deployment， 采用默认值maxSurge与maxUnavaiable都为25% // 向上取整为 3 maxSurge = replicas * deployment.spec.strategy.rollingUpdate.maxSurge(25%)= 2.5 // 向下取整为 2 maxUnavailable = replicas * deployment.spec.strategy.rollingUpdate.maxUnavailable(25%)= 2.5 maxAvailable = replicas(10) + MaxSurge（3） = 13 minAvailable := *(deployment.Spec.Replicas)（10） - maxUnavailable（2）= 8 在升级过程中，首先创建 newRS，然后为其设定 replicas，此时计算出 replicas 结果为 3。等到下一个 syncLoop 时，所有 rs 的 replicas 已经达到最大值 10 + 3 = 13，此时需要 scale down oldRSs 了，scale down 的数量是通过以下公式得到的： // 13 = 10 + 3 allPodsCount := newRS(10) + oldRS(3) // ??? newRSUnavailablePodCount := *(newRS.Spec.Replicas) - newRS.Status.AvailableReplicas // 13 - 8 - ??? maxScaledDown := allPodsCount - minAvailable - newRSUnavailablePodCount newRSUnavailablePodCount 此时不确定，但是值在 [0,3] 中，此时假设 newRS 的三个 pod 还处于 containerCreating 状态，则newRSUnavailablePodCount 为 3，根据以上公式计算所知 maxScaledDown 为 2。如果有个新版本pod已经ready，则maxScaledDown 为 4。 特殊情况，当只有一个副本，maxSurge与maxUnavaiable都为1时，按照以上公式，先扩容1个新版pod，再缩一个旧版的，如果旧版已经删除了而新版还没有起来可能会丟流量，可以将maxUnavaiable设置为0可避免以上情况。 ","date":"Jun 19, 2020","objectID":"/k8s-graceful-update-app/:3:0","series":null,"tags":["k8s","ingress","nginx"],"title":"k8s如何优雅升级应用","uri":"/k8s-graceful-update-app/#升级策略"},{"categories":["cloud"],"content":" 实验验证滚动升级终于也是通过扩缩新旧版本来实现的，我们只需要分析扩缩容过程中会不会丢流量即可。 ","date":"Jun 19, 2020","objectID":"/k8s-graceful-update-app/:4:0","series":null,"tags":["k8s","ingress","nginx"],"title":"k8s如何优雅升级应用","uri":"/k8s-graceful-update-app/#实验验证"},{"categories":["cloud"],"content":" 实验环境image: nginx tool: wrk -c 2 -d 120 -H \"Connection:Close\" http://my.nginx.svc ","date":"Jun 19, 2020","objectID":"/k8s-graceful-update-app/:4:1","series":null,"tags":["k8s","ingress","nginx"],"title":"k8s如何优雅升级应用","uri":"/k8s-graceful-update-app/#实验环境"},{"categories":["cloud"],"content":" 扩容 从1扩到10个 不丢流量，nginx启动很快不需要额外的初始化工作，正常情况需要配置健康检查 ","date":"Jun 19, 2020","objectID":"/k8s-graceful-update-app/:4:2","series":null,"tags":["k8s","ingress","nginx"],"title":"k8s如何优雅升级应用","uri":"/k8s-graceful-update-app/#扩容"},{"categories":["cloud"],"content":" 缩容1) 10 → 1 缩容时会有502错误 Running 2m test @ http://my.nginx.svc 2 threads and 2 connections Thread Stats Avg Stdev Max +/- Stdev Latency 11.73ms 27.02ms 229.17ms 95.14% Req/Sec 162.91 45.77 232.00 74.13% 8969 requests in 28.24s, 2.40MB read Non-2xx or 3xx responses: 366 Requests/sec: 317.62 Transfer/sec: 86.93KB 查看ingress日志 2020/06/19 08:12:28 [error] 9533#9533: *197916788 connect() failed (111: Connection refused) while connecting to upstream, client: 10.232.41.102, server: my.nginx.svc, request: \"GET / HTTP/1.1\", upstream: \"http://10.126.110.3:80/\", host: \"my.nginx.svc\" 2020/06/19 08:12:33 [error] 8935#8935: *197916707 upstream timed out (110: Operation timed out) while connecting to upstream, client: 10.232.41.102, server: my.nginx.svc, request: \"GET / HTTP/1.1\", upstream: \"http://10.126.69.136:80/\", host: \"my.nginx.svc\" 2020/06/19 08:12:33 [error] 9533#9533: *197916788 upstream timed out (110: Operation timed out) while connecting to upstream, client: 10.232.41.102, server: my.nginx.svc, request: \"GET / HTTP/1.1\", upstream: \"http://10.126.69.136:80/\", host: \"my.nginx.svc 10.232.41.102 - - [18/Jun/2020:09:14:35 +0000] \"GET / HTTP/1.1\" 502 157 \"-\" \"-\" 38 0.001 [default-my-nginx-80] [] 10.46.12.80:80, 10.46.12.79:80, 10.46.12.80:80 0, 0, 0 0.000, 0.000, 0.000 502, 502, 502 5cfc063dbe7daf1db953a0e16891f100 2) 4→1 会丟流量 3）3→1 测试多次，偶现过丢流量的情况，这与ingress重试算法有关系 4） 10→1, 忽略term信号, 不丢流量 Running 2m test @ http://my.nginx.svc 2 threads and 2 connections Thread Stats Avg Stdev Max +/- Stdev Latency 12.12ms 16.66ms 214.89ms 88.39% Req/Sec 129.75 74.05 250.00 62.35% 8811 requests in 34.24s, 2.35MB read Requests/sec: 257.35 Transfer/sec: 70.41KB ","date":"Jun 19, 2020","objectID":"/k8s-graceful-update-app/:4:3","series":null,"tags":["k8s","ingress","nginx"],"title":"k8s如何优雅升级应用","uri":"/k8s-graceful-update-app/#缩容"},{"categories":["cloud"],"content":" 总结通过分析及实验，在pod启动时可配置健康检查避免请求异常；同一时刻大于2个pod终止可能会丢失流量，通过监听退出信号可避免此种情况。综上，应用的优化升级需要做到以下几点： 健康检测，pod ready时能够正常接受流量 优雅停止，保证处理完请求再退出，在这段时间内实例ip可从ingress后端摘除 滚动升级配置，若只有1个实例需设置maxsurge=0，更建议副本数设置多个 ","date":"Jun 19, 2020","objectID":"/k8s-graceful-update-app/:5:0","series":null,"tags":["k8s","ingress","nginx"],"title":"k8s如何优雅升级应用","uri":"/k8s-graceful-update-app/#总结"},{"categories":["cloud"],"content":"一般情况下，经过ingress的请求会携带headerX-Real-IP，用户可根据header解析出真实访问IP。 特殊情况，用户请求可能经过多个nginx才达到ingress, 通过上述方法得到的并不是用户的真实IP。 request -\u003e nginx -\u003e … -\u003e ingress-nginx -\u003e backend ","date":"Jun 05, 2020","objectID":"/ingress-real-ip/:0:0","series":null,"tags":["ingress","nginx","k8s"],"title":"Ingress获取真实IP","uri":"/ingress-real-ip/#"},{"categories":["cloud"],"content":" 方案1 use-forwarded-headersnginx-ingress官方的建议是开启use-forwarded-headers, 配置如下： kind: ConfigMap apiVersion: v1 metadata: name: nginx-configuration data: compute-full-forwarded-for: 'true' use-forwarded-headers: 'true' ","date":"Jun 05, 2020","objectID":"/ingress-real-ip/:1:0","series":null,"tags":["ingress","nginx","k8s"],"title":"Ingress获取真实IP","uri":"/ingress-real-ip/#方案1-use-forwarded-headers"},{"categories":["cloud"],"content":" 方案2 real_ip_header这种方式确实可以起作用，但是有用户反馈开启后访问ingres后端服务一直报308，检查了ingress的代码开启use-forwarded-headers后会同时开启ssl-redirect导致308。 那么我们只需要开启nginx配置中的相关real-ip的配置，如下在http-snippet添加real_ip_header X-Forwarded-For; kind: ConfigMap apiVersion: v1 metadata: name: nginx-configuration data: http-snippet: | real_ip_header X-Forwarded-For; ","date":"Jun 05, 2020","objectID":"/ingress-real-ip/:2:0","series":null,"tags":["ingress","nginx","k8s"],"title":"Ingress获取真实IP","uri":"/ingress-real-ip/#方案2-real_ip_header"},{"categories":["cloud"],"content":" golang中获取真实ipfunc RemoteIP(r *http.Request) string { // ingress 行为，将真实ip放到header `X-Original-Forwarded-For`, 普通nginx可去掉此条 ip := strings.TrimSpace(strings.Split(r.Header.Get(\"X-Original-Forwarded-For\"), \",\")[0]) if ip != \"\" { return ip } ip = strings.TrimSpace(strings.Split(r.Header.Get(\"X-Forwarded-For\"), \",\")[0]) if ip != \"\" { return ip } ip = strings.TrimSpace(r.Header.Get(\"X-Real-Ip\")) if ip != \"\" { return ip } if ip, _, err := net.SplitHostPort(strings.TrimSpace(r.RemoteAddr)); err == nil { return ip } return \"\" } ","date":"Jun 05, 2020","objectID":"/ingress-real-ip/:3:0","series":null,"tags":["ingress","nginx","k8s"],"title":"Ingress获取真实IP","uri":"/ingress-real-ip/#golang中获取真实ip"},{"categories":["cloud"],"content":" 注意nginx-ingress configmap中的配置会是全局生效的，上线前需要严格测试。 ","date":"Jun 05, 2020","objectID":"/ingress-real-ip/:4:0","series":null,"tags":["ingress","nginx","k8s"],"title":"Ingress获取真实IP","uri":"/ingress-real-ip/#注意"},{"categories":["cloud"],"content":"线上遇到多次由ingress header过大引起的请求失败, 可能返回502/400，解决方案如下。 ","date":"Jun 05, 2020","objectID":"/ingress-header-too-large/:0:0","series":null,"tags":["ingress","nginx","k8s"],"title":"Ingress Header Too Large","uri":"/ingress-header-too-large/#"},{"categories":["cloud"],"content":" 502 – too big header502错误一般是后端服务不可用，但这里是nginx-ingress返回的，在nginx-ingress可看到如下日志： upstream sent too big header while reading response header from upstream, client... 需要在ingress配置如下参数 apiVersion: extensions/v1beta1 kind: Ingress metadata: annotations: nginx.ingress.kubernetes.io/proxy-buffer-size: 128k #根据实际情况配置 nginx.ingress.kubernetes.io/proxy-buffering: \"on\" nginx.ingress.kubernetes.io/server-snippet: | large_client_header_buffers 16 128K; client_header_buffer_size 128k; ","date":"Jun 05, 2020","objectID":"/ingress-header-too-large/:1:0","series":null,"tags":["ingress","nginx","k8s"],"title":"Ingress Header Too Large","uri":"/ingress-header-too-large/#502--too-big-header"},{"categories":["cloud"],"content":" 431/400 – too big headerhttp header过大也有可能返回400/431, 可按照上述调整，如果还是有问题需要检查后端服务的header设置，比如golang http header默认是1M; springboot应用需要在application.properties加上server.max-http-header-size=32KB等 ","date":"Jun 05, 2020","objectID":"/ingress-header-too-large/:2:0","series":null,"tags":["ingress","nginx","k8s"],"title":"Ingress Header Too Large","uri":"/ingress-header-too-large/#431400--too-big-header"},{"categories":["cloud"],"content":" 413 – too large body如果返回413，则超过了body size的限制（默认1M）, 可在ingress annotation添加 nginx.ingress.kubernetes.io/proxy-body-size: 8m ","date":"Jun 05, 2020","objectID":"/ingress-header-too-large/:3:0","series":null,"tags":["ingress","nginx","k8s"],"title":"Ingress Header Too Large","uri":"/ingress-header-too-large/#413--too-large-body"},{"categories":["cloud"],"content":"Ingress是目前Kubernetes集群流量接入的重要入口，了解其性能指标有助于用户选用合适的网络方案。 ","date":"May 21, 2020","objectID":"/ingress-benchmark/:0:0","series":null,"tags":["k8s","ingress","nginx"],"title":"ingress nginx benchmark","uri":"/ingress-benchmark/#"},{"categories":["cloud"],"content":" 测试方案通过wrk压测后端nginx服务，对比ingress-nginx, 原生nginx，以及直连后端性能的差异，如下图: 方案1，经过ingress 方案2，经过nginx 方案3，直连ip ","date":"May 21, 2020","objectID":"/ingress-benchmark/:1:0","series":null,"tags":["k8s","ingress","nginx"],"title":"ingress nginx benchmark","uri":"/ingress-benchmark/#测试方案"},{"categories":["cloud"],"content":" 硬件环境 CPU： 2x Intel(R) Xeon(R) CPU E5-2620 v4 @ 2.10GHz, 32 cores Network： 10-Gigabit Memory： 128 GB ","date":"May 21, 2020","objectID":"/ingress-benchmark/:1:1","series":null,"tags":["k8s","ingress","nginx"],"title":"ingress nginx benchmark","uri":"/ingress-benchmark/#硬件环境"},{"categories":["cloud"],"content":" 测试工具 wrk, 4.1.0, 在k8s master测试，减少网络影响 ingress-nginx, 0.30.0, https://github.com/kubernetes/ingress-nginx nginx, 1.13.5 k8s, v1.14.9 centos, 7.3.1611(Linux 4.9.2) ","date":"May 21, 2020","objectID":"/ingress-benchmark/:1:2","series":null,"tags":["k8s","ingress","nginx"],"title":"ingress nginx benchmark","uri":"/ingress-benchmark/#测试工具"},{"categories":["cloud"],"content":" 测试方法ingress-nginx主要工作是转发请求到后端pod, 我们着重对其RPS（每秒请求量）进行测试 通过以下命令 wrk -t4 -c1000 -d120s --latency http://my.nginx.svc/1kb.bin ","date":"May 21, 2020","objectID":"/ingress-benchmark/:1:3","series":null,"tags":["k8s","ingress","nginx"],"title":"ingress nginx benchmark","uri":"/ingress-benchmark/#测试方法"},{"categories":["cloud"],"content":" 测试结果","date":"May 21, 2020","objectID":"/ingress-benchmark/:2:0","series":null,"tags":["k8s","ingress","nginx"],"title":"ingress nginx benchmark","uri":"/ingress-benchmark/#测试结果"},{"categories":["cloud"],"content":" 不同cpu下的性能对比不同ingress-nginx启动不同worker数量的性能差异，以下测试ingress-nginx开启了keepalive等特性 CPU RPS 1 5534 2 11203 4 22890 8 47025 16 93644 24 125990 32 153473 如图所示，不同cpu下，ingress的rps与cpu成正比，cpu在16核之后增长趋势放缓。 ","date":"May 21, 2020","objectID":"/ingress-benchmark/:2:1","series":null,"tags":["k8s","ingress","nginx"],"title":"ingress nginx benchmark","uri":"/ingress-benchmark/#不同cpu下的性能"},{"categories":["cloud"],"content":" 不同方案的性能对比 方案 RPS 备注 ingress-nginx(原始) 69171 ingress-nginx(配置优化) 153473 调整worker，access-log, keepalive等 nginx 336769 开启keepalive, 关闭log 直连ip 340748 测试中的pod ip为真实ip 通过实验可以看到，使用nginx代理和直连ip，rps相差不大；原始ingress-nginx rps很低，优化后rps提升一倍，但对比nginx还是有较大的性能差异。 ","date":"May 21, 2020","objectID":"/ingress-benchmark/:2:2","series":null,"tags":["k8s","ingress","nginx"],"title":"ingress nginx benchmark","uri":"/ingress-benchmark/#不同方案的性能对比"},{"categories":["cloud"],"content":" 结论默认ingress-nginx性能较差，配置优化后也只有15w RPS，对比原生nginx（33W) 差距较大。经过分析主要瓶颈在于ingress-nginx的lua过滤脚本，具体原因需要进一步分析。 ","date":"May 21, 2020","objectID":"/ingress-benchmark/:3:0","series":null,"tags":["k8s","ingress","nginx"],"title":"ingress nginx benchmark","uri":"/ingress-benchmark/#结论"},{"categories":["cloud"],"content":" 参考 https://kubernetes.github.io/ingress-nginx/user-guide/nginx-configuration/configmap/#upstream-keepalive-connections https://www.nginx.com/blog/testing-performance-nginx-ingress-controller-kubernetes/ ","date":"May 21, 2020","objectID":"/ingress-benchmark/:4:0","series":null,"tags":["k8s","ingress","nginx"],"title":"ingress nginx benchmark","uri":"/ingress-benchmark/#参考"},{"categories":["cloud"],"content":" 配置文件本测试所有配置见qingwave/ingress-nginx-benchmark ","date":"May 21, 2020","objectID":"/ingress-benchmark/:5:0","series":null,"tags":["k8s","ingress","nginx"],"title":"ingress nginx benchmark","uri":"/ingress-benchmark/#配置文件"},{"categories":["cloud"],"content":"为了避免广告法，题目还是加个可能吧。 想要安全就必须复杂起来，证书是少不了的。在Kubernetes中提供了非常丰富的证书类型，满足各种不同场景的需求，今天我们就来看一看Kubernetes中的证书。 ","date":"Apr 25, 2020","objectID":"/k8s-tls/:0:0","series":null,"tags":["k8s","tls","apiserver"],"title":"可能是史上最全的Kubernetes证书解析","uri":"/k8s-tls/#"},{"categories":["cloud"],"content":" k8s证书分类在说证书之前，先想想作为集群的入口apiserver需要提供那些服务，与那些组件通信，通信的两方可能需要配置证书。 与apiserver通信的组件大体可以分为以下几类： client(kubectl，restapi等)：普通用户与apiserver之间的通信，对各类资源进行操作 kubelet，kubeproxy：master与node之间的通信 etcd：k8s的存储库 webhook：这里指apiserver提供的admission-webhook，在数据持久化前调用webhook aggregation layer：扩展apiserver, 需要将自定义的api注册到k8s中，相比CRD性能更新 pod: 在pod中调用apiserver(一般调用为10.254.0.1:433) 居然有这么多种，除了在pod中通过serviceacount认证（当然pod需要认证apiserver的证书），其他几种都需要配置证书。 其他集群内组件与apiserver通信的，kubelet/etcd/kube-proxy对应的也可以配置证书。 ","date":"Apr 25, 2020","objectID":"/k8s-tls/:1:0","series":null,"tags":["k8s","tls","apiserver"],"title":"可能是史上最全的Kubernetes证书解析","uri":"/k8s-tls/#k8s证书分类"},{"categories":["cloud"],"content":" apiserver证书简单列举下apiserver证书相关的启动参数 --cert-dir string The directory where the TLS certs are located. If --tls-cert-file and --tls-private-key-file are provided, this flag will be ignored. (default \"/var/run/kubernetes\") --client-ca-file string If set, any request presenting a client certificate signed by one of the authorities in the client-ca-file is authenticated with an identity corresponding to the CommonName of the client certificate. --etcd-certfile string SSL certification file used to secure etcd communication. --etcd-keyfile string SSL key file used to secure etcd communication. --kubelet-certificate-authority string Path to a cert file for the certificate authority. --kubelet-client-certificate string Path to a client cert file for TLS. --kubelet-client-key string Path to a client key file for TLS. --proxy-client-cert-file string Client certificate used to prove the identity of the aggregator or kube-apiserver when it must call out during a request. This includes proxying requests to a user api-server and calling out to webhook admission plugins. It is expected that this cert includes a signature from the CA in the --requestheader-client-ca-file flag. That CA is published in the 'extension-apiserver-authentication' configmap in the kube-system namespace. Components recieving calls from kube-aggregator should use that CA to perform their half of the mutual TLS verification. --proxy-client-key-file string Private key for the client certificate used to prove the identity of the aggregator or kube-apiserver when it must call out during a request. This includes proxying requests to a user api-server and calling out to webhook admission plugins. --requestheader-allowed-names stringSlice List of client certificate common names to allow to provide usernames in headers specified by --requestheader-username-headers. If empty, any client certificate validated by the authorities in --requestheader-client-ca-file is allowed. --requestheader-client-ca-file string Root certificate bundle to use to verify client certificates on incoming requests before trusting usernames in headers specified by --requestheader-username-headers --service-account-key-file stringArray File containing PEM-encoded x509 RSA or ECDSA private or public keys, used to verify ServiceAccount tokens. If unspecified, --tls-private-key-file is used. The specified file can contain multiple keys, and the flag can be specified multiple times with different files. --ssh-keyfile string If non-empty, use secure SSH proxy to the nodes, using this user keyfile --tls-ca-file string If set, this certificate authority will used for secure access from Admission Controllers. This must be a valid PEM-encoded CA bundle. Alternatively, the certificate authority can be appended to the certificate provided by --tls-cert-file. --tls-cert-file string File containing the default x509 Certificate for HTTPS. (CA cert, if any, concatenated after server cert). If HTTPS serving is enabled, and --tls-cert-file and --tls-private-key-file are not provided, a self-signed certificate and key are generated for the public address and saved to /var/run/kubernetes. --tls-private-key-file string File containing the default x509 private key matching --tls-cert-file. --tls-sni-cert-key namedCertKey A pair of x509 certificate and private key file paths, optionally suffixed with a list of domain patterns which are fully qualified domain names, possibly with prefixed wildcard segments. If no domain patterns are provided, the names of the certificate are extracted. Non-wildcard matches trump over wildcard matches, explicit domain patterns trump over extracted names. For multiple key/certificate pairs, use the --tls-sni-cert-key multiple times. Examples: \"example.crt,example.key\" or \"foo.crt,foo.key:*.foo.com,foo.com\". (default []) --oidc-ca-file string If set, the OpenID server's certificate will be verified by one of the authorities in the oidc-ca-file, otherwise the host's root CA set will be used. --tl","date":"Apr 25, 2020","objectID":"/k8s-tls/:2:0","series":null,"tags":["k8s","tls","apiserver"],"title":"可能是史上最全的Kubernetes证书解析","uri":"/k8s-tls/#apiserver证书"},{"categories":["cloud"],"content":" tls证书首先，apiserver本身是一个http服务器，需要tls证书 --tls-cert-file string File containing the default x509 Certificate for HTTPS. (CA cert, if any, concatenated after server cert). If HTTPS serving is enabled, and --tls-cert-file and --tls-private-key-file are not provided, a self-signed certificate and key are generated for the public address and saved to the directory specified by --cert-dir. --tls-private-key-file string File containing the default x509 private key matching --tls-cert-file. 其他client验证apiserver时可以通过签署这两个证书的CA，我们称为`tls-ca` ","date":"Apr 25, 2020","objectID":"/k8s-tls/:2:1","series":null,"tags":["k8s","tls","apiserver"],"title":"可能是史上最全的Kubernetes证书解析","uri":"/k8s-tls/#tls证书"},{"categories":["cloud"],"content":" client证书apiserver提供了tls证书，同样也需要验证client的配置，但是client太多了(kubectl,各种restapi调用的), 这些client需要统一用一个CA签发，我们称为client-ca。 --client-ca-file string If set, any request presenting a client certificate signed by one of the authorities in the client-ca-file is authenticated with an identity corresponding to the CommonName of the client certificate. 需要注意的是，在apiserver认证中，通过CN和O来识别用户，开启RBAC的用户要配置CN和O做一些授权： CN：Common Name，kube-apiserver 从证书中提取作为请求的用户名 (User Name)；浏览器使用该字段验证网站是否合法； O：Organization，kube-apiserver 从证书中提取该字段作为请求用户所属的组 (Group) 如kube-proxy的证书申请, User为system:kube-proxy, Group为k8s { \"CN\": \"system:kube-proxy\", \"hosts\": [], \"key\": { \"algo\": \"rsa\", \"size\": 2048 }, \"names\": [ { \"C\": \"CN\", \"ST\": \"BeiJing\", \"L\": \"BeiJing\", \"O\": \"k8s\", \"OU\": \"System\" } ] } ","date":"Apr 25, 2020","objectID":"/k8s-tls/:2:2","series":null,"tags":["k8s","tls","apiserver"],"title":"可能是史上最全的Kubernetes证书解析","uri":"/k8s-tls/#client证书"},{"categories":["cloud"],"content":" requestheader证书apiserver可以使用HTTP请求头中的指定字段来进行认证，相关配置如下: --requestheader-allowed-names stringSlice List of client certificate common names to allow to provide usernames in headers specified by --requestheader-username-headers. If empty, any client certificate validated by the authorities in --requestheader-client-ca-file is allowed. --requestheader-client-ca-file string Root certificate bundle to use to verify client certificates on incoming requests before trusting usernames in headers specified by --requestheader-username-headers. WARNING: generally do not depend on authorization being already done for incoming requests. --requestheader-extra-headers-prefix strings List of request header prefixes to inspect. X-Remote-Extra- is suggested. --requestheader-group-headers strings List of request headers to inspect for groups. X-Remote-Group is suggested. --requestheader-username-headers strings List of request headers to inspect for usernames. X-Remote-User is common. 收到请求时，apiserver会首先认证requsetheader-ca，验证成功并且CN在requestheader-allowed-names（默认全部需求）中，然后通过Http header中的X-Remote-User, X-Remote-Group去得到用户；如果匹配不成功回去验证client-ca。 如上，requestheader证书与client-ca不能是同一个。 ","date":"Apr 25, 2020","objectID":"/k8s-tls/:2:3","series":null,"tags":["k8s","tls","apiserver"],"title":"可能是史上最全的Kubernetes证书解析","uri":"/k8s-tls/#requestheader证书"},{"categories":["cloud"],"content":" proxy证书k8s提供了丰富的扩展机制，CRD与[API Aggregation][https://kubernetes.io/zh/docs/tasks/access-kubernetes-api/configure-aggregation-layer/]。 对于API Aggregation(例如metrics-server提供了metrics.k8s.io api), apiserver接受到请求后经过一系列验证过滤，会将请求转发到扩展API，这里apisever作为代理服务器，需要配置配置证书。 --proxy-client-cert-file string Client certificate used to prove the identity of the aggregator or kube-apiserver when it must call out during a request. This includes proxying requests to a user api-server and calling out to webhook admission plugins. It is expected that this cert includes a signature from the CA in the --requestheader-client-ca-file flag. That CA is published in the 'extension-apiserver-authentication' configmap in the kube-system namespace. Components recieving calls from kube-aggregator should use that CA to perform their half of the mutual TLS verification. --proxy-client-key-file string Private key for the client certificate used to prove the identity of the aggregator or kube-apiserver when it must call out during a request. This includes proxying requests to a user api-server and calling out to webhook admission plugins. 需要注意的是对证书需要通过requestheader-ca签发，扩展api会通过requestheader证书去验证，具体流程后面会写一篇，下图为官方提供的流程 ","date":"Apr 25, 2020","objectID":"/k8s-tls/:2:4","series":null,"tags":["k8s","tls","apiserver"],"title":"可能是史上最全的Kubernetes证书解析","uri":"/k8s-tls/#proxy证书"},{"categories":["cloud"],"content":" kubelet证书对于kubelet，apiserver单独提供了证书配置选项，同时kubelet组件也提供了反向设置的相关选项: # API Server --kubelet-certificate-authority string Path to a cert file for the certificate authority. --kubelet-client-certificate string Path to a client cert file for TLS. --kubelet-client-key string Path to a client key file for TLS. # kubelet --client-ca-file string If set, any request presenting a client certificate signed by one of the authorities in the client-ca-file is authenticated with an identity corresponding to the CommonName of the client certificate. --tls-cert-file string File containing x509 Certificate used for serving HTTPS (with intermediate certs, if any, concatenated after server cert). If --tls-cert-file and --tls-private-key-file are not provided, a self-signed certificate and key are generated for the public address and saved to the directory passed to --cert-dir. --tls-private-key-file string File containing x509 private key matching --tls-cert-file. kubelet也是即作为server也作为client, 需要提供tls证书和client-ca, 我们称这个CA为kubelet-ca, 可以是单独的CA。 ","date":"Apr 25, 2020","objectID":"/k8s-tls/:2:5","series":null,"tags":["k8s","tls","apiserver"],"title":"可能是史上最全的Kubernetes证书解析","uri":"/k8s-tls/#kubelet证书"},{"categories":["cloud"],"content":" etcd证书这个也不用多说，用来连接etcd，由etcd-ca签发 --etcd-certfile string SSL certification file used to secure etcd communication. --etcd-keyfile string SSL key file used to secure etcd communication. ","date":"Apr 25, 2020","objectID":"/k8s-tls/:2:6","series":null,"tags":["k8s","tls","apiserver"],"title":"可能是史上最全的Kubernetes证书解析","uri":"/k8s-tls/#etcd证书"},{"categories":["cloud"],"content":" serviceaccount证书在k8s中，通过JWT认证serviecaccount，同样有两个证书配置: # apiserver --service-account-key-file stringArray # 用于验证sa File containing PEM-encoded x509 RSA or ECDSA private or public keys, used to verify ServiceAccount tokens. The specified file can contain multiple keys, and the flag can be specified multiple times with different files. If unspecified, --tls-private-key-file is used. Must be specified when --service-account-signing-key is provided --service-account-signing-key-file string Path to the file that contains the current private key of the service account token issuer. The issuer will sign issued ID tokens with this private key. (Requires the 'TokenRequest' feature gate.) # controller-manager –service-account-private-key-file #用于签署sa 这两个配置描述了对serviceaccount进行签名验证时所使用的证书；可以是单独的生成，我们称为sa-key。 ","date":"Apr 25, 2020","objectID":"/k8s-tls/:2:7","series":null,"tags":["k8s","tls","apiserver"],"title":"可能是史上最全的Kubernetes证书解析","uri":"/k8s-tls/#serviceaccount证书"},{"categories":["cloud"],"content":" 其他证书其他还有oidc证书，用于OpenID认证；ssh证书，用来连接node，目前以及废弃。 etcd与kubelet证书上面已经提过了，需要双方都配置。 k8s中也支持证书申请，用户可以创建CertificateSigningRequest来申请证书，需要在controller-manager配置下面的证书，用于签发证书称为sing-ca，多用于webhook的证书配置。 --cluster-signing-cert-file string Filename containing a PEM-encoded X509 CA certificate used to issue cluster-scoped certificates (default \"/etc/kubernetes/ca/ca.pem\") --cluster-signing-key-file string Filename containing a PEM-encoded RSA or ECDSA private key used to sign cluster-scoped certificates (default \"/etc/kubernetes/ca/ca.key\") ","date":"Apr 25, 2020","objectID":"/k8s-tls/:3:0","series":null,"tags":["k8s","tls","apiserver"],"title":"可能是史上最全的Kubernetes证书解析","uri":"/k8s-tls/#其他证书"},{"categories":["cloud"],"content":" 总结k8s提供了强大的功能，需要考虑到各个场景的安全问题，上面我们梳理了遍目前常用的证书 tls-ca client-ca requestheader-ca proxy-ca kubelet-ca etcd-ca sa-key sign-ca 上面除了proxy-ca必须使用requestheader-ca签发，其他所有的都可以是单独的CA，可以根据安全性评估是使用一个CA还是多个CA，我们建议下面的CA尽量是独立的 client-ca requestheader-ca etcd-ca kubelet-ca sign-ca 终于理完了，可以起床啦。 ","date":"Apr 25, 2020","objectID":"/k8s-tls/:4:0","series":null,"tags":["k8s","tls","apiserver"],"title":"可能是史上最全的Kubernetes证书解析","uri":"/k8s-tls/#总结"},{"categories":["cloud"],"content":"kube-apiserver 共由 3 个组件构成（Aggregator. KubeAPIServer. APIExtensionServer），这些组件依次通过 Delegation 处理请求： Aggregator：暴露的功能类似于一个七层负载均衡，将来自用户的请求拦截转发给其他服务器，并且负责整个 APIServer 的 Discovery 功能；也负责处理ApiService，注册对应的扩展api。 KubeAPIServer ：负责对请求的一些通用处理，认证. 鉴权等，以及处理各个内建资源的 REST 服务； APIExtensionServer：主要处理 CustomResourceDefinition（CRD）和 CustomResource（CR）的 REST 请求，也是 Delegation 的最后一环，如果对应 CR 不能被处理的话则会返回 404。 ","date":"Apr 24, 2020","objectID":"/kube-apiserver-start/:0:0","series":null,"tags":["k8s","apiserver"],"title":"kube-apiserver启动流程分析","uri":"/kube-apiserver-start/#"},{"categories":["cloud"],"content":" kube-apiserver启动流程Apiserver通过Run方法启动, 主要逻辑为： 调用CreateServerChain构建服务调用链并判断是否启动非安全的httpserver，httpserver链中包含 apiserver要启动的三个server，以及为每个server注册对应资源的路由； 调用server.PrepareRun进行服务运行前的准备，该方法主要完成了健康检查. 存活检查和OpenAPI路由的注册工作； 调用prepared.Run启动server； // Run runs the specified APIServer. This should never exit. func Run(completeOptions completedServerRunOptions, stopCh \u003c-chan struct{}) error { // To help debugging, immediately log version klog.Infof(\"Version: %+v\", version.Get()) // 创建调用链 server, err := CreateServerChain(completeOptions, stopCh) if err != nil { return err } // 进行一些准备工作， 注册一些hander，执行hook等 prepared, err := server.PrepareRun() if err != nil { return err } // 开始执行 return prepared.Run(stopCh) } 执行具体的Run方法 // Run spawns the secure http server. It only returns if stopCh is closed // or the secure port cannot be listened on initially. func (s preparedGenericAPIServer) Run(stopCh \u003c-chan struct{}) error { delayedStopCh := make(chan struct{}) go func() { defer close(delayedStopCh) \u003c-stopCh // As soon as shutdown is initiated, /readyz should start returning failure. // This gives the load balancer a window defined by ShutdownDelayDuration to detect that /readyz is red // and stop sending traffic to this server. // 当终止时，关闭readiness close(s.readinessStopCh) time.Sleep(s.ShutdownDelayDuration) }() // 执行非阻塞Run // close socket after delayed stopCh err := s.NonBlockingRun(delayedStopCh) if err != nil { return err } \u003c-stopCh // run shutdown hooks directly. This includes deregistering from the kubernetes endpoint in case of kube-apiserver. // 关闭前执行一些hook操作 err = s.RunPreShutdownHooks() if err != nil { return err } // wait for the delayed stopCh before closing the handler chain (it rejects everything after Wait has been called). \u003c-delayedStopCh // 等待所有请求执行完 // Wait for all requests to finish, which are bounded by the RequestTimeout variable. s.HandlerChainWaitGroup.Wait() return nil } 执行NonBlockingRun k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/server/genericapiserver.go:351 func (s preparedGenericAPIServer) NonBlockingRun(stopCh \u003c-chan struct{}) error { auditStopCh := make(chan struct{}) // 1. 判断是否要启动审计日志 if s.AuditBackend != nil { if err := s.AuditBackend.Run(auditStopCh); err != nil { return fmt.Errorf(\"failed to run the audit backend: %v\", err) } } // 2. 启动 https server internalStopCh := make(chan struct{}) var stoppedCh \u003c-chan struct{} if s.SecureServingInfo != nil \u0026\u0026 s.Handler != nil { var err error stoppedCh, err = s.SecureServingInfo.Serve(s.Handler, s.ShutdownTimeout, internalStopCh) if err != nil { close(internalStopCh) close(auditStopCh) return err } } go func() { \u003c-stopCh close(s.readinessStopCh) close(internalStopCh) if stoppedCh != nil { \u003c-stoppedCh } s.HandlerChainWaitGroup.Wait() close(auditStopCh) }() // 3. 执行 postStartHooks s.RunPostStartHooks(stopCh) // 4. 向 systemd 发送 ready 信号 if _, err := systemd.SdNotify(true, \"READY=1\\n\"); err != nil { klog.Errorf(\"Unable to send systemd daemon successful start message: %v\\n\", err) } return nil } ","date":"Apr 24, 2020","objectID":"/kube-apiserver-start/:1:0","series":null,"tags":["k8s","apiserver"],"title":"kube-apiserver启动流程分析","uri":"/kube-apiserver-start/#kube-apiserver启动流程"},{"categories":["cloud"],"content":" 调用链分析上一节简单分析了Apiserver的启动流程，通过初始化各种配置，封装调用链，启动Server。这节主要分析调用链。 初始化阶段, 通过CreateServerChain创建调用链， 代码在server.go // CreateServerChain creates the apiservers connected via delegation. func CreateServerChain(completedOptions completedServerRunOptions, stopCh \u003c-chan struct{}) (*aggregatorapiserver.APIAggregator, error) { // nodetunneler与node通信，proxy实现代理功能，转发请求给其他apiservice // apiserver到cluster的通信可以通过三种方法 // apiserver到kubelet的endpoint，用于logs功能，exec功能，port-forward功能 // HTTP连接，即使可以用HTTPS也不做任何其他校验，并不安全 // ssh tunnel，不推荐使用 nodeTunneler, proxyTransport, err := CreateNodeDialer(completedOptions) // 1. 为 kubeAPIServer 创建配置 kubeAPIServerConfig, insecureServingInfo, serviceResolver, pluginInitializer, admissionPostStartHook, err := CreateKubeAPIServerConfig(completedOptions, nodeTunneler, proxyTransport) if err != nil { return nil, err } // 2. 判断是否配置了 APIExtensionsServer，创建 apiExtensionsConfig apiExtensionsConfig, err := createAPIExtensionsConfig(*kubeAPIServerConfig.GenericConfig, kubeAPIServerConfig.ExtraConfig.VersionedInformers, pluginInitializer, completedOptions.ServerRunOptions, completedOptions.MasterCount,vc serviceResolver, webhook.NewDefaultAuthenticationInfoResolverWrapper(proxyTransport, kubeAPIServerConfig.GenericConfig.LoopbackClientConfig)) if err != nil { return nil, err } // 3. 初始化 APIExtensionsServer, 通过一个空的delegate初始化 apiExtensionsServer, err := createAPIExtensionsServer(apiExtensionsConfig, genericapiserver.NewEmptyDelegate()) if err != nil { return nil, err } // 4. 初始化 KubeAPIServer kubeAPIServer, err := CreateKubeAPIServer(kubeAPIServerConfig, apiExtensionsServer.GenericAPIServer, admissionPostStartHook) if err != nil { return nil, err } // 5. 创建 AggregatorConfig aggregatorConfig, err := createAggregatorConfig(*kubeAPIServerConfig.GenericConfig, completedOptions.ServerRunOptions, kubeAPIServerConfig. ExtraConfig.VersionedInformers, serviceResolver, proxyTransport, pluginInitializer) if err != nil { return nil, err } // 6. 初始化 AggregatorServer aggregatorServer, err := createAggregatorServer(aggregatorConfig, kubeAPIServer.GenericAPIServer, apiExtensionsServer.Informers) if err != nil { return nil, err } // 7. 判断是否启动非安全端口的 http server if insecureServingInfo != nil { insecureHandlerChain := kubeserver.BuildInsecureHandlerChain(aggregatorServer.GenericAPIServer.UnprotectedHandler(), kubeAPIServerConfig.GenericConfig) if err := insecureServingInfo.Serve(insecureHandlerChain, kubeAPIServerConfig.GenericConfig.RequestTimeout, stopCh); err != nil { return nil, err } } return aggregatorServer, nil } 创建过程主要有以下步骤： 根据配置构造apiserver的配置，调用方法CreateKubeAPIServerConfig 根据配置构造扩展的apiserver的配置，调用方法为createAPIExtensionsConfig 创建server，包括扩展的apiserver和原生的apiserver，调用方法为createAPIExtensionsServer和CreateKubeAPIServer。主要就是将各个handler的路由方法注册到Container中去，完全遵循go-restful的设计模式，即将处理方法注册到Route中去，同一个根路径下的Route注册到WebService中去，WebService注册到Container中，Container负责分发。访问的过程为Container--\u003eWebService--\u003eRoute 聚合server的配置和和创建。主要就是将原生的apiserver和扩展的apiserver的访问进行整合，添加后续的一些处理接口。调用方法为createAggregatorConfig和createAggregatorServer 创建完成，返回配置的server信息 以上几个步骤，最核心的就是apiserver如何创建，即如何按照go-restful的模式，添加路由和相应的处理方法。 ","date":"Apr 24, 2020","objectID":"/kube-apiserver-start/:2:0","series":null,"tags":["k8s","apiserver"],"title":"kube-apiserver启动流程分析","uri":"/kube-apiserver-start/#调用链分析"},{"categories":["cloud"],"content":" 配置初始化先看apiserver配置的创建CreateKubeAPIServerConfig-\u003ebuildGenericConfig-\u003egenericapiserver.NewConfig // BuildGenericConfig takes the master server options and produces the genericapiserver.Config associated with it func buildGenericConfig( s *options.ServerRunOptions, proxyTransport *http.Transport, ) ( genericConfig *genericapiserver.Config, versionedInformers clientgoinformers.SharedInformerFactory, insecureServingInfo *genericapiserver.DeprecatedInsecureServingInfo, serviceResolver aggregatorapiserver.ServiceResolver, pluginInitializers []admission.PluginInitializer, admissionPostStartHook genericapiserver.PostStartHookFunc, storageFactory *serverstorage.DefaultStorageFactory, lastErr error, ) { // 创建genericConfig,其中包括DefaultBuildHandlerChain，一系列认证授权的中间件 genericConfig = genericapiserver.NewConfig(legacyscheme.Codecs) genericConfig.MergedResourceConfig = master.DefaultAPIResourceConfigSource() // 初始化各种配置 if lastErr = s.GenericServerRunOptions.ApplyTo(genericConfig); lastErr != nil { return } // ... genericConfig.OpenAPIConfig = genericapiserver.DefaultOpenAPIConfig(generatedopenapi.GetOpenAPIDefinitions, openapinamer.NewDefinitionNamer(legacyscheme.Scheme, extensionsapiserver.Scheme, aggregatorscheme.Scheme)) genericConfig.OpenAPIConfig.Info.Title = \"Kubernetes\" // 长连接请求 genericConfig.LongRunningFunc = filters.BasicLongRunningRequestCheck( sets.NewString(\"watch\", \"proxy\"), sets.NewString(\"attach\", \"exec\", \"proxy\", \"log\", \"portforward\"), ) kubeVersion := version.Get() genericConfig.Version = \u0026kubeVersion // 初始化storageFactory， 用来连接etcd storageFactoryConfig := kubeapiserver.NewStorageFactoryConfig() storageFactoryConfig.APIResourceConfig = genericConfig.MergedResourceConfig completedStorageFactoryConfig, err := storageFactoryConfig.Complete(s.Etcd) if err != nil { lastErr = err return } storageFactory, lastErr = completedStorageFactoryConfig.New() if lastErr != nil { return } if genericConfig.EgressSelector != nil { storageFactory.StorageConfig.Transport.EgressLookup = genericConfig.EgressSelector.Lookup } if lastErr = s.Etcd.ApplyWithStorageFactoryTo(storageFactory, genericConfig); lastErr != nil { return } // Use protobufs for self-communication. // Since not every generic apiserver has to support protobufs, we // cannot default to it in generic apiserver and need to explicitly // set it in kube-apiserver. // 内部使用protobufs通信 genericConfig.LoopbackClientConfig.ContentConfig.ContentType = \"application/vnd.kubernetes.protobuf\" // Disable compression for self-communication, since we are going to be // on a fast local network genericConfig.LoopbackClientConfig.DisableCompression = true // clientset初始化 kubeClientConfig := genericConfig.LoopbackClientConfig clientgoExternalClient, err := clientgoclientset.NewForConfig(kubeClientConfig) if err != nil { lastErr = fmt.Errorf(\"failed to create real external clientset: %v\", err) return } versionedInformers = clientgoinformers.NewSharedInformerFactory(clientgoExternalClient, 10*time.Minute) // 初始化认证实例，支持多种认证方式：requestheader,token, tls等 genericConfig.Authentication.Authenticator, genericConfig.OpenAPIConfig.SecurityDefinitions, err = BuildAuthenticator(s, genericConfig.EgressSelector, clientgoExternalClient, versionedInformers) if err != nil { lastErr = fmt.Errorf(\"invalid authentication config: %v\", err) return } // 初始化鉴权配置 genericConfig.Authorization.Authorizer, genericConfig.RuleResolver, err = BuildAuthorizer(s, genericConfig.EgressSelector, versionedInformers) if err != nil { lastErr = fmt.Errorf(\"invalid authorization config: %v\", err) return } if !sets.NewString(s.Authorization.Modes...).Has(modes.ModeRBAC) { genericConfig.DisabledPostStartHooks.Insert(rbacrest.PostStartHookName) } // 初始化admission webhook的配置 admissionConfig := \u0026kubeapiserveradmission.Config{ ExternalInformers: versionedInformers, LoopbackClientConfig: genericConfig.LoopbackClientConfig, CloudConfigFile: s.CloudProvider.CloudConfigFile, } serviceResolver = buildServiceResolver(s.EnableAggregatorRouting, genericConfig.Loop","date":"Apr 24, 2020","objectID":"/kube-apiserver-start/:2:1","series":null,"tags":["k8s","apiserver"],"title":"kube-apiserver启动流程分析","uri":"/kube-apiserver-start/#配置初始化"},{"categories":["cloud"],"content":" APIExtensionsServer初始化APIExtensionsServer最先初始化，在调用链的末尾, 处理CR、CRD相关资源. 其中包含的 controller 以及功能如下所示： openapiController：将 crd 资源的变化同步至提供的 OpenAPI 文档，可通过访问 /openapi/v2 进行查看； crdController：负责将 crd 信息注册到 apiVersions 和 apiResources 中，两者的信息可通过 $ kubectl api-versions 和 $ kubectl api-resources 查看； namingController：检查 crd obj 中是否有命名冲突，可在 crd .status.conditions 中查看； establishingController：检查 crd 是否处于正常状态，可在 crd .status.conditions 中查看； nonStructuralSchemaController：检查 crd obj 结构是否正常，可在 crd .status.conditions 中查看； apiApprovalController：检查 crd 是否遵循 kubernetes API 声明策略，可在 crd .status.conditions 中查看； finalizingController：类似于 finalizes 的功能，与 CRs 的删除有关； createAPIExtensionsServer调用apiextensionsConfig.Complete().New(delegateAPIServer) k8s.io/kubernetes/staging/src/k8s.io/apiextensions-apiserver/pkg/apiserver/apiserver.go:132 / New returns a new instance of CustomResourceDefinitions from the given config. func (c completedConfig) New(delegationTarget genericapiserver.DelegationTarget) (*CustomResourceDefinitions, error) { // 初始化 genericServer genericServer, err := c.GenericConfig.New(\"apiextensions-apiserver\", delegationTarget) if err != nil { return nil, err } s := \u0026CustomResourceDefinitions{ GenericAPIServer: genericServer, } // 初始化apigroup, 即需要暴露的api，这里extension apiserver只注册了cr于crd相关的 apiResourceConfig := c.GenericConfig.MergedResourceConfig apiGroupInfo := genericapiserver.NewDefaultAPIGroupInfo(apiextensions.GroupName, Scheme, metav1.ParameterCodec, Codecs) if apiResourceConfig.VersionEnabled(v1beta1.SchemeGroupVersion) { storage := map[string]rest.Storage{} // customresourcedefinitions customResourceDefintionStorage := customresourcedefinition.NewREST(Scheme, c.GenericConfig.RESTOptionsGetter) storage[\"customresourcedefinitions\"] = customResourceDefintionStorage storage[\"customresourcedefinitions/status\"] = customresourcedefinition.NewStatusREST(Scheme, customResourceDefintionStorage) apiGroupInfo.VersionedResourcesStorageMap[v1beta1.SchemeGroupVersion.Version] = storage } if apiResourceConfig.VersionEnabled(v1.SchemeGroupVersion) { storage := map[string]rest.Storage{} // customresourcedefinitions customResourceDefintionStorage := customresourcedefinition.NewREST(Scheme, c.GenericConfig.RESTOptionsGetter) storage[\"customresourcedefinitions\"] = customResourceDefintionStorage storage[\"customresourcedefinitions/status\"] = customresourcedefinition.NewStatusREST(Scheme, customResourceDefintionStorage) apiGroupInfo.VersionedResourcesStorageMap[v1.SchemeGroupVersion.Version] = storage } // 注册apigroup if err := s.GenericAPIServer.InstallAPIGroup(\u0026apiGroupInfo); err != nil { return nil, err } // clientset创建 crdClient, err := clientset.NewForConfig(s.GenericAPIServer.LoopbackClientConfig) if err != nil { // it's really bad that this is leaking here, but until we can fix the test (which I'm pretty sure isn't even testing what it wants to test), // we need to be able to move forward return nil, fmt.Errorf(\"failed to create clientset: %v\", err) } s.Informers = externalinformers.NewSharedInformerFactory(crdClient, 5*time.Minute) // 创建各种handler delegateHandler := delegationTarget.UnprotectedHandler() if delegateHandler == nil { delegateHandler = http.NotFoundHandler() } versionDiscoveryHandler := \u0026versionDiscoveryHandler{ discovery: map[schema.GroupVersion]*discovery.APIVersionHandler{}, delegate: delegateHandler, } groupDiscoveryHandler := \u0026groupDiscoveryHandler{ discovery: map[string]*discovery.APIGroupHandler{}, delegate: delegateHandler, } establishingController := establish.NewEstablishingController(s.Informers.Apiextensions().V1().CustomResourceDefinitions(), crdClient.ApiextensionsV1()) crdHandler, err := NewCustomResourceDefinitionHandler( versionDiscoveryHandler, groupDiscoveryHandler, s.Informers.Apiextensions().V1().CustomResourceDefinitions(), delegateHandler, c.ExtraConfig.CRDRESTOptionsGetter, c.GenericConfig.AdmissionControl, establishingController, c.ExtraConfig.ServiceResolver, c.ExtraConfig.AuthResolverWrapper, c.ExtraConfig.MasterCount, s.","date":"Apr 24, 2020","objectID":"/kube-apiserver-start/:2:2","series":null,"tags":["k8s","apiserver"],"title":"kube-apiserver启动流程分析","uri":"/kube-apiserver-start/#apiextensionsserver初始化"},{"categories":["cloud"],"content":" KubeAPIServer初始化KubeAPIServer 主要是提供对 API Resource 的操作请求，为 kubernetes 中众多 API 注册路由信息，暴露 RESTful API 并且对外提供 kubernetes service，使集群中以及集群外的服务都可以通过 RESTful API 操作 kubernetes 中的资源。 与APIExtensionsServer，KubeAPIServer初始化流程如下 CreateKubeAPIServer调用kubeAPIServerConfig.Complete().New来初始化 New函数创建默认的apigroup(pod,deployment等内部资源), 调用InstallAPIs注册 启动相关controller, 加入到poststarthook ","date":"Apr 24, 2020","objectID":"/kube-apiserver-start/:2:3","series":null,"tags":["k8s","apiserver"],"title":"kube-apiserver启动流程分析","uri":"/kube-apiserver-start/#kubeapiserver初始化"},{"categories":["cloud"],"content":" AggregatorServer初始化Aggregator通过APIServices对象关联到某个Service来进行请求的转发，其关联的Service类型进一步决定了请求转发形式。Aggregator包括一个GenericAPIServer和维护自身状态的Controller。其中 GenericAPIServer主要处理apiregistration.k8s.io组下的APIService资源请求。 Aggregator除了处理资源请求外还包含几个controller： apiserviceRegistrationController：负责APIServices中资源的注册与删除； availableConditionController：维护APIServices的可用状态，包括其引用Service是否可用等； autoRegistrationController：用于保持API中存在的一组特定的APIServices； crdRegistrationController：负责将CRD GroupVersions自动注册到APIServices中； openAPIAggregationController：将APIServices资源的变化同步至提供的OpenAPI文档； kubernetes中的一些附加组件，比如metrics-server就是通过Aggregator的方式进行扩展的，实际环境中可以通过使用apiserver-builder工具轻松以Aggregator的扩展方式创建自定义资源。 初始化AggregatorServer的主要逻辑为： 调用aggregatorConfig.Complete().NewWithDelegate创建aggregatorServer 初始化crdRegistrationController和autoRegistrationController，crdRegistrationController负责注册CRD，autoRegistrationController负责将 CRD 对应的 APIServices自动注册到apiserver中，CRD 创建后可通过$ kubectl get apiservices查看是否注册到 apiservices中 将autoRegistrationController和crdRegistrationController加入到PostStartHook中 首先，初始化配置createAggregatorConfig func createAggregatorConfig( kubeAPIServerConfig genericapiserver.Config, commandOptions *options.ServerRunOptions, externalInformers kubeexternalinformers.SharedInformerFactory, serviceResolver aggregatorapiserver.ServiceResolver, proxyTransport *http.Transport, pluginInitializers []admission.PluginInitializer, ) (*aggregatorapiserver.Config, error) { // make a shallow copy to let us twiddle a few things // most of the config actually remains the same. We only need to mess with a couple items related to the particulars of the aggregator genericConfig := kubeAPIServerConfig genericConfig.PostStartHooks = map[string]genericapiserver.PostStartHookConfigEntry{} genericConfig.RESTOptionsGetter = nil // override genericConfig.AdmissionControl with kube-aggregator's scheme, // because aggregator apiserver should use its own scheme to convert its own resources. // 取消admission的配置，aggregator自行处理请求，不需要admissions err := commandOptions.Admission.ApplyTo( \u0026genericConfig, externalInformers, genericConfig.LoopbackClientConfig, feature.DefaultFeatureGate, pluginInitializers...) if err != nil { return nil, err } // copy the etcd options so we don't mutate originals. etcdOptions := *commandOptions.Etcd etcdOptions.StorageConfig.Paging = utilfeature.DefaultFeatureGate.Enabled(features.APIListChunking) etcdOptions.StorageConfig.Codec = aggregatorscheme.Codecs.LegacyCodec(v1beta1.SchemeGroupVersion, v1.SchemeGroupVersion) etcdOptions.StorageConfig.EncodeVersioner = runtime.NewMultiGroupVersioner(v1beta1.SchemeGroupVersion, schema.GroupKind{Group: v1beta1.GroupName}) genericConfig.RESTOptionsGetter = \u0026genericoptions.SimpleRestOptionsFactory{Options: etcdOptions} // override MergedResourceConfig with aggregator defaults and registry if err := commandOptions.APIEnablement.ApplyTo( \u0026genericConfig, aggregatorapiserver.DefaultAPIResourceConfigSource(), aggregatorscheme.Scheme); err != nil { return nil, err } // 配置proxy证书，用于apiserver与扩展服务的通信，使用requestheader证书签发 var certBytes, keyBytes []byte if len(commandOptions.ProxyClientCertFile) \u003e 0 \u0026\u0026 len(commandOptions.ProxyClientKeyFile) \u003e 0 { certBytes, err = ioutil.ReadFile(commandOptions.ProxyClientCertFile) if err != nil { return nil, err } keyBytes, err = ioutil.ReadFile(commandOptions.ProxyClientKeyFile) if err != nil { return nil, err } } aggregatorConfig := \u0026aggregatorapiserver.Config{ GenericConfig: \u0026genericapiserver.RecommendedConfig{ Config: genericConfig, SharedInformerFactory: externalInformers, }, ExtraConfig: aggregatorapiserver.ExtraConfig{ ProxyClientCert: certBytes, ProxyClientKey: keyBytes, ServiceResolver: serviceResolver, // 代理请求的具体实现 ProxyTransport: proxyTransport, }, } // we need to clear the poststarthooks so we don't add them multiple times to all the servers (that fails) // 加入PostStartHook aggregatorConfig.GenericConfig.PostStartHooks = map[string]genericapiserver.PostStartHookConfigEntry{} return aggregatorConfig, nil } createAggregatorServ","date":"Apr 24, 2020","objectID":"/kube-apiserver-start/:2:4","series":null,"tags":["k8s","apiserver"],"title":"kube-apiserver启动流程分析","uri":"/kube-apiserver-start/#aggregatorserver初始化"},{"categories":["cloud"],"content":" 请求分析上面我们分析了apiserver的调用链，大体如下 DefaultHandlerChain-\u003e{handler/crdhandler/proxy}-\u003eadmission-\u003evalidation-\u003eetcd 请求进入时，会经过defaultchain做一些认证鉴权工作 然后通过route执行对应的handler，如果为aggration api, 将直接转发请求到对应service handler处理完，经过admission与validation，做一些修改和检查，用户在这部分可以自定义webhook 最后存入etcd ","date":"Apr 24, 2020","objectID":"/kube-apiserver-start/:3:0","series":null,"tags":["k8s","apiserver"],"title":"kube-apiserver启动流程分析","uri":"/kube-apiserver-start/#请求分析"},{"categories":["cloud"],"content":" 总结本文大体对apiserver的启动流程，以及初始化过程做了分析，由于apiserver实现复杂，中间一些细节没涉及到，还需要对着代码研究研究。 ","date":"Apr 24, 2020","objectID":"/kube-apiserver-start/:4:0","series":null,"tags":["k8s","apiserver"],"title":"kube-apiserver启动流程分析","uri":"/kube-apiserver-start/#总结"},{"categories":["cloud"],"content":" 参考 https://juejin.im/post/5c934e5a5188252d7c216981 https://blog.tianfeiyu.com/2020/02/24/kube_apiserver/ ","date":"Apr 24, 2020","objectID":"/kube-apiserver-start/:5:0","series":null,"tags":["k8s","apiserver"],"title":"kube-apiserver启动流程分析","uri":"/kube-apiserver-start/#参考"},{"categories":["cloud"],"content":"Kubernetes提供了丰富的扩展功能，实现自定义资源有两种方式CRD与Aggregation API。相对于CRD，扩展API功能更丰富，可以实现单独的存储。今天来聊一聊，k8s是如是实现扩展api的，它与apiserver之间又是如何协作的 ","date":"Apr 24, 2020","objectID":"/kube-apiserver-aggretation-api/:0:0","series":null,"tags":["k8s","apiserver"],"title":"kubernetes扩展apiserver实现分析","uri":"/kube-apiserver-aggretation-api/#"},{"categories":["cloud"],"content":" AggregationApiserver介绍Aggregator类似于一个七层负载均衡，将来自用户的请求拦截转发给其他服务器，并且负责整个 APIServer 的 Discovery 功能。 通过APIServices对象关联到某个Service来进行请求的转发，其关联的Service类型进一步决定了请求转发形式。Aggregator包括一个GenericAPIServer和维护自身状态的Controller。其中 GenericAPIServer主要处理apiregistration.k8s.io组下的APIService资源请求。 主要controller包括： apiserviceRegistrationController：负责APIServices中资源的注册与删除； availableConditionController：维护APIServices的可用状态，包括其引用Service是否可用等； autoRegistrationController：用于保持API中存在的一组特定的APIServices； crdRegistrationController：负责将CRD GroupVersions自动注册到APIServices中； openAPIAggregationController：将APIServices资源的变化同步至提供的OpenAPI文档； 在 kube-apiserver 中需要增加以下配置来开启 API Aggregation： --proxy-client-cert-file=/etc/kubernetes/certs/proxy.crt --proxy-client-key-file=/etc/kubernetes/certs/proxy.key --requestheader-client-ca-file=/etc/kubernetes/certs/proxy-ca.crt --requestheader-extra-headers-prefix=X-Remote-Extra- --requestheader-group-headers=X-Remote-Group --requestheader-username-headers=X-Remote-User 如果 kube-proxy 没有和 API server 运行在同一台主机上，那么需要确保启用了如下 apiserver 标记： --enable-aggregator-routing=true 在apiserver启动流程中，分析了AggregationApiserver的初始化流程, 需要了解的可以回去看下。 ","date":"Apr 24, 2020","objectID":"/kube-apiserver-aggretation-api/:1:0","series":null,"tags":["k8s","apiserver"],"title":"kubernetes扩展apiserver实现分析","uri":"/kube-apiserver-aggretation-api/#aggregationapiserver介绍"},{"categories":["cloud"],"content":" AggregationApiserver认证流程与自定义资源定义（CRD）不同，除标准的 Kubernetes apiserver 外，Aggregation API 还涉及另一个服务器：扩展 apiserver。Kubernetes apiserver 将需要与您的扩展 apiserver 通信，并且您的扩展 apiserver 也需要与 Kubernetes apiserver 通信。为了确保此通信的安全，Kubernetes apiserver 使用 x509 证书向扩展 apiserver 认证。 AggregationApi的请求链路如下： defaultHandlerChain-\u003eaggregator-\u003eaggregation-apiserver-\u003eaggregator-\u003euser 大致流程如下： Kubernetes apiserver：对发出请求的用户身份认证，并对请求的 API 路径执行鉴权。 Kubernetes apiserver：将请求转发到扩展 apiserver 扩展 apiserver：认证来自 Kubernetes apiserver 的请求 扩展 apiserver：对来自原始用户的请求鉴权 扩展 apiserver：执行对应操作返回 如图所示： aggregation-apiserver-auth apiserver与扩展apiserver通过证书认证, apiserver配置porxy-client证书(使用requestheader根证书签发)，扩展apiserver配置reqeustheader根证书，如果没配置，会默认从configmap kube-system/extension-apiserver-authentication 去找 扩展apiserver通过extension-apiserver-authentication获取apiserver的client-ca，生成证书对，apiserver可以使用client-ca验证它 由于apiserver-\u003e扩展apiserver通过reqeustheader方式认证，apiserver会将接受到的请求经过认证，转换为header，扩展apiserver通过header获取用户，再通过apiserver接口做权限校验。 有同学有疑问，为什么这里需要做两次认证，两次鉴权。这是由于扩展apiserveer是一个单独的服务器，如果接受非apiserver的请求也是需要做认证鉴权的。那能不能认证是apiserver后就不做鉴权了呢，这得需要apiserver在转发请求时加入鉴权信息就行。 ","date":"Apr 24, 2020","objectID":"/kube-apiserver-aggretation-api/:2:0","series":null,"tags":["k8s","apiserver"],"title":"kubernetes扩展apiserver实现分析","uri":"/kube-apiserver-aggretation-api/#aggregationapiserver认证流程"},{"categories":["cloud"],"content":" AggregationApiserver处理流程","date":"Apr 24, 2020","objectID":"/kube-apiserver-aggretation-api/:3:0","series":null,"tags":["k8s","apiserver"],"title":"kubernetes扩展apiserver实现分析","uri":"/kube-apiserver-aggretation-api/#aggregationapiserver处理流程"},{"categories":["cloud"],"content":" apiserver处理逻辑在apiserver认证时，认证接受会将认证信息删除, 可参考前面的[apiserver认证源码分析] 处理逻辑如下： 通过context获取user信息 构造请求，删除reqeustheader信息，通过user重新填充 通过proxyRoundTripper转发请求 (kube-apiserver-authentication-code.md) aggregation的hander的实现： // 通过context获取user user, ok := genericapirequest.UserFrom(req.Context()) if !ok { proxyError(w, req, \"missing user\", http.StatusInternalServerError) return } // 构造请求url,通过apiservice配置的service/namespace随机得到某个endpoint后端 location := \u0026url.URL{} location.Scheme = \"https\" rloc, err := r.serviceResolver.ResolveEndpoint(handlingInfo.serviceNamespace, handlingInfo.serviceName, handlingInfo.servicePort) if err != nil { klog.Errorf(\"error resolving %s/%s: %v\", handlingInfo.serviceNamespace, handlingInfo.serviceName, err) proxyError(w, req, \"service unavailable\", http.StatusServiceUnavailable) return } location.Host = rloc.Host location.Path = req.URL.Path location.RawQuery = req.URL.Query().Encode() // we need to wrap the roundtripper in another roundtripper which will apply the front proxy headers // 包裹请求信息，将user信息放到header中 proxyRoundTripper, upgrade, err := maybeWrapForConnectionUpgrades(handlingInfo.restConfig, handlingInfo.proxyRoundTripper, req) if err != nil { proxyError(w, req, err.Error(), http.StatusInternalServerError) return } proxyRoundTripper = transport.NewAuthProxyRoundTripper(user.GetName(), user.GetGroups(), user.GetExtra(), proxyRoundTripper) // 调用后端 handler := proxy.NewUpgradeAwareHandler(location, proxyRoundTripper, true, upgrade, \u0026responder{w: w}) handler.ServeHTTP(w, newReq) 根据扩展apiserver找到后端时通过service获取对应endpoint列表，随机选择某个endpoint、 实现如下： // ResourceLocation returns a URL to which one can send traffic for the specified service. func ResolveEndpoint(services listersv1.ServiceLister, endpoints listersv1.EndpointsLister, namespace, id string, port int32) (*url.URL, error) { svc, err := services.Services(namespace).Get(id) if err != nil { return nil, err } svcPort, err := findServicePort(svc, port) if err != nil { return nil, err } switch { case svc.Spec.Type == v1.ServiceTypeClusterIP, svc.Spec.Type == v1.ServiceTypeLoadBalancer, svc.Spec.Type == v1.ServiceTypeNodePort: // these are fine default: return nil, fmt.Errorf(\"unsupported service type %q\", svc.Spec.Type) } eps, err := endpoints.Endpoints(namespace).Get(svc.Name) if err != nil { return nil, err } if len(eps.Subsets) == 0 { return nil, errors.NewServiceUnavailable(fmt.Sprintf(\"no endpoints available for service %q\", svc.Name)) } // Pick a random Subset to start searching from. ssSeed := rand.Intn(len(eps.Subsets)) // Find a Subset that has the port. for ssi := 0; ssi \u003c len(eps.Subsets); ssi++ { ss := \u0026eps.Subsets[(ssSeed+ssi)%len(eps.Subsets)] if len(ss.Addresses) == 0 { continue } for i := range ss.Ports { if ss.Ports[i].Name == svcPort.Name { // Pick a random address. // 核心，随机选择endpoint ip := ss.Addresses[rand.Intn(len(ss.Addresses))].IP port := int(ss.Ports[i].Port) return \u0026url.URL{ Scheme: \"https\", Host: net.JoinHostPort(ip, strconv.Itoa(port)), }, nil } } } return nil, errors.NewServiceUnavailable(fmt.Sprintf(\"no endpoints available for service %q\", id)) } ProxyRoundTripper创建在round_trippers.go func NewAuthProxyRoundTripper(username string, groups []string, extra map[string][]string, rt http.RoundTripper) http.RoundTripper { return \u0026authProxyRoundTripper{ username: username, groups: groups, extra: extra, rt: rt, } } func (rt *authProxyRoundTripper) RoundTrip(req *http.Request) (*http.Response, error) { req = utilnet.CloneRequest(req) // 包裹user信息 SetAuthProxyHeaders(req, rt.username, rt.groups, rt.extra) return rt.rt.RoundTrip(req) } // SetAuthProxyHeaders stomps the auth proxy header fields. It mutates its argument. func SetAuthProxyHeaders(req *http.Request, username string, groups []string, extra map[string][]string) { // 清楚原始url的requestheader信息 req.Header.Del(\"X-Remote-User\") req.Header.Del(\"X-Remote-Group\") for key := range req.Header { if strings.HasPrefix(strings.ToLower(key), strings.ToLower(\"X-Remote-Extra-\")) { req.Header.Del(key) } } // 通过","date":"Apr 24, 2020","objectID":"/kube-apiserver-aggretation-api/:3:1","series":null,"tags":["k8s","apiserver"],"title":"kubernetes扩展apiserver实现分析","uri":"/kube-apiserver-aggretation-api/#apiserver处理逻辑"},{"categories":["cloud"],"content":" 扩展apiserver处理逻辑下以metrics-server为例说明扩展apiserver在收到apiserver请求后的处理 与apiserver初始化相同，metrics-server也需要初始化生成genericServer, 然后注册apigroup pkg/metrics-server/config.go func (c Config) Complete() (*MetricsServer, error) { informer, err := c.informer() if err != nil { return nil, err } kubeletClient, err := c.kubeletClient() if err != nil { return nil, err } addressResolver := c.addressResolver() // 创建scraper，负责抓取监控数据 scrape := scraper.NewScraper(informer.Core().V1().Nodes().Lister(), kubeletClient, addressResolver, c.ScrapeTimeout) scraper.RegisterScraperMetrics(c.ScrapeTimeout) RegisterServerMetrics(c.MetricResolution) // 生成genericServer, 包裹有 DefaultBuildHandlerChain genericServer, err := c.Apiserver.Complete(informer).New(\"metrics-server\", genericapiserver.NewEmptyDelegate()) if err != nil { return nil, err } store := storage.NewStorage() // 注册api if err := api.Install(store, informer.Core().V1(), genericServer); err != nil { return nil, err } return \u0026MetricsServer{ GenericAPIServer: genericServer, storage: store, scraper: scrape, resolution: c.MetricResolution, }, nil } api注册代码，通过Build生成apigroup，调用InstallAPIGroup进行注册 pkg/api/install.go // InstallStorage builds the metrics for the metrics.k8s.io API, and then installs it into the given API metrics-server. func Install(metrics MetricsGetter, informers coreinf.Interface, server *genericapiserver.GenericAPIServer) error { info := Build(metrics, informers) // 注册apigroup return server.InstallAPIGroup(\u0026info) } // Build constructs APIGroupInfo the metrics.k8s.io API group using the given getters. func Build(m MetricsGetter, informers coreinf.Interface) genericapiserver.APIGroupInfo { apiGroupInfo := genericapiserver.NewDefaultAPIGroupInfo(metrics.GroupName, Scheme, metav1.ParameterCodec, Codecs) // 注册metrics相关api node := newNodeMetrics(metrics.Resource(\"nodemetrics\"), m, informers.Nodes().Lister()) pod := newPodMetrics(metrics.Resource(\"podmetrics\"), m, informers.Pods().Lister()) metricsServerResources := map[string]rest.Storage{ \"nodes\": node, \"pods\": pod, } apiGroupInfo.VersionedResourcesStorageMap[v1beta1.SchemeGroupVersion.Version] = metricsServerResources return apiGroupInfo } 同apiserver，metrics-server收到请求后会经过DefaultBuildHandlerChain 认证，从apiserver转发来的请求是reqeustheader形式，metrics-server会使用requestheader-ca验证证书 鉴权，同apiserver一样 注意, 如果apiserver未配置proxy-client证书，metrics-server认证不通过，即使apiserver认证通过，metrics-server也会认为是匿名用户system:anonymous 最后，metrics-server执行具体逻辑，返回结果。 ","date":"Apr 24, 2020","objectID":"/kube-apiserver-aggretation-api/:3:2","series":null,"tags":["k8s","apiserver"],"title":"kubernetes扩展apiserver实现分析","uri":"/kube-apiserver-aggretation-api/#扩展apiserver处理逻辑"},{"categories":["cloud"],"content":" 总结扩容apiserver的创建，处理流程与apiserver完全一样，可以直接调用apiserver的库，扩展apiserver直接处理请求，不需要经过webhook，性能更好，更强大的是完全不使用etcd，替换成时序数据库或者其他数据库。后续可以分析下CRD与扩展apiserver的区别以及使用场景。 ","date":"Apr 24, 2020","objectID":"/kube-apiserver-aggretation-api/:4:0","series":null,"tags":["k8s","apiserver"],"title":"kubernetes扩展apiserver实现分析","uri":"/kube-apiserver-aggretation-api/#总结"},{"categories":["cloud"],"content":" 简介kube-apiserver中与权限相关的主要有三种机制，即认证、鉴权和准入控制。上节讲到认证流程。 认证与授权很容易混淆： 认证(Authentication), 负责检查你是谁，识别user 授权(Authorization), 你能做什么，是否允许User对资源的操作 审计(Audit), 负责记录操作信息，方便后续审查 本文主要分析apiserver的rbac授权流程。 ","date":"Apr 23, 2020","objectID":"/kube-apiserver-authorization-code/:1:0","series":null,"tags":["k8s","rbac"],"title":"kube-apiserver鉴权源码分析","uri":"/kube-apiserver-authorization-code/#简介"},{"categories":["cloud"],"content":" 认证流程分析权限相关代码从k8s.io/apiserver/pkg/server/config.go中DefaultBuildHandlerChain函数开始执行 func DefaultBuildHandlerChain(apiHandler http.Handler, c *Config) http.Handler { handler := genericapifilters.WithAuthorization(apiHandler, c.Authorization.Authorizer, c.Serializer) handler = genericfilters.WithMaxInFlightLimit(handler, c.MaxRequestsInFlight, c.MaxMutatingRequestsInFlight, c.LongRunningFunc) handler = genericapifilters.WithImpersonation(handler, c.Authorization.Authorizer, c.Serializer) handler = genericapifilters.WithAudit(handler, c.AuditBackend, c.AuditPolicyChecker, c.LongRunningFunc) failedHandler := genericapifilters.Unauthorized(c.Serializer, c.Authentication.SupportsBasicAuth) failedHandler = genericapifilters.WithFailedAuthenticationAudit(failedHandler, c.AuditBackend, c.AuditPolicyChecker) handler = genericapifilters.WithAuthentication(handler, c.Authentication.Authenticator, failedHandler, c.Authentication.APIAudiences) handler = genericfilters.WithCORS(handler, c.CorsAllowedOriginList, nil, nil, nil, \"true\") handler = genericfilters.WithTimeoutForNonLongRunningRequests(handler, c.LongRunningFunc, c.RequestTimeout) handler = genericfilters.WithWaitGroup(handler, c.LongRunningFunc, c.HandlerChainWaitGroup) handler = genericapifilters.WithRequestInfo(handler, c.RequestInfoResolver) handler = genericfilters.WithPanicRecovery(handler) return handler } DefaultBuildHandlerChain中包含了多种filter（如认证，链接数检验，RBAC权限检验等），授权步骤在WithAuthorization中，如下： // WithAuthorizationCheck passes all authorized requests on to handler, and returns a forbidden error otherwise. func WithAuthorization(handler http.Handler, a authorizer.Authorizer, s runtime.NegotiatedSerializer) http.Handler { // 检查是否需要权限校验 if a == nil { klog.Warningf(\"Authorization is disabled\") return handler } return http.HandlerFunc(func(w http.ResponseWriter, req *http.Request) { ctx := req.Context() // 用作审计 ae := request.AuditEventFrom(ctx) // 获取Attribute, 通过reqeust获取到请求的user, resource, verb, 是否为namespace级别的等 attributes, err := GetAuthorizerAttributes(ctx) if err != nil { responsewriters.InternalError(w, req, err) return } // 执行认证流程 authorized, reason, err := a.Authorize(ctx, attributes) // an authorizer like RBAC could encounter evaluation errors and still allow the request, so authorizer decision is checked before error here. if authorized == authorizer.DecisionAllow { audit.LogAnnotation(ae, decisionAnnotationKey, decisionAllow) audit.LogAnnotation(ae, reasonAnnotationKey, reason) // 校验成功，记录信息，转到下一个handler handler.ServeHTTP(w, req) return } if err != nil { audit.LogAnnotation(ae, reasonAnnotationKey, reasonError) responsewriters.InternalError(w, req, err) return } // 校验失败返回403，注意认证失败返回的是401 klog.V(4).Infof(\"Forbidden: %#v, Reason: %q\", req.RequestURI, reason) audit.LogAnnotation(ae, decisionAnnotationKey, decisionForbid) audit.LogAnnotation(ae, reasonAnnotationKey, reason) responsewriters.Forbidden(ctx, attributes, w, req, reason, s) }) } 授权流程比较清晰，从request获取请求信息，进行鉴权，成功进入后续handler，失败返回403。 Authorize接口有多种实现，通过在apiserver配置--authorization-mode选择鉴权模式，包括： ABAC RBAC Node, 用于kubelet鉴权exec/logs等 AlwaysAllow AlwaysDeny Webhook， 用于扩展权限，用户可实现Webhook与其他权限系统集成 如果选择AlwaysAllow,即不做鉴权, 开启后强制不允许匿名用户 // ApplyAuthorization will conditionally modify the authentication options based on the authorization options func (o *BuiltInAuthenticationOptions) ApplyAuthorization(authorization *BuiltInAuthorizationOptions) { if o == nil || authorization == nil || o.Anonymous == nil { return } // authorization ModeAlwaysAllow cannot be combined with AnonymousAuth. // in such a case the AnonymousAuth is stomped to false and you get a message if o.Anonymous.Allow \u0026\u0026 sets.NewString(authorization.Modes...).Has(authzmodes.ModeAlwaysAllow) { klog.Warningf(\"AnonymousAuth is not allowed with the AlwaysAllow authorizer. Resetting AnonymousAuth to false. You should use a different authorizer\") o.Anonymous.Allow = false } } ","date":"Apr 23, 2020","objectID":"/kube-apiserver-authorization-code/:2:0","series":null,"tags":["k8s","rbac"],"title":"kube-apiserver鉴权源码分析","uri":"/kube-apiserver-authorization-code/#认证流程分析"},{"categories":["cloud"],"content":" rbac鉴权rbac是常用的鉴权方式，实现Authorize接口, 代码在rbac.go func (r *RBACAuthorizer) Authorize(ctx context.Context, requestAttributes authorizer.Attributes) (authorizer.Decision, string, error) { ruleCheckingVisitor := \u0026authorizingVisitor{requestAttributes: requestAttributes} // 调用VisitRulesFor来检查是否用权限 r.authorizationRuleResolver.VisitRulesFor(requestAttributes.GetUser(), requestAttributes.GetNamespace(), ruleCheckingVisitor.visit) if ruleCheckingVisitor.allowed { // 成功直接返回 return authorizer.DecisionAllow, ruleCheckingVisitor.reason, nil } // 失败，打印日志返回失败原因 // Build a detailed log of the denial. // Make the whole block conditional so we don't do a lot of string-building we won't use. if klog.V(5) { var operation string if requestAttributes.IsResourceRequest() { b := \u0026bytes.Buffer{} b.WriteString(`\"`) b.WriteString(requestAttributes.GetVerb()) b.WriteString(`\" resource \"`) b.WriteString(requestAttributes.GetResource()) if len(requestAttributes.GetAPIGroup()) \u003e 0 { b.WriteString(`.`) b.WriteString(requestAttributes.GetAPIGroup()) } if len(requestAttributes.GetSubresource()) \u003e 0 { b.WriteString(`/`) b.WriteString(requestAttributes.GetSubresource()) } b.WriteString(`\"`) if len(requestAttributes.GetName()) \u003e 0 { b.WriteString(` named \"`) b.WriteString(requestAttributes.GetName()) b.WriteString(`\"`) } operation = b.String() } else { operation = fmt.Sprintf(\"%q nonResourceURL %q\", requestAttributes.GetVerb(), requestAttributes.GetPath()) } var scope string if ns := requestAttributes.GetNamespace(); len(ns) \u003e 0 { scope = fmt.Sprintf(\"in namespace %q\", ns) } else { scope = \"cluster-wide\" } klog.Infof(\"RBAC DENY: user %q groups %q cannot %s %s\", requestAttributes.GetUser().GetName(), requestAttributes.GetUser().GetGroups(), operation, scope) } reason := \"\" if len(ruleCheckingVisitor.errors) \u003e 0 { reason = fmt.Sprintf(\"RBAC: %v\", utilerrors.NewAggregate(ruleCheckingVisitor.errors)) } return authorizer.DecisionNoOpinion, reason, nil } Authorize调用了VisitRulesFor来处理具体鉴权操作, 代码在rule.go func (r *DefaultRuleResolver) VisitRulesFor(user user.Info, namespace string, visitor func(source fmt.Stringer, rule *rbacv1.PolicyRule, err error) bool) { // 获取所有clusterrolebinding if clusterRoleBindings, err := r.clusterRoleBindingLister.ListClusterRoleBindings(); err != nil { if !visitor(nil, nil, err) { return } } else { sourceDescriber := \u0026clusterRoleBindingDescriber{} // 遍历clusterrolebing for _, clusterRoleBinding := range clusterRoleBindings { // 检查是否有对应的user subjectIndex, applies := appliesTo(user, clusterRoleBinding.Subjects, \"\") if !applies { continue } // 如果user存在于subject, 获取对应的rules即clusterrole rules, err := r.GetRoleReferenceRules(clusterRoleBinding.RoleRef, \"\") if err != nil { if !visitor(nil, nil, err) { return } continue } sourceDescriber.binding = clusterRoleBinding sourceDescriber.subject = \u0026clusterRoleBinding.Subjects[subjectIndex] for i := range rules { // 调用visitor判断是否需要进入下一步鉴权 if !visitor(sourceDescriber, \u0026rules[i], nil) { return } } } } // clusterrole遍历完还没有鉴权成功，接着遍历所在namespace的role，流程同上 if len(namespace) \u003e 0 { if roleBindings, err := r.roleBindingLister.ListRoleBindings(namespace); err != nil { if !visitor(nil, nil, err) { return } } else { sourceDescriber := \u0026roleBindingDescriber{} for _, roleBinding := range roleBindings { subjectIndex, applies := appliesTo(user, roleBinding.Subjects, namespace) if !applies { continue } rules, err := r.GetRoleReferenceRules(roleBinding.RoleRef, namespace) if err != nil { if !visitor(nil, nil, err) { return } continue } sourceDescriber.binding = roleBinding sourceDescriber.subject = \u0026roleBinding.Subjects[subjectIndex] for i := range rules { if !visitor(sourceDescriber, \u0026rules[i], nil) { return } } } } } } visit函数, 用来判断是否认证成功，成功返回false, 不需要进行下一步鉴权 func (v *authorizingVisitor) visit(source fmt.Stringer, rule *rbacv1.PolicyRule, err error) bool { if rule != nil \u0026\u0026 RuleAllows(v.requestAttributes, rule) { // allowed用来表示是否认证成功 v.allowed = true v.reason = fmt.Sprintf(\"RBAC: allowed by %s\", source.String()) return false } if er","date":"Apr 23, 2020","objectID":"/kube-apiserver-authorization-code/:3:0","series":null,"tags":["k8s","rbac"],"title":"kube-apiserver鉴权源码分析","uri":"/kube-apiserver-authorization-code/#rbac鉴权"},{"categories":["cloud"],"content":" 总结本文结合RBAC分析了Kubernetes的鉴权流程，整体这部分比较代码清晰。RBAC是Kubernetes比较推荐的鉴权方式，了解完整个流程后，居然所有请求都会先遍历一遍ClusterRoleBindings，这样实现起来比较简单，但随着规模和用户的扩大，这部分是否会有性能问题，需不需要实现能够快速鉴权的方式。 ","date":"Apr 23, 2020","objectID":"/kube-apiserver-authorization-code/:4:0","series":null,"tags":["k8s","rbac"],"title":"kube-apiserver鉴权源码分析","uri":"/kube-apiserver-authorization-code/#总结"},{"categories":["cloud"],"content":" 背景业务反馈使用Ingress的ip-hash, 同一个服务开启了http和websocket分别是两个端口, 但是配置ip-hash后, 同一个client的请求http和websocket不在同一个后端. ","date":"Apr 15, 2020","objectID":"/ingress-ip-hash/:1:0","series":null,"tags":["k8s","ingress","nginx"],"title":"多端口服务的Ingress IP-hash问题","uri":"/ingress-ip-hash/#背景"},{"categories":["cloud"],"content":" 探究根据业务Ingress配置,配置如下实例: apiVersion: extensions/v1beta1 kind: Ingress metadata: annotations: nginx.ingress.kubernetes.io/cors-allow-origin: '*' nginx.ingress.kubernetes.io/enable-cors: \"true\" nginx.ingress.kubernetes.io/proxy-body-size: 200m nginx.ingress.kubernetes.io/proxy-read-timeout: \"300\" nginx.ingress.kubernetes.io/upstream-hash-by: $binary_remote_addr name: hellogo spec: rules: - host: hellogo.d.xiaomi.net http: paths: - backend: serviceName: hellogo #http1, 8080 servicePort: 8080 path: /8080 - backend: serviceName: hellogo #http2, 9090 servicePort: 9090 path: /9090 - backend: serviceName: hellogo #websocket, 8081 servicePort: 8081 path: /ws 创建多个副本 $ kubectl get po -l app=hellogo NAME READY STATUS RESTARTS AGE hellogo-699f997454-b5vs4 1/1 Running 0 66m hellogo-699f997454-hm924 1/1 Running 0 66m hellogo-699f997454-mfbqv 1/1 Running 0 66m hellogo-699f997454-qdrwn 1/1 Running 0 66m hellogo-699f997454-srh9b 1/1 Running 0 66m hellogo-699f997454-wlwfh 1/1 Running 0 66m 测试http 8080端口, 请求到pod hellogo-699f997454-qdrwn $ curl http://hellogo.d.xiaomi.net/8080 hello 8080! host hellogo.d.xiaomi.net remoteaddr 10.46.23.1:15340 realip 10.232.41.102 hostname hellogo-699f997454-qdrwn $ curl http://hellogo.d.xiaomi.net/8080 hello 8080! host hellogo.d.xiaomi.net remoteaddr 10.46.23.1:15866 realip 10.232.41.102 hostname hellogo-699f997454-qdrwn 测试http 8080端口, 请求到pod hellogo-699f997454-b5vs4 $ curl http://hellogo.d.xiaomi.net/9090 hello 9090! host hellogo.d.xiaomi.net remoteaddr 10.38.200.195:23706 realip 10.232.41.102 hostname hellogo-699f997454-b5vs4 $ curl http://hellogo.d.xiaomi.net/9090 hello 9090! host hellogo.d.xiaomi.net remoteaddr 10.38.200.195:23706 realip 10.232.41.102 hostname hellogo-699f997454-b5vs4 猜想是由于获取的nginx server列表顺序不一致导致的, 但是看源码ip list是直接从endpoint获取的, 进入nginx-ingress查看 $ kubectl exec -it -n kube-system nginx-ingress-controller-m496n sh # dbg工具查看nginx后端列表 /etc/nginx $ /dbg backends list | grep hellogo default-hellogo-8080 default-hellogo-8081 default-hellogo-9090 # 8080端口的列表 /etc/nginx $ /dbg backends get default-hellogo-8080 { \"endpoints\": [ { \"address\": \"10.46.12.107\", \"port\": \"8080\" }, { \"address\": \"10.46.12.108\", \"port\": \"8080\" }, { \"address\": \"10.46.12.109\", \"port\": \"8080\" }, { \"address\": \"10.46.23.23\", \"port\": \"8080\" }, { \"address\": \"10.46.23.25\", \"port\": \"8080\" }, { \"address\": \"10.46.23.29\", \"port\": \"8080\" } ], \"name\": \"default-hellogo-8080\", \"noServer\": false, \"port\": 8080, ... } # 9090端口的列表 /etc/nginx $ /dbg backends get default-hellogo-9090 { \"endpoints\": [ { \"address\": \"10.46.12.107\", \"port\": \"9090\" }, { \"address\": \"10.46.12.108\", \"port\": \"9090\" }, { \"address\": \"10.46.12.109\", \"port\": \"9090\" }, { \"address\": \"10.46.23.23\", \"port\": \"9090\" }, { \"address\": \"10.46.23.25\", \"port\": \"9090\" }, { \"address\": \"10.46.23.29\", \"port\": \"9090\" } ], \"name\": \"default-hellogo-9090\", \"noServer\": false, \"port\": 9090, ... } 对比发现两个端口的列表是一样的,只能看看代码. ip-hash代码在https://github.com/kubernetes/ingress-nginx/blob/master/rootfs/etc/nginx/lua/balancer/chash.lua function _M.new(self, backend) local nodes = util.get_nodes(backend.endpoints) local o = { instance = self.factory:new(nodes), --获取后端pod ip列表 hash_by = backend[\"upstreamHashByConfig\"][\"upstream-hash-by\"], traffic_shaping_policy = backend.trafficShapingPolicy, alternative_backends = backend.alternativeBackends, } setmetatable(o, self) self.__index = self return o end function _M.balance(self) local key = util.lua_ngx_var(self.hash_by) --获取需要hash的变量 return self.instance:find(key) --计算hash值 end return _M 关键是在get_nodes函数,位于https://github.com/kubernetes/ingress-nginx/blob/master/rootfs/etc/nginx/lua/util.lua function _M.get_nodes(endpoints) local nodes = {} local weight = 1 --所有后端weight相同都为1 for _, endpoint in pairs(endpoints) do local endpoint_string = endpoint.address .. \":\" .. endpoint.port --endpoint为ip+port nodes[endpoint_string] = weight end return nodes end 通过代码可以看到在ingress-nginx中,实际的后端(upstream)是包含端口的,通过hash计算得到的值也不一样。 ","date":"Apr 15, 2020","objectID":"/ingress-ip-hash/:2:0","series":null,"tags":["k8s","ingress","nginx"],"title":"多端口服务的Ingress IP-hash问题","uri":"/ingress-ip-hash/#探究"},{"categories":["cloud"],"content":" 解决建议首先确认系统的架构是不是合理，不同的端口提供不同的服务，一般是相互独立的。 如果确实有类似需求： 通过同一个端口提供服务，使用path来区分不同功能 修改代码，也比较简单 ","date":"Apr 15, 2020","objectID":"/ingress-ip-hash/:3:0","series":null,"tags":["k8s","ingress","nginx"],"title":"多端口服务的Ingress IP-hash问题","uri":"/ingress-ip-hash/#解决建议"},{"categories":["cloud"],"content":"没有人不想优雅的活着，在这喧闹的生活中过得优雅从容并不容易。但在k8s的世界中，如何做个优雅的Pod还是有套路可循的。 ","date":"Apr 11, 2020","objectID":"/pod-graceful-lifecycle/:0:0","series":null,"tags":["k8s"],"title":"如何做一个优雅的Pod","uri":"/pod-graceful-lifecycle/#"},{"categories":["cloud"],"content":" Pod的生命周期在优雅之前，我们先谈谈Pod的一生，大体分为以下几个阶段 创建，通过kubectl或者api创建pod, apiserver收到请求后存储到etcd 调度，scheduler检测到pod创建后，通过预选优选为pod选取合适的人家(node) 启动，kubelet检测到有pod调度到当前节点，开始启动pod 终止，不同的pod有不同的谢幕方式，有的正常运行结束没有restart就completed，有的被kill就入土为安了，有的被驱逐换种方式重新开始 今天我们主要讨论3-4阶段，前面部分更多是deployment/daemonset这些pod的父母所决定的。 ","date":"Apr 11, 2020","objectID":"/pod-graceful-lifecycle/:1:0","series":null,"tags":["k8s"],"title":"如何做一个优雅的Pod","uri":"/pod-graceful-lifecycle/#pod的生命周期"},{"categories":["cloud"],"content":" 优雅的启动","date":"Apr 11, 2020","objectID":"/pod-graceful-lifecycle/:2:0","series":null,"tags":["k8s"],"title":"如何做一个优雅的Pod","uri":"/pod-graceful-lifecycle/#优雅的启动"},{"categories":["cloud"],"content":" init container通常pod有一些初始化操作，创建文件夹，初始化磁盘，检查某些依赖服务是不是正常，这些操作放在代码中会污染代码，写在启动命令中不方便管理，出问题也不方便排查，更优雅的方式是使用k8s的[init container][1]。 理解 Init 容器 Pod 可以包含多个容器，应用运行在这些容器里面，同时 Pod 也可以有一个或多个先于应用容器启动的 Init 容器。 Init 容器与普通的容器非常像，除了如下两点： 它们总是运行到完成。 每个都必须在下一个启动之前成功完成。 如果 Pod 的 Init 容器失败，Kubernetes 会不断地重启该 Pod，直到 Init 容器成功为止。然而，如果 Pod 对应的 restartPolicy 值为 Never，它不会重新启动。 如果为一个 Pod 指定了多个 Init 容器，这些容器会按顺序逐个运行。每个 Init 容器必须运行成功，下一个才能够运行。当所有的 Init 容器运行完成时，Kubernetes 才会为 Pod 初始化应用容器并像平常一样运行。 Init 容器能做什么？ 因为 Init 容器具有与应用容器分离的单独镜像，其启动相关代码具有如下优势： Init 容器可以包含一些安装过程中应用容器中不存在的实用工具或个性化代码。例如，没有必要仅为了在安装过程中使用类似 sed、 awk、 python 或 dig 这样的工具而去FROM 一个镜像来生成一个新的镜像。 Init 容器可以安全地运行这些工具，避免这些工具导致应用镜像的安全性降低。 应用镜像的创建者和部署者可以各自独立工作，而没有必要联合构建一个单独的应用镜像。 Init 容器能以不同于Pod内应用容器的文件系统视图运行。因此，Init容器可具有访问 Secrets 的权限，而应用容器不能够访问。 由于 Init 容器必须在应用容器启动之前运行完成，因此 Init 容器提供了一种机制来阻塞或延迟应用容器的启动，直到满足了一组先决条件。一旦前置条件满足，Pod内的所有的应用容器会并行启动。 示例 下面的例子定义了一个具有 2 个 Init 容器的简单 Pod。 第一个等待 myservice 启动，第二个等待 mydb 启动。 一旦这两个 Init容器 都启动完成，Pod 将启动spec区域中的应用容器。 apiVersion: v1 kind: Pod metadata: name: myapp-pod labels: app: myapp spec: containers: - name: myapp-container image: busybox:1.28 command: ['sh', '-c', 'echo The app is running! \u0026\u0026 sleep 3600'] initContainers: - name: init-myservice image: busybox:1.28 command: ['sh', '-c', \"until nslookup myservice.$(cat /var/run/secrets/kubernetes.io/serviceaccount/namespace).svc.cluster.local; do echo waiting for myservice; sleep 2; done\"] - name: init-mydb image: busybox:1.28 command: ['sh', '-c', \"until nslookup mydb.$(cat /var/run/secrets/kubernetes.io/serviceaccount/namespace).svc.cluster.local; do echo waiting for mydb; sleep 2; done\"] ","date":"Apr 11, 2020","objectID":"/pod-graceful-lifecycle/:2:1","series":null,"tags":["k8s"],"title":"如何做一个优雅的Pod","uri":"/pod-graceful-lifecycle/#init-container"},{"categories":["cloud"],"content":" readinessProbepod启动后，如果直接加入endpoint，有可能服务还没初始化完成，端口没有就绪，这时候接收流量肯定无法正常处理。如果能判断pod是否ready就好了，当当当，readiness来了，可以通过http，tcp以及执行命令的方式来检查服务情况，检查成功后再将pod状态设置为ready,ready后才会加入到endpoint中。 下为一个readiness探测，5秒执行一次命令，执行成功则pod变为ready readinessProbe: exec: command: - cat - /tmp/healthy initialDelaySeconds: 5 periodSeconds: 5 注 http, tcp探针是kubelet执行的，所以无法探测容器中localhost的端口，也无法解析service exec则在容器内执行的 ","date":"Apr 11, 2020","objectID":"/pod-graceful-lifecycle/:2:2","series":null,"tags":["k8s"],"title":"如何做一个优雅的Pod","uri":"/pod-graceful-lifecycle/#readinessprobe"},{"categories":["cloud"],"content":" ReadinessGatesReadinessProbe机制可能无法满足某些复杂应用对容器内服务可用状态的判断，所以kubernetes从1.11版本开始引入了Pod Ready++特性对Readiness探测机制进行扩展，在1.14版本时达到GA稳定版本，称其为Pod Readiness Gates。 通过Pod Readiness Gates机制，用户可以将自定义的ReadinessProbe探测方式设置在Pod上，辅助kubernetes设置Pod何时达到服务可用状态Ready，为了使自定义的ReadinessProbe生效，用户需要提供一个外部的控制器Controller来设置相应的Condition状态。Pod的Readiness Gates在pod定义中的ReadinessGates字段进行设置， 如下示例设置了一个类型为www.example.com/feature-1的新Readiness Gates： Kind: Pod spec: readinessGates: - conditionType: \"www.example.com/feature-1\" status: conditions: - type: Ready # kubernetes系统内置的名为Ready的Condition status: \"True\" lastProbeTime: null lastTransitionTime: 2018-01-01T00:00:00Z - type: \"www.example.com/feature-1\" # 用户定义的Condition status: \"False\" lastProbeTime: null lastTransitionTime: 2018-01-01T00:00:00Z containerStatuses: - containerID: docker://abcd... ready: true 新增的自定义Condition的状态status将由用户自定义的外部控制器设置，默认值为False，kubernetes将在判断全部readinessGates条件都为True时，才设置pod为服务可用状态（Ready或True）。 ","date":"Apr 11, 2020","objectID":"/pod-graceful-lifecycle/:2:3","series":null,"tags":["k8s"],"title":"如何做一个优雅的Pod","uri":"/pod-graceful-lifecycle/#readinessgates"},{"categories":["cloud"],"content":" poststart另外也可以通过poststart设置hook操作，做一些额外工作。k8s在容器创建后立即发送 postStart 事件。然而，postStart 处理函数的调用不保证早于容器的入口点（entrypoint） 的执行。postStart 处理函数与容器的代码是异步执行的，但 Kubernetes 的容器管理逻辑会一直阻塞等待 postStart 处理函数执行完毕。只有 postStart 处理函数执行完毕，容器的状态才会变成RUNNING。 ","date":"Apr 11, 2020","objectID":"/pod-graceful-lifecycle/:2:4","series":null,"tags":["k8s"],"title":"如何做一个优雅的Pod","uri":"/pod-graceful-lifecycle/#poststart"},{"categories":["cloud"],"content":" 优雅的运行","date":"Apr 11, 2020","objectID":"/pod-graceful-lifecycle/:3:0","series":null,"tags":["k8s"],"title":"如何做一个优雅的Pod","uri":"/pod-graceful-lifecycle/#优雅的运行"},{"categories":["cloud"],"content":" livenessProbe同readinessProbe探针，livenessProbe是用来检查pod运行状态是否正常，如果探测失败，pod被kill掉，重启启动pod。 ","date":"Apr 11, 2020","objectID":"/pod-graceful-lifecycle/:3:1","series":null,"tags":["k8s"],"title":"如何做一个优雅的Pod","uri":"/pod-graceful-lifecycle/#livenessprobe"},{"categories":["cloud"],"content":" restartpolicy如果pod运行时意外退出(程序故障)，kubelet会根据restart policy来判断是否重启pod，可能的值为 Always、OnFailure 和 Never。默认为 Always，如果容器退出会再再启动，pod启动次数加1。 ","date":"Apr 11, 2020","objectID":"/pod-graceful-lifecycle/:3:2","series":null,"tags":["k8s"],"title":"如何做一个优雅的Pod","uri":"/pod-graceful-lifecycle/#restartpolicy"},{"categories":["cloud"],"content":" 优雅的结束首先谈下pod的删除流程： 用户发送命令删除 Pod，使用的是默认的宽限期（grace period 30秒） apiserver中的 Pod 会随着宽限期规定的时间进行更新，过了这个时间 Pod 就会被认为已\"dead\" 当使用客户端命令查询 Pod 状态时，Pod 显示为 “Terminating” （和第 3 步同步进行）当 Kubelet 看到 Pod 由于步骤 2 中设置的时间而被标记为 terminating 状态时，它就开始执行关闭 Pod 流程 如果 Pod 定义了 preStop 钩子，就在 Pod 内部调用它。如果宽限期结束了，但是 preStop 钩子还在运行，那么就用小的（2 秒）扩展宽限期调用步骤 2。 给 Pod 内的进程发送 TERM 信号(即kill, kill -15)。请注意，并不是所有 Pod 中的容器都会同时收到 TERM 信号，如果它们关闭的顺序很重要，则每个容器可能都需要一个 preStop 钩子。 （和第 3 步同步进行）从服务的endpoint列表中删除 Pod，Pod 也不再被视为副本控制器的运行状态的 Pod 集的一部分。因为负载均衡器（如服务代理）会将其从轮换中删除，所以缓慢关闭的 Pod 无法继续为流量提供服务。 当宽限期到期时，仍在 Pod 中运行的所有进程都会被SIGKILL(即kill -9)信号杀死。 ","date":"Apr 11, 2020","objectID":"/pod-graceful-lifecycle/:4:0","series":null,"tags":["k8s"],"title":"如何做一个优雅的Pod","uri":"/pod-graceful-lifecycle/#优雅的结束"},{"categories":["cloud"],"content":" 捕捉SIGTERM如果pod没有捕捉SIGTERM信号就直接退出，有些请求还没处理完，这势必影响服务质量，所以需要优雅退出，很多库都提供了类似的功能，当接受到退出信号时，清理空闲链接，等待当前请求处理完后再退出。如果善后工作较长，比较适当增加terminationGracePeriodSeconds的时间。 ","date":"Apr 11, 2020","objectID":"/pod-graceful-lifecycle/:4:1","series":null,"tags":["k8s"],"title":"如何做一个优雅的Pod","uri":"/pod-graceful-lifecycle/#捕捉sigterm"},{"categories":["cloud"],"content":" prestop另外也可以通过prestop设置hook操作，做一些额外的清理工作， apiVersion: v1 kind: Pod metadata: name: lifecycle-demo spec: containers: - name: lifecycle-demo-container image: nginx lifecycle: preStop: exec: command: [\"/bin/sh\",\"-c\",\"nginx -s quit; while killall -0 nginx; do sleep 1; done\"] 命令 preStop 负责优雅地终止 nginx 服务。当因为失效而导致容器终止时，这一处理方式很有用。 注 Kubernetes 只有在 Pod 结束（Terminated） 的时候才会发送 preStop 事件，这意味着在 Pod 完成（Completed） 时 preStop 的事件处理逻辑不会被触发。 ","date":"Apr 11, 2020","objectID":"/pod-graceful-lifecycle/:4:2","series":null,"tags":["k8s"],"title":"如何做一个优雅的Pod","uri":"/pod-graceful-lifecycle/#prestop"},{"categories":["cloud"],"content":" 总结优雅就不要怕麻烦，来我们总结下优雅的秘诀： 需要初始化的操作使用initcontainer来做 就绪检查，探活检查少不了,必要时也可以配置ReadinessGates 优雅退出要处理SIGTERM 需要时也可以设置下poststart, prestop 其他的，设置limit/reqeust也是必须的 ","date":"Apr 11, 2020","objectID":"/pod-graceful-lifecycle/:5:0","series":null,"tags":["k8s"],"title":"如何做一个优雅的Pod","uri":"/pod-graceful-lifecycle/#总结"},{"categories":["cloud"],"content":" 引用 https://kubernetes.io/zh/docs/concepts/workloads/pods/init-containers/ https://kubernetes.io/zh/docs/tasks/configure-pod-container/attach-handler-lifecycle-event/ ","date":"Apr 11, 2020","objectID":"/pod-graceful-lifecycle/:6:0","series":null,"tags":["k8s"],"title":"如何做一个优雅的Pod","uri":"/pod-graceful-lifecycle/#引用"},{"categories":["cloud"],"content":" 背景k8s中大多使用nginx-ingress-controller来实现ingress, 但是脆弱的nginx-controller通过ingress解析出nginx配置, 对于某些annotation会reload nignx配置失败, 然后controller就卡死了, 不断重启, 除非删除对应的ingress. ","date":"Apr 03, 2020","objectID":"/ingress-nginx-controller-admission-webhook/:1:0","series":null,"tags":["k8s","ingress"],"title":"nginx ingress controller 最后的倔强: admission webhook","uri":"/ingress-nginx-controller-admission-webhook/#背景"},{"categories":["cloud"],"content":" 问题复现创建有问题的ingress apiVersion: extensions/v1beta1 kind: Ingress metadata: annotations: nginx.ingress.kubernetes.io/auth-tls-pass-certificate-to-upstream: \"false\" nginx.ingress.kubernetes.io/auth-tls-verify-client: optional nginx.ingress.kubernetes.io/auth-tls-verify-depth: \"1\" nginx.ingress.kubernetes.io/configuration-snippet: | proxy_set_header Host $targethost; proxy_buffering off; proxy_pass http://$targetbackend; proxy_next_upstream error timeout invalid_header http_500 http_502 http_503 http_504; proxy_redirect off; proxy_set_header X-SSL-Client-Verify $ssl_client_verify; proxy_set_header X-SSL-Client-DN $ssl_client_s_dn; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; creationTimestamp: \"2020-03-23T04:57:22Z\" generation: 1 name: example-ingress namespace: kube-system resourceVersion: \"57681168\" selfLink: /apis/extensions/v1beta1/namespaces/kube-system/ingresses/example-ingress uid: c7f66385-6cc2-11ea-b6a8-246e96d4b538 spec: rules: - host: example.com http: paths: - backend: serviceName: example-svc servicePort: 8008 path: / tls: - hosts: - example.com secretName: example-tls status: loadBalancer: {} 查看nginx-ingress-controller状态全部为CrashLoopBackOff # kubectl get po -n kube-system -owide |grep ingress nginx-ingress-controller-ftfbg 1/2 CrashLoopBackOff 6 8m27s nginx-ingress-controller-hp4pf 1/2 CrashLoopBackOff 11 24m nginx-ingress-controller-qlb4l 1/2 CrashLoopBackOff 11 24m 查看nginx-ingress-controller日志, 显示reload失败\"proxy_pass\" directive is duplicate in /tmp/nginx-cfg911768424:822 ------------------------------------------------------------------------------- W0403 10:26:14.716246 1 queue.go:130] requeuing kube-system/nginx-ingress-controller-4txfk, err ------------------------------------------------------------------------------- Error: exit status 1 2020/04/03 10:26:14 [notice] 137#137: ModSecurity-nginx v1.0.0 2020/04/03 10:26:14 [warn] 137#137: duplicate value \"error\" in /tmp/nginx-cfg911768424:815 nginx: [warn] duplicate value \"error\" in /tmp/nginx-cfg911768424:815 2020/04/03 10:26:14 [warn] 137#137: duplicate value \"timeout\" in /tmp/nginx-cfg911768424:815 nginx: [warn] duplicate value \"timeout\" in /tmp/nginx-cfg911768424:815 2020/04/03 10:26:14 [emerg] 137#137: \"proxy_pass\" directive is duplicate in /tmp/nginx-cfg911768424:822 nginx: [emerg] \"proxy_pass\" directive is duplicate in /tmp/nginx-cfg911768424:822 nginx: configuration file /tmp/nginx-cfg911768424 test failed ------------------------------------------------------------------------------- W0403 10:26:16.998897 1 nginx_status.go:207] unexpected error obtaining nginx status info: unexpected error scraping nginx status page: unexpected error scraping nginx : Get http://0.0.0.0:18080/nginx_status: dial tcp 0.0.0.0:18080: connect: connection refused I0403 10:26:17.526801 1 main.go:167] Received SIGTERM, shutting down I0403 10:26:17.526827 1 nginx.go:364] Shutting down controller queues I0403 10:26:17.526845 1 status.go:200] updating status of Ingress rules (remove) I0403 10:26:17.537511 1 status.go:219] removing address from ingress status ([]) I0403 10:26:17.537593 1 nginx.go:372] Stopping NGINX process 2020/04/03 10:26:17 [notice] 141#141: signal process started I0403 10:26:20.547669 1 nginx.go:385] NGINX process has stopped I0403 10:26:20.547692 1 main.go:175] Handled quit, awaiting Pod deletion I0403 10:26:30.547824 1 main.go:178] Exiting with 0 ","date":"Apr 03, 2020","objectID":"/ingress-nginx-controller-admission-webhook/:1:1","series":null,"tags":["k8s","ingress"],"title":"nginx ingress controller 最后的倔强: admission webhook","uri":"/ingress-nginx-controller-admission-webhook/#问题复现"},{"categories":["cloud"],"content":" 解决方案创建一个有问题的ingress, 会影响所有新创建的ingress规则, 又一个集群级别的Bug诞生了.那么有没有办法, 提前检验ingress配置, 有问题就不去reload. 那验证步骤肯定要在请求到达nginx-controller之前来做, 是不是想到了k8s-admission-webhook, 可以在apiserver持久化对象前拦截请求, 去实现自定义的验证规则. 好在新版本的nginx-ingress-controller(v0.25.0+)已经实现了相关的功能, 只需开启对应配置就行. ","date":"Apr 03, 2020","objectID":"/ingress-nginx-controller-admission-webhook/:2:0","series":null,"tags":["k8s","ingress"],"title":"nginx ingress controller 最后的倔强: admission webhook","uri":"/ingress-nginx-controller-admission-webhook/#解决方案"},{"categories":["cloud"],"content":" ApiServer配置Apiserver开启webhook相关配置, 必须包含MutatingAdmissionWebhook与ValidatingAdmissionWebhook --admission-control=MutatingAdmissionWebhook,ValidatingAdmissionWebhook ","date":"Apr 03, 2020","objectID":"/ingress-nginx-controller-admission-webhook/:2:1","series":null,"tags":["k8s","ingress"],"title":"nginx ingress controller 最后的倔强: admission webhook","uri":"/ingress-nginx-controller-admission-webhook/#apiserver配置"},{"categories":["cloud"],"content":" 创建webhook相关配置启用ValidatingAdmissionWebhook必须使用https, 需要配置对应证书 手动生成: openssl req -x509 -newkey rsa:2048 -keyout certificate.pem -out key.pem -days 365 -nodes -subj \"/CN=ingress-validation-webhook.ingress-nginx.svc\" CertificateSigningRequest 通过k8s CertificateSigningRequest来创建(controller-manager需要开启--cluster-signing-cert-file与--cluster-signing-key-file) 可通过如下脚本创建, namespace与service替换成自己的 SERVICE_NAME=ingress-nginx NAMESPACE=ingress-nginx TEMP_DIRECTORY=$(mktemp -d) echo \"creating certs in directory ${TEMP_DIRECTORY}\" cat \u003c\u003cEOF \u003e\u003e ${TEMP_DIRECTORY}/csr.conf [req] req_extensions = v3_req distinguished_name = req_distinguished_name [req_distinguished_name] [ v3_req ] basicConstraints = CA:FALSE keyUsage = nonRepudiation, digitalSignature, keyEncipherment extendedKeyUsage = serverAuth subjectAltName = @alt_names [alt_names] DNS.1 = ${SERVICE_NAME} DNS.2 = ${SERVICE_NAME}.${NAMESPACE} DNS.3 = ${SERVICE_NAME}.${NAMESPACE}.svc EOF openssl genrsa -out ${TEMP_DIRECTORY}/server-key.pem 2048 openssl req -new -key ${TEMP_DIRECTORY}/server-key.pem \\ -subj \"/CN=${SERVICE_NAME}.${NAMESPACE}.svc\" \\ -out ${TEMP_DIRECTORY}/server.csr \\ -config ${TEMP_DIRECTORY}/csr.conf cat \u003c\u003cEOF | kubectl create -f - apiVersion: certificates.k8s.io/v1beta1 kind: CertificateSigningRequest metadata: name: ${SERVICE_NAME}.${NAMESPACE}.svc spec: request: $(cat ${TEMP_DIRECTORY}/server.csr | base64 | tr -d '\\n') usages: - digital signature - key encipherment - server auth EOF kubectl certificate approve ${SERVICE_NAME}.${NAMESPACE}.svc for x in $(seq 10); do SERVER_CERT=$(kubectl get csr ${SERVICE_NAME}.${NAMESPACE}.svc -o jsonpath='{.status.certificate}') if [[ ${SERVER_CERT} != '' ]]; then break fi sleep 1 done if [[ ${SERVER_CERT} == '' ]]; then echo \"ERROR: After approving csr ${SERVICE_NAME}.${NAMESPACE}.svc, the signed certificate did not appear on the resource. Giving up after 10 attempts.\" \u003e\u00262 exit 1 fi echo ${SERVER_CERT} | openssl base64 -d -A -out ${TEMP_DIRECTORY}/server-cert.pem kubectl create secret generic ingress-nginx.svc \\ --from-file=key.pem=${TEMP_DIRECTORY}/server-key.pem \\ --from-file=cert.pem=${TEMP_DIRECTORY}/server-cert.pem \\ -n ${NAMESPACE} ","date":"Apr 03, 2020","objectID":"/ingress-nginx-controller-admission-webhook/:2:2","series":null,"tags":["k8s","ingress"],"title":"nginx ingress controller 最后的倔强: admission webhook","uri":"/ingress-nginx-controller-admission-webhook/#创建webhook相关配置"},{"categories":["cloud"],"content":" 配置ingress controlleringress controller需要启用如下参数, 挂载需要的tls证书 flag description example usage --validating-webhook admission webhook的地址 :8080 --validating-webhook-certificate webhook证书 /usr/local/certificates/validating-webhook.pem --validating-webhook-key webhook私钥 /usr/local/certificates/validating-webhook-key.pem ","date":"Apr 03, 2020","objectID":"/ingress-nginx-controller-admission-webhook/:2:3","series":null,"tags":["k8s","ingress"],"title":"nginx ingress controller 最后的倔强: admission webhook","uri":"/ingress-nginx-controller-admission-webhook/#配置ingress-controller"},{"categories":["cloud"],"content":" 验证更新后, 创建有问题的ingress则会拦截, 符合预期 # kubectl apply -f ing.yaml Error from server: error when creating \"ing.yaml\": admission webhook \"validate.nginx.ingress.kubernetes.io\" denied the request: ------------------------------------------------------------------------------- Error: exit status 1 2020/04/02 10:26:04 [emerg] 331#331: directive \"proxy_pass\" is not terminated by \";\" in /tmp/nginx-cfg461116913:2165 nginx: [emerg] directive \"proxy_pass\" is not terminated by \";\" in /tmp/nginx-cfg461116913:2165 nginx: configuration file /tmp/nginx-cfg461116913 test failed ","date":"Apr 03, 2020","objectID":"/ingress-nginx-controller-admission-webhook/:3:0","series":null,"tags":["k8s","ingress"],"title":"nginx ingress controller 最后的倔强: admission webhook","uri":"/ingress-nginx-controller-admission-webhook/#验证"},{"categories":["cloud"],"content":" 引用 https://kubernetes.io/zh/docs/reference/access-authn-authz/extensible-admission-controllers/ https://kubernetes.github.io/ingress-nginx/deploy/validating-webhook/ ","date":"Apr 03, 2020","objectID":"/ingress-nginx-controller-admission-webhook/:4:0","series":null,"tags":["k8s","ingress"],"title":"nginx ingress controller 最后的倔强: admission webhook","uri":"/ingress-nginx-controller-admission-webhook/#引用"},{"categories":["cloud"],"content":" 背景今天在k8s更新服务时,发现pod启动失败,报错failed to start sandbox container,如下所示: Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Scheduled 28m default-scheduler Successfully assigned kube-system/k8s-proxy-7wkt4 to tj1-staging-com-ocean007-201812.kscn Warning FailedCreatePodSandBox 28m (x13 over 28m) kubelet, tj1-staging-com-ocean007-201812.kscn Failed create pod sandbox: rpc error: code = Unknown desc = failed to start sandbox container for pod \"k8s-proxy-7wkt4\": Error response from daemon: OCI runtime create failed: container_linux.go:345: starting container process caused \"process_linux.go:297: getting the final child's pid from pipe caused \\\"EOF\\\"\": unknown Normal SandboxChanged 3m19s (x1364 over 28m) kubelet, tj1-staging-com-ocean007-201812.kscn Pod sandbox changed, it will be killed and re-created. ","date":"Mar 18, 2020","objectID":"/pod-sandbox-recreated/:1:0","series":null,"tags":["k8s","docker"],"title":"pod sandbox 创建失败","uri":"/pod-sandbox-recreated/#背景"},{"categories":["cloud"],"content":" 分析sandbox 创建失败只是表象,是宿主机其他异常导致的,一般是(cpu,diskio,mem)导致的. 首先,上节点看kubelet,docker有无异常,日志没有明显错误,通过top看到docker cpu占用非常高 [root@tj1-staging-com-ocean007-201812 ~]# top top - 17:55:00 up 265 days, 3:41, 1 user, load average: 10.71, 11.34, 10.76 Tasks: 816 total, 5 running, 811 sleeping, 0 stopped, 0 zombie %Cpu(s): 24.0 us, 34.5 sy, 0.0 ni, 41.4 id, 0.0 wa, 0.0 hi, 0.1 si, 0.0 st KiB Mem : 65746380 total, 20407940 free, 11007040 used, 34331400 buff/cache KiB Swap: 0 total, 0 free, 0 used. 49134416 avail Mem PID USER PR NI VIRT RES SHR S %CPU %MEM TIME+ COMMAND 115483 root 20 0 3965212 273188 34564 S 489.7 0.4 382260:40 dockerd 1367523 root 20 0 18376 2972 2716 R 66.9 0.0 20163:45 bash 1367487 root 20 0 11856 5616 4512 S 54.0 0.0 16748:26 containerd-shim 3200169 root 20 0 1300 4 0 R 53.3 0.0 14913:49 sh 2429952 root 20 0 1300 4 0 S 49.3 0.0 9620:56 sh 3200130 root 20 0 9392 4756 3884 S 47.7 0.0 13417:30 containerd-shim 3718475 root 20 0 1300 4 0 R 47.4 0.0 8600:20 sh 3718440 root 20 0 10736 5516 4512 S 42.1 0.0 7575:31 containerd-shim 2429917 root 20 0 11856 5556 4512 S 40.1 0.0 8313:22 containerd-shim 3205493 root 20 0 3775924 230996 66704 S 18.9 0.4 2559:07 kubelet 1 root 20 0 195240 157000 3932 S 7.9 0.2 1417:46 systemd 804 dbus 20 0 30308 6460 2464 S 1.7 0.0 462:18.84 dbus-daemon 1011737 root 20 0 277656 122788 18428 S 1.3 0.2 768:03.00 cadvisor 115508 root 20 0 7139200 32896 24288 S 1.0 0.1 662:25.27 containerd 806 root 20 0 24572 3060 2480 S 0.7 0.0 171:22.52 systemd-logind 511080 root 0 -20 2751348 52552 15744 S 0.7 0.1 178:27.51 sagent 1102507 root 20 0 11792 7292 4512 S 0.7 0.0 23:36.37 containerd-shim 1272223 root 20 0 164800 5296 3824 R 0.7 0.0 0:00.38 top 2866292 root 20 0 5045000 1.983g 3080 S 0.7 3.2 230:09.47 redis 同时, cpu system异常高. %Cpu(s): 24.0 us, 34.5 sy, 0.0 ni, 41.4 id, 0.0 wa, 0.0 hi, 0.1 si, 0.0 st 按照以前的经验,一般是由某些容器引起的,通过top看到个别sh进程占用cpu较高. 通过ps看到进程居然是个死循环 [root@tj1-staging-com-ocean007-201812 ~]# ps -ef |grep 1367523 root 1287628 1247781 0 17:55 pts/1 00:00:00 grep --color=auto 1367523 root 1367523 1367504 72 Feb28 ? 14-00:04:17 /bin/bash -c while true; do echo hello; done 通过/proc/pid/cgroup找到对应容器 # cat /proc/1367523/cgroup 11:freezer:/kubepods/besteffort/pod55d3adf2-67f7-11ea-93f2-246e968203b8/29842d5544b701dbb5ff647dba19bb4ebec821edc6ee1ffbd7aeee58fa5038fd 10:devices:/kubepods/besteffort/pod55d3adf2-67f7-11ea-93f2-246e968203b8/29842d5544b701dbb5ff647dba19bb4ebec821edc6ee1ffbd7aeee58fa5038fd 9:hugetlb:/kubepods/besteffort/pod55d3adf2-67f7-11ea-93f2-246e968203b8/29842d5544b701dbb5ff647dba19bb4ebec821edc6ee1ffbd7aeee58fa5038fd 8:blkio:/kubepods/besteffort/pod55d3adf2-67f7-11ea-93f2-246e968203b8/29842d5544b701dbb5ff647dba19bb4ebec821edc6ee1ffbd7aeee58fa5038fd 7:memory:/kubepods/besteffort/pod55d3adf2-67f7-11ea-93f2-246e968203b8/29842d5544b701dbb5ff647dba19bb4ebec821edc6ee1ffbd7aeee58fa5038fd 6:perf_event:/kubepods/besteffort/pod55d3adf2-67f7-11ea-93f2-246e968203b8/29842d5544b701dbb5ff647dba19bb4ebec821edc6ee1ffbd7aeee58fa5038fd 5:cpuset:/kubepods/besteffort/pod55d3adf2-67f7-11ea-93f2-246e968203b8/29842d5544b701dbb5ff647dba19bb4ebec821edc6ee1ffbd7aeee58fa5038fd 4:pids:/kubepods/besteffort/pod55d3adf2-67f7-11ea-93f2-246e968203b8/29842d5544b701dbb5ff647dba19bb4ebec821edc6ee1ffbd7aeee58fa5038fd 3:net_cls,net_prio:/kubepods/besteffort/pod55d3adf2-67f7-11ea-93f2-246e968203b8/29842d5544b701dbb5ff647dba19bb4ebec821edc6ee1ffbd7aeee58fa5038fd 2:cpu,cpuacct:/kubepods/besteffort/pod55d3adf2-67f7-11ea-93f2-246e968203b8/29842d5544b701dbb5ff647dba19bb4ebec821edc6ee1ffbd7aeee58fa5038fd 1:name=systemd:/kubepods/besteffort/pod55d3adf2-67f7-11ea-93f2-246e968203b8/29842d5544b701dbb5ff647dba19bb4ebec821edc6ee1ffbd7aeee58fa5038fd 找到对应容器 docker ps | grep 29842d554 清理完相关pod后,系统恢复正常 top - 18:25:57 up 265 days, 4:12, 1 user, load average: 1.05, 1.24, 4.02 Tasks: 769 total, 1 running, 768 sleeping, 0 stopped, 0 zombie %Cpu(s): 1.7 us, 0.9 sy, 0.0 ni, 97.3 id, 0.0 wa, 0.0 hi, 0.0 si, 0.0 st KiB Mem : 65746380 total, 22106960 free, 10759860 us","date":"Mar 18, 2020","objectID":"/pod-sandbox-recreated/:2:0","series":null,"tags":["k8s","docker"],"title":"pod sandbox 创建失败","uri":"/pod-sandbox-recreated/#分析"},{"categories":["cloud"],"content":" 总结sandox创建失败的原因是各种各样的, 如[memory设置错误触发的异常][1],[dockerd异常][2]. 针对此处问题是由于某些测试pod通过while true; do echo hello; done启动,死循环一直echo hello产生大量read()系统调用,所在cpu飙升.多个类似pod导致系统非常繁忙,无法正常处理其他请求. 此类问题不容易在pod创建时直接检测到,只能通过添加物理节点相关报警(dockerd cpu使用率, node cpu.sys使用率)及时发现问题. ","date":"Mar 18, 2020","objectID":"/pod-sandbox-recreated/:3:0","series":null,"tags":["k8s","docker"],"title":"pod sandbox 创建失败","uri":"/pod-sandbox-recreated/#总结"},{"categories":["cloud"],"content":" 引用 https://github.com/kubernetes/kubernetes/issues/56996 https://plugaru.org/2018/05/21/pod-sandbox-changed-it-will-be-killed-and-re-created/ ","date":"Mar 18, 2020","objectID":"/pod-sandbox-recreated/:4:0","series":null,"tags":["k8s","docker"],"title":"pod sandbox 创建失败","uri":"/pod-sandbox-recreated/#引用"},{"categories":["tool"],"content":"一直以来使用Ubuntu开发，前两天Ubuntu桌面环境崩了，一些工作软件在Ubuntu下很不好用，恰好WSL2(Windows Linux子系统)发布已经有一段日子，而且支持了Docker，上手看看可用性如何。 ","date":"Feb 11, 2020","objectID":"/wsl2-vscode/:0:0","series":null,"tags":["linux","wsl"],"title":"WSL2+VSCode+Zsh打造Windows下Linux开发环境","uri":"/wsl2-vscode/#"},{"categories":["tool"],"content":" 配置WSL2","date":"Feb 11, 2020","objectID":"/wsl2-vscode/:1:0","series":null,"tags":["linux","wsl"],"title":"WSL2+VSCode+Zsh打造Windows下Linux开发环境","uri":"/wsl2-vscode/#配置wsl2"},{"categories":["tool"],"content":" 必要条件 Windows 10 Build 18917或更新版本 启用虚拟化 ","date":"Feb 11, 2020","objectID":"/wsl2-vscode/:1:1","series":null,"tags":["linux","wsl"],"title":"WSL2+VSCode+Zsh打造Windows下Linux开发环境","uri":"/wsl2-vscode/#必要条件"},{"categories":["tool"],"content":" 安装步骤 启用“虚拟机平台”可选组件，以管理员身份打开 PowerShell 并运行： Enable-WindowsOptionalFeature -Online -FeatureName VirtualMachinePlatform 启用安装子系统 Enable-WindowsOptionalFeature -Online -FeatureName Microsoft-Windows-Subsystem-Linux 启用这些更改后，你需要重新启动计算机。 应用商店安装ubuntu，如Ubuntu-18.04 使用命令行设置要由 WSL 2 支持的发行版，在 PowerShell 中运行： wsl --set-version \u003cDistro\u003e 2 ","date":"Feb 11, 2020","objectID":"/wsl2-vscode/:1:2","series":null,"tags":["linux","wsl"],"title":"WSL2+VSCode+Zsh打造Windows下Linux开发环境","uri":"/wsl2-vscode/#安装步骤"},{"categories":["tool"],"content":" 配置Ubuntu配置源，配置Sudo免密码，安装必要软件Python、Git、Docker等，终端美化可通过安装Zsh… ","date":"Feb 11, 2020","objectID":"/wsl2-vscode/:1:3","series":null,"tags":["linux","wsl"],"title":"WSL2+VSCode+Zsh打造Windows下Linux开发环境","uri":"/wsl2-vscode/#配置ubuntu"},{"categories":["tool"],"content":" 安装VSCode WSL插件VSCode已经支持了WSL插件 最终界面如下： ","date":"Feb 11, 2020","objectID":"/wsl2-vscode/:2:0","series":null,"tags":["linux","wsl"],"title":"WSL2+VSCode+Zsh打造Windows下Linux开发环境","uri":"/wsl2-vscode/#安装vscode-wsl插件"},{"categories":["tool"],"content":" 总结可以愉快的使用VSCode开发，目前也发现了几点小问题： Vscode Terminal改为WSL后，启动会有1-2秒延时 WSL2中的软件配置开机自启比较麻烦，网上有方案，我是通过快捷命令如启动 Docker alias sds=\"sudo service docker start\" WSL2本质是个虚拟机，网络方式和本地有一定差异，对我来说影响不大 目前在家办公已两周，此方案感觉良好。 ","date":"Feb 11, 2020","objectID":"/wsl2-vscode/:3:0","series":null,"tags":["linux","wsl"],"title":"WSL2+VSCode+Zsh打造Windows下Linux开发环境","uri":"/wsl2-vscode/#总结"},{"categories":["cloud"],"content":" 前言目前Gitlab11已经支持了Kubernetes Runner, 任务可以跑在Pod中。本文介绍如何通过CICD接入Kubernetes，开始前需要以下必备条件： Kubernetes集群 配置Kubernetes Runner, 网上有很多教程，若是生产环境或是多租户k8s集群，建议通过yaml手动配置；默认通过helm安装权限比较大，而且配置不灵活 ","date":"Feb 11, 2020","objectID":"/k8s-gitlab-cicd/:1:0","series":null,"tags":["k8s","gitlab","cicd"],"title":"k8s+gitlab实现cicd","uri":"/k8s-gitlab-cicd/#前言"},{"categories":["cloud"],"content":" CI过程通常编译镜像有三种方式： docker in docker：与物理方式类似，需要权限多，性能较差 kaniko：镜像编译工具，性能好 我们使用kaniko编译镜像，push到镜像仓库，过程如下： 配置变量 配置镜像相关变量，仓库的账户密码，推送的镜像名称CI_REGISTRY_IMAGE等 gitlab-ci配置如下 build: stage: build image: name: gcr.io/kaniko-project/executor:debug entrypoint: [\"\"] script: - echo \"{\\\"auths\\\":{\\\"$CI_REGISTRY\\\":{\\\"username\\\":\\\"$CI_REGISTRY_USER\\\",\\\"password\\\":\\\"$CI_REGISTRY_PASSWORD\\\"}}}\" \u003e /kaniko/.docker/config.json - /kaniko/executor --context $CI_PROJECT_DIR --dockerfile $CI_PROJECT_DIR/Dockerfile --destination $CI_REGISTRY_IMAGE:$CI_COMMIT_TAG after_script: - echo \"build completed\" only: - tags # 打tag才会执行，测试可去掉 ","date":"Feb 11, 2020","objectID":"/k8s-gitlab-cicd/:2:0","series":null,"tags":["k8s","gitlab","cicd"],"title":"k8s+gitlab实现cicd","uri":"/k8s-gitlab-cicd/#ci过程"},{"categories":["cloud"],"content":" CD过程CD即需要将生成的镜像更新到Kubernetes集群中，有如下几种方式： k8s restful api：需要对api较了解，更新过程需要调用PATH方法，不推荐 kubectl: 常规方式 helm: 如有可用的helm仓库，也可使用helm进行更新 我们以kubectl为例，CD配置如下： 配置变量 配置必须的集群地址，token，需要更新服务的namespace, container等 CD配置 配置与物理环境类似，首先配置kubectl token、集群等，最后调用set image更新服务 deploy: image: name: kubectl:1.17 entrypoint: [\"\"] before_script: script: - IMAGE=$CI_REGISTRY_IMAGE:$CI_COMMIT_SHORT_SHA - kubectl config set-credentials $CD_USER --token $CD_APP_AK - kubectl config set-cluster $CD_CLUSTER --server https://$CD_SERVER - kubectl config set-context $CD_USER@$CD_CLUSTER/$CD_NAMESPACE --user $CD_USER --cluster $CD_CLUSTER --namespace $CD_NAMESPACE - kubectl config use-context $CD_USER@$CD_CLUSTER/$CD_NAMESPACE - kubectl set image -n $CD_NAMESPACE $CD_APP_TYPE/$CD_APP_NAME $CD_CONTAINER=$IMAGE only: - tags 运行结果 $ kubectl set image -n $CD_NAMESPACE $CD_APP_TYPE/$CD_APP_NAME $CD_CONTAINER=$IMAGE deployment.extensions/helloworld image updated Job succeeded ","date":"Feb 11, 2020","objectID":"/k8s-gitlab-cicd/:3:0","series":null,"tags":["k8s","gitlab","cicd"],"title":"k8s+gitlab实现cicd","uri":"/k8s-gitlab-cicd/#cd过程"},{"categories":["cloud"],"content":" 备注本文所列举的CICD过程较简单，可以使用CICD完成服务的多集群部署，更新结果检查等功能。 ","date":"Feb 11, 2020","objectID":"/k8s-gitlab-cicd/:4:0","series":null,"tags":["k8s","gitlab","cicd"],"title":"k8s+gitlab实现cicd","uri":"/k8s-gitlab-cicd/#备注"},{"categories":["cloud"],"content":" 参考 https://docs.gitlab.com/ee/ci/docker/using_kaniko.html https://docs.gitlab.com/runner/executors/kubernetes.html ","date":"Feb 11, 2020","objectID":"/k8s-gitlab-cicd/:5:0","series":null,"tags":["k8s","gitlab","cicd"],"title":"k8s+gitlab实现cicd","uri":"/k8s-gitlab-cicd/#参考"},{"categories":["杂记"],"content":"一轮明月沉到了脸底 迷恋这红尘 不愿睡去 又变成红娘的大痣 在招摇过市 有了糖 她变得小心翼翼 吃饭睡觉也含着久久不肯放 是通灵宝玉 守护着片刻荣光 亦或是君子之遗 化作脸颊的相思 没人知道它的滋味 除了耳边的风 远方的惦记 以及轻轻拂过的吻 ","date":"Feb 02, 2020","objectID":"/candy/:0:0","series":null,"tags":["诗"],"title":"糖","uri":"/candy/#"},{"categories":["cloud"],"content":" 简介kube-apiserver中与权限相关的主要有三种机制，即认证、鉴权和准入控制。本文主要分析apiserver的认证流程。 ","date":"Jan 31, 2020","objectID":"/kube-apiserver-authentication-code/:1:0","series":null,"tags":["k8s"],"title":"kube-apiserver认证源码分析","uri":"/kube-apiserver-authentication-code/#简介"},{"categories":["cloud"],"content":" 认证流程分析权限相关代码从k8s.io/apiserver/pkg/server/config.go中DefaultBuildHandlerChain函数开始执行 func DefaultBuildHandlerChain(apiHandler http.Handler, c *Config) http.Handler { handler := genericapifilters.WithAuthorization(apiHandler, c.Authorization.Authorizer, c.Serializer) handler = genericfilters.WithMaxInFlightLimit(handler, c.MaxRequestsInFlight, c.MaxMutatingRequestsInFlight, c.LongRunningFunc) handler = genericapifilters.WithImpersonation(handler, c.Authorization.Authorizer, c.Serializer) handler = genericapifilters.WithAudit(handler, c.AuditBackend, c.AuditPolicyChecker, c.LongRunningFunc) failedHandler := genericapifilters.Unauthorized(c.Serializer, c.Authentication.SupportsBasicAuth) failedHandler = genericapifilters.WithFailedAuthenticationAudit(failedHandler, c.AuditBackend, c.AuditPolicyChecker) handler = genericapifilters.WithAuthentication(handler, c.Authentication.Authenticator, failedHandler, c.Authentication.APIAudiences) handler = genericfilters.WithCORS(handler, c.CorsAllowedOriginList, nil, nil, nil, \"true\") handler = genericfilters.WithTimeoutForNonLongRunningRequests(handler, c.LongRunningFunc, c.RequestTimeout) handler = genericfilters.WithWaitGroup(handler, c.LongRunningFunc, c.HandlerChainWaitGroup) handler = genericapifilters.WithRequestInfo(handler, c.RequestInfoResolver) handler = genericfilters.WithPanicRecovery(handler) return handler } DefaultBuildHandlerChain中包含了多种filter（如认证，链接数检验，RBAC权限检验等），认证步骤在WithAuthorization中，如下： // WithAuthentication creates an http handler that tries to authenticate the given request as a user, and then // stores any such user found onto the provided context for the request. If authentication fails or returns an error // the failed handler is used. On success, \"Authorization\" header is removed from the request and handler // is invoked to serve the request. func WithAuthentication(handler http.Handler, auth authenticator.Request, failed http.Handler, apiAuds authenticator.Audiences) http.Handler { if auth == nil { klog.Warningf(\"Authentication is disabled\") return handler } return http.HandlerFunc(func(w http.ResponseWriter, req *http.Request) { authenticationStart := time.Now() if len(apiAuds) \u003e 0 { req = req.WithContext(authenticator.WithAudiences(req.Context(), apiAuds)) } // 认证请求 resp, ok, err := auth.AuthenticateRequest(req) if err != nil || !ok { if err != nil { klog.Errorf(\"Unable to authenticate the request due to an error: %v\", err) authenticatedAttemptsCounter.WithLabelValues(errorLabel).Inc() authenticationLatency.WithLabelValues(errorLabel).Observe(time.Since(authenticationStart).Seconds()) } else if !ok { authenticatedAttemptsCounter.WithLabelValues(failureLabel).Inc() authenticationLatency.WithLabelValues(failureLabel).Observe(time.Since(authenticationStart).Seconds()) } failed.ServeHTTP(w, req) return } if len(apiAuds) \u003e 0 \u0026\u0026 len(resp.Audiences) \u003e 0 \u0026\u0026 len(authenticator.Audiences(apiAuds).Intersect(resp.Audiences)) == 0 { klog.Errorf(\"Unable to match the audience: %v , accepted: %v\", resp.Audiences, apiAuds) failed.ServeHTTP(w, req) return } // authorization header is not required anymore in case of a successful authentication. // 认证完则删除header认证信息，exec/log请求将不会携带Authorization，只使用token认证将无法通过 req.Header.Del(\"Authorization\") req = req.WithContext(genericapirequest.WithUser(req.Context(), resp.User)) authenticatedUserCounter.WithLabelValues(compressUsername(resp.User.GetName())).Inc() authenticatedAttemptsCounter.WithLabelValues(successLabel).Inc() authenticationLatency.WithLabelValues(successLabel).Observe(time.Since(authenticationStart).Seconds()) handler.ServeHTTP(w, req) }) } WithAuthentication调用AuthenticateRequest进行认证： // AuthenticateRequest authenticates the request using a chain of authenticator.Request objects. func (authHandler *unionAuthRequestHandler) AuthenticateRequest(req *http.Request) (*authenticator.Response, bool, error) { var errlist []error // 按照Handlers顺序进行认证 for _, currAuthRequestHandler := range authHandler.Handlers { resp, ok, err := currAuthReq","date":"Jan 31, 2020","objectID":"/kube-apiserver-authentication-code/:2:0","series":null,"tags":["k8s"],"title":"kube-apiserver认证源码分析","uri":"/kube-apiserver-authentication-code/#认证流程分析"},{"categories":["cloud"],"content":" 总结Apiserver的认证方式有多种，通过源码分析每次请求都会安装固定的认证顺序执行，高qps下认证配置势必会影响Apiserver的响应延迟，需要根据集群的实际情况配置合理的认证方式。 目前在我们的线上系统，主要通过RequestHeader(认证普通用户)，基本认证(个别系统组件)，X509（认证kubelet），ServieceAccout（认证Pod）进行认证，仅供参考。 ","date":"Jan 31, 2020","objectID":"/kube-apiserver-authentication-code/:3:0","series":null,"tags":["k8s"],"title":"kube-apiserver认证源码分析","uri":"/kube-apiserver-authentication-code/#总结"},{"categories":["cloud"],"content":" 简介k8s中为了实现高可用，需要部署多个副本，例如多个apiserver、scheduler、controller-manager等，其中apiserver是无状态的每个组件都可以工作，而scheduler与controller-manager是有状态的，同一时刻只能存在一个活跃的，需要进行选主。 k8s使用了资源锁（endpoints/configmap/lease）的方式来实现选主，多个副本去创建资源，创建成功则获得锁成为leader，leader在租约内去刷新锁，其他副本则通过比对锁的更新时间判断是否成为新的leader。 k8s采用了资源版本号的乐观锁方式来实现选主，对比etcd选主，效率更高，并发性更好。 ","date":"Dec 27, 2019","objectID":"/k8s-leaderelection-code/:1:0","series":null,"tags":["k8s"],"title":"k8s基于资源锁的选主分析","uri":"/k8s-leaderelection-code/#简介"},{"categories":["cloud"],"content":" 源码分析k8s选主实现在client-go中，包k8s.io/client-go/tools/leaderelection ","date":"Dec 27, 2019","objectID":"/k8s-leaderelection-code/:2:0","series":null,"tags":["k8s"],"title":"k8s基于资源锁的选主分析","uri":"/k8s-leaderelection-code/#源码分析"},{"categories":["cloud"],"content":" 结构定义锁结构定义如下： // k8s.io/client-go/tools/leaderelection/resourcelock/interface.go type LeaderElectionRecord struct { // leader 标识，通常为 hostname HolderIdentity string `json:\"holderIdentity\"` // 同启动参数 --leader-elect-lease-duration LeaseDurationSeconds int `json:\"leaseDurationSeconds\"` // Leader 第一次成功获得租约时的时间戳 AcquireTime unversioned.Time `json:\"acquireTime\"` // leader 定时 renew 的时间戳 RenewTime unversioned.Time `json:\"renewTime\"` LeaderTransitions int `json:\"leaderTransitions\"` } k8s中的选举锁需实现resourcelock.Interface接口，基本上实现CRU，将leader信息存在在annotation中 // k8s.io/client-go/tools/leaderelection/resourcelock/interface.go type Interface interface { // Get returns the LeaderElectionRecord Get() (*LeaderElectionRecord, []byte, error) // Create attempts to create a LeaderElectionRecord Create(ler LeaderElectionRecord) error // Update will update and existing LeaderElectionRecord Update(ler LeaderElectionRecord) error // RecordEvent 记录锁切换事件 RecordEvent(string) // Identity will return the locks Identity Identity() string // Describe is used to convert details on current resource lock // into a string Describe() string } ","date":"Dec 27, 2019","objectID":"/k8s-leaderelection-code/:2:1","series":null,"tags":["k8s"],"title":"k8s基于资源锁的选主分析","uri":"/k8s-leaderelection-code/#结构定义"},{"categories":["cloud"],"content":" 创建资源锁锁类型包括：configmaps， endpoints, lease, 以及 multiLock // k8s.io/client-go/tools/leaderelection/resourcelock/interface.go func New(lockType string, ns string, name string, coreClient corev1.CoreV1Interface, coordinationClient coordinationv1.CoordinationV1Interface, rlc ResourceLockConfig) (Interface, error) { endpointsLock := \u0026EndpointsLock{ EndpointsMeta: metav1.ObjectMeta{ Namespace: ns, Name: name, }, Client: coreClient, LockConfig: rlc, } configmapLock := \u0026ConfigMapLock{ ConfigMapMeta: metav1.ObjectMeta{ Namespace: ns, Name: name, }, Client: coreClient, LockConfig: rlc, } leaseLock := \u0026LeaseLock{ LeaseMeta: metav1.ObjectMeta{ Namespace: ns, Name: name, }, Client: coordinationClient, LockConfig: rlc, } switch lockType { case EndpointsResourceLock: return endpointsLock, nil case ConfigMapsResourceLock: return configmapLock, nil case LeasesResourceLock: return leaseLock, nil case EndpointsLeasesResourceLock: return \u0026MultiLock{ Primary: endpointsLock, Secondary: leaseLock, }, nil case ConfigMapsLeasesResourceLock: return \u0026MultiLock{ Primary: configmapLock, Secondary: leaseLock, }, nil default: return nil, fmt.Errorf(\"Invalid lock-type %s\", lockType) } } 使用者首先通过new()函数创建资源锁，需要提供锁类型、namespace、name、唯一标示等。 ","date":"Dec 27, 2019","objectID":"/k8s-leaderelection-code/:2:2","series":null,"tags":["k8s"],"title":"k8s基于资源锁的选主分析","uri":"/k8s-leaderelection-code/#创建资源锁"},{"categories":["cloud"],"content":" 进行选举创建选举配置，通常如下： // start the leader election code loop leaderelection.RunOrDie(ctx, leaderelection.LeaderElectionConfig{ // 资源锁类型 Lock: lock, // 租约时长，非主候选者用来判断资源锁是否过期 LeaseDuration: 60 * time.Second, // leader刷新资源锁超时时间 RenewDeadline: 15 * time.Second, // 调用资源锁间隔 RetryPeriod: 5 * time.Second, // 回调函数，根据选举不同事件触发 Callbacks: leaderelection.LeaderCallbacks{ OnStartedLeading: func(ctx context.Context) { run(ctx) }, OnStoppedLeading: func() { klog.Infof(\"leader lost: %s\", id) os.Exit(0) // 必须要退出，重启开始选主，否则将不会参与到选主中 }, OnNewLeader: func(identity string) { if identity == id { return } klog.Infof(\"new leader elected: %s\", identity) }, }, }) 创建选举对象后，执行Run函数开始选主 // k8s.io/client-go/tools/leaderelection/leaderelection.go // Run starts the leader election loop func (le *LeaderElector) Run(ctx context.Context) { defer func() { runtime.HandleCrash() // 锁丢失时执行OnStoppedLeading回调函数 le.config.Callbacks.OnStoppedLeading() }() // 尝试获得锁 if !le.acquire(ctx) { return // ctx signalled done } ctx, cancel := context.WithCancel(ctx) defer cancel() // 获得锁后执行OnStartedLeading回调函数 go le.config.Callbacks.OnStartedLeading(ctx) // 定期刷新锁 le.renew(ctx) } acruire方法： // k8s.io/client-go/tools/leaderelection/leaderelection.go // acquire loops calling tryAcquireOrRenew and returns true immediately when tryAcquireOrRenew succeeds. // Returns false if ctx signals done. func (le *LeaderElector) acquire(ctx context.Context) bool { ctx, cancel := context.WithCancel(ctx) defer cancel() succeeded := false desc := le.config.Lock.Describe() klog.Infof(\"attempting to acquire leader lease %v...\", desc) // 调用 JitterUntil 函数，以 RetryPeriod 为间隔去刷新资源锁，直到获取锁 wait.JitterUntil(func() { // tryAcquireOrRenew 方法去调度资源更新接口，判断是否刷新成功 succeeded = le.tryAcquireOrRenew() le.maybeReportTransition() if !succeeded { klog.V(4).Infof(\"failed to acquire lease %v\", desc) return } le.config.Lock.RecordEvent(\"became leader\") le.metrics.leaderOn(le.config.Name) klog.Infof(\"successfully acquired lease %v\", desc) cancel() }, le.config.RetryPeriod, JitterFactor, true, ctx.Done()) return succeeded } renew方法，只有在获取锁之后才会调用，它会通过持续更新资源锁的数据，来确保继续持有已获得的锁，保持自己的leader 状态。 // renew loops calling tryAcquireOrRenew and returns immediately when tryAcquireOrRenew fails or ctx signals done. func (le *LeaderElector) renew(ctx context.Context) { ctx, cancel := context.WithCancel(ctx) defer cancel() wait.Until(func() { timeoutCtx, timeoutCancel := context.WithTimeout(ctx, le.config.RenewDeadline) defer timeoutCancel() // err := wait.PollImmediateUntil(le.config.RetryPeriod, func() (bool, error) { done := make(chan bool, 1) go func() { defer close(done) done \u003c- le.tryAcquireOrRenew() }() // 超时返回error, 否则返回更新结果 select { case \u003c-timeoutCtx.Done(): return false, fmt.Errorf(\"failed to tryAcquireOrRenew %s\", timeoutCtx.Err()) case result := \u003c-done: return result, nil } }, timeoutCtx.Done()) le.maybeReportTransition() desc := le.config.Lock.Describe() if err == nil { klog.V(5).Infof(\"successfully renewed lease %v\", desc) return } le.config.Lock.RecordEvent(\"stopped leading\") le.metrics.leaderOff(le.config.Name) klog.Infof(\"failed to renew lease %v: %v\", desc, err) cancel() }, le.config.RetryPeriod, ctx.Done()) // if we hold the lease, give it up if le.config.ReleaseOnCancel { le.release() } } 这里使用了wait包，wait.Until会不断的调用wait.PollImmediateUntil方法，前者是进行无限循环操作，直到 stop chan被关闭，wait.PollImmediateUntil则不断的对某一条件进行检查，以RetryPeriod为间隔，直到该条件返回true、error或者超时。这一条件是一个需要满足 func() (bool, error) 签名的方法，比如这个例子只是调用了 le.tryAcquireOrRenew()。 最后看下tryAcquireOrRenew方法： // tryAcquireOrRenew tries to acquire a leader lease if it is not already acquired, // else it tries to renew the lease if it has already been acquired. Returns true // on success else returns false. func (le *LeaderElector) tryAcquireOrRenew() bool { now := metav1.Now() // 这个 leaderElectionRecord 就是保存在 endpoint/configmap 的 annotation 中的值。 // 每个节点都将 HolderIdentity 设置为自己，以及关于获取和更新锁的时间。后面会对时间进行修正，才会更新到 API server leaderElectionRecord := rl.LeaderElectionRecord{ HolderIdentity: le.config.Lock.Ident","date":"Dec 27, 2019","objectID":"/k8s-leaderelection-code/:2:3","series":null,"tags":["k8s"],"title":"k8s基于资源锁的选主分析","uri":"/k8s-leaderelection-code/#进行选举"},{"categories":["cloud"],"content":" 总结当应用在k8s上部署时，使用k8s的资源锁，可方便的实现高可用，但需要注意以下几点： 推荐使用configmap作为资源锁，原因是某些组件如kube-proxy会去监听endpoints来更新节点iptables规则，当有大量资源锁时，势必会对性能有影响。 当选举结束时调用OnStoppedLeading需要退出程序(例如os.Exit(0))，若不退出程序，所有副本选举结束不会去竞争资源锁，就没有leader，造成服务不可用而这时程序并没有异常。需要执行退出逻辑，让Daemon程序k8s/systemd等重启服务来重新参与选主。 ","date":"Dec 27, 2019","objectID":"/k8s-leaderelection-code/:3:0","series":null,"tags":["k8s"],"title":"k8s基于资源锁的选主分析","uri":"/k8s-leaderelection-code/#总结"},{"categories":["cloud"],"content":" 背景用户发现线上某容器请求hbase延迟较大，其他容器无类似现象，发现问题容器宿主机系统cpu占用较大（30%左右，正常在5%以下）。通过top查看lxcfs占用cpu较多（200%以上）。 ","date":"Nov 13, 2019","objectID":"/lxcfs-high-system-cpu/:1:0","series":null,"tags":["k8s"],"title":"cgroup引起的应用延迟","uri":"/lxcfs-high-system-cpu/#背景"},{"categories":["cloud"],"content":" 探究查看宿主机(内核 4.9.2)top,1显示每个cpu使用信息。查看最高的cpu占用是lxcfs造成的。 strace查看lxcfs调用 #查看调用情况，read占用99% $ strace -p 18521 -c % time seconds usecs/call calls errors syscall ------ ----------- ----------- --------- --------- ---------------- 99.82 78.360112 39797 1969 read 0.11 0.088295 122 722 munmap 0.01 0.011649 416 28 wait4 0.01 0.010611 14 736 open 0.01 0.005685 75 76 18 futex 0.01 0.005288 7 792 close 0.01 0.005115 14 366 writev 0.01 0.004750 7 722 mmap 0.00 0.003552 5 722 fstat 0.00 0.002989 107 28 epoll_wait 0.00 0.002102 17 126 stat 0.00 0.000202 14 14 socketpair 0.00 0.000157 11 14 write 0.00 0.000122 4 28 epoll_create 0.00 0.000111 8 14 recvmsg 0.00 0.000104 4 28 epoll_ctl 0.00 0.000091 3 28 clone 0.00 0.000071 5 14 setsockopt 0.00 0.000059 4 14 setns 0.00 0.000012 1 14 recvfrom 0.00 0.000011 1 14 sendmsg 0.00 0.000003 0 14 set_robust_list 0.00 0.000000 0 14 getpid ------ ----------- ----------- --------- --------- ---------------- # 查看详细情况，大量读取cgroup下memory的调用 $ strace -p 18521 -f -T -tt -o lx.log cat lx.log 79153 14:20:31.122630 open(\"/run/lxcfs/controllers/memory//kubepods/burstable/pod7077217d-de6f-11e9-9352-246e96d53468/bcac6516ca5b2a60880fcbc752bf6878ddc77905db71269d852d17f5dc90b148/memory.memsw.limit_in_bytes\", O_RDONLY) = 5 \u003c0.000017\u003e 经发现某个pod调用的次数明显高于其他pod，排查到其容器内每隔2s执行ps -auf，会调用/proc/pid/stat其中就有memory相关的。 开开心心联系业务将其驱逐，宿主机没有明显变化。。。，再次查看top top - 13:43:56 up 120 days, 19:21, 1 user, load average: 6.59, 3.26, 2.34 Tasks: 630 total, 1 running, 629 sleeping, 0 stopped, 0 zombie %Cpu(s): 0.8 us, 7.1 sy, 0.0 ni, 92.1 id, 0.0 wa, 0.0 hi, 0.0 si, 0.0 st KiB Mem : 13170992+total, 93100928 free, 7571536 used, 31037456 buff/cache KiB Swap: 0 total, 0 free, 0 used. 11042460+avail Mem PID USER PR NI VIRT RES SHR S %CPU %MEM TIME+ COMMAND 20686 root 20 0 141152 49032 17960 S 51.3 0.0 5890:25 cadvisor 115798 root 20 0 0 0 0 D 19.5 0.0 0:09.62 kworker/14:0 95501 root 20 0 0 0 0 D 17.2 0.0 0:10.11 kworker/0:1 38620 root 20 0 0 0 0 D 13.9 0.0 0:07.92 kworker/2:1 111178 root 20 0 0 0 0 D 13.9 0.0 0:10.67 kworker/6:0 58741 root 20 0 0 0 0 D 12.3 0.0 0:10.50 kworker/15:1 104600 root 20 0 0 0 0 D 12.3 0.0 0:05.55 kworker/8:2 15166 root 20 0 0 0 0 D 10.9 0.0 0:04.44 kworker/16:1 89483 root 20 0 0 0 0 D 10.9 0.0 0:04.73 kworker/11:0 30487 root 20 0 3905496 152268 36216 S 9.3 0.1 3060:33 dockerd 41220 work 20 0 687540 300368 16012 S 4.0 0.2 235:53.07 lottery-service 125923 root 20 0 4892136 181572 58924 S 3.6 0.1 21469:57 kubelet ... 发现cadvisor占用较高的cpu，联系以前遇到的问题，cadvisor也是采集memory时变慢,测试居然需要2秒多！ $ time cat /sys/fs/cgroup/memory/memory.stat cache 25691987968 rss 3426922496 rss_huge 2759852032 ... real 0m2.485s user 0m0.000s sys 0m2.484s 主要原因是产生了某些僵尸cgroup(比如反复启动，进程不存在了，但cgroup还没来得及回收，cgroup会反复计算这些cgroup的内存会占用)，导致cpu使用增加相关issue 以及thread ","date":"Nov 13, 2019","objectID":"/lxcfs-high-system-cpu/:2:0","series":null,"tags":["k8s"],"title":"cgroup引起的应用延迟","uri":"/lxcfs-high-system-cpu/#探究"},{"categories":["cloud"],"content":" 解决根本原因还需要进一步分析，临时解决办法，通过手动释放内存 echo 2 \u003e /proc/sys/vm/drop_caches 如果没效果可尝试 echo 3 \u003e /proc/sys/vm/drop_caches 释放后，果然系统cpu逐渐恢复正常了，从falcon查看cpu确实下降了 ","date":"Nov 13, 2019","objectID":"/lxcfs-high-system-cpu/:3:0","series":null,"tags":["k8s"],"title":"cgroup引起的应用延迟","uri":"/lxcfs-high-system-cpu/#解决"},{"categories":["cloud"],"content":" 跟进经排查，我们使用的内核较旧为（4.9.2）;僵尸cgroup过多, 导致遍历cgroup读取per_cpu变量时可能引起锁的争用。 僵尸cgroup：没有进程运行，并已经被删除的cgroup，但是所占用的内存并没有被完全回收(inode，dentry等缓存资源)，在读取memory.stat仍会计算这部分cgroup的缓存空间。 目前该问题在新版的内核（如5.4）中得到修复，新内核引用新的数据结构解决该问题：每次分配内存时，会即时更新cgroup的内存使用情况存储到专用的统计变量，因此读取某个cgroup的mem stat不会涉及到per_cpu变量，可以立即返回。 ","date":"Nov 13, 2019","objectID":"/lxcfs-high-system-cpu/:4:0","series":null,"tags":["k8s"],"title":"cgroup引起的应用延迟","uri":"/lxcfs-high-system-cpu/#跟进"},{"categories":["cloud"],"content":" 背景在上线fd隔离后，多个用户反馈部署有问题，日志显示 su could not open session，dolphin（主进程） 启动用户程序时如果用户部署账号为work，会通过su切换到work下启动用户程序，报错正是这时产生。 ","date":"Jul 16, 2019","objectID":"/k8s-limit-fd-and-thread2/:1:0","series":null,"tags":["k8s","docker"],"title":"k8s中fd与thread限制(二)","uri":"/k8s-limit-fd-and-thread2/#背景"},{"categories":["cloud"],"content":" 探究通过复现问题，确实存在su切换失败，通过strace su work显示： sh-4.1# strace -o strace.log su work could not open session sh-4.1# vim strace.log execve(\"/bin/su\", [\"su\", \"work\"], [/* 18 vars */]) = 0 brk(0) /su ... stat(\"/etc/pam.d\", {st_mode=S_IFDIR|0755, st_size=4096, ...}) = 0 open(\"/etc/pam.d/su\", O_RDONLY) = 3 ... open(\"/etc/pam.d/system-auth\", O_RDONLY) = 4 ... getrlimit(RLIMIT_CPU, {rlim_cur=RLIM_INFINITY, rlim_max=RLIM_INFINITY}) = 0 # 通过getrlimit获取当前ulimit设置 getrlimit(RLIMIT_FSIZE, {rlim_cur=RLIM_INFINITY, rlim_max=RLIM_INFINITY}) = 0 getrlimit(RLIMIT_DATA, {rlim_cur=RLIM_INFINITY, rlim_max=RLIM_INFINITY}) = 0 getrlimit(RLIMIT_STACK, {rlim_cur=8192*1024, rlim_max=RLIM_INFINITY}) = 0 getrlimit(RLIMIT_CORE, {rlim_cur=RLIM_INFINITY, rlim_max=RLIM_INFINITY}) = 0 getrlimit(RLIMIT_RSS, {rlim_cur=RLIM_INFINITY, rlim_max=RLIM_INFINITY}) = 0 getrlimit(RLIMIT_NPROC, {rlim_cur=2048*1024, rlim_max=2048*1024}) = 0 getrlimit(RLIMIT_NOFILE, {rlim_cur=10*1024, rlim_max=20*1024}) = 0 getrlimit(RLIMIT_MEMLOCK, {rlim_cur=RLIM_INFINITY, rlim_max=RLIM_INFINITY}) = 0 getrlimit(RLIMIT_AS, {rlim_cur=RLIM_INFINITY, rlim_max=RLIM_INFINITY}) = 0 getrlimit(RLIMIT_LOCKS, {rlim_cur=RLIM_INFINITY, rlim_max=RLIM_INFINITY}) = 0 getrlimit(RLIMIT_SIGPENDING, {rlim_cur=256736, rlim_max=256736}) = 0 getrlimit(RLIMIT_MSGQUEUE, {rlim_cur=800*1024, rlim_max=800*1024}) = 0 getrlimit(RLIMIT_NICE, {rlim_cur=0, rlim_max=0}) = 0 getrlimit(RLIMIT_RTPRIO, {rlim_cur=0, rlim_max=0}) = 0 getpriority(PRIO_PROCESS, 0) = 20 open(\"/etc/security/limits.conf\", O_RDONLY) = 3 # 读取limits.conf配置 fstat(3, {st_mode=S_IFREG|0644, st_size=1973, ...}) = 0 mmap(NULL, 4096, PROT_READ|PROT_WRITE, MAP_PRIVATE|MAP_ANONYMOUS, -1, 0) = 0x7f2b03deb000 read(3, \"# /etc/security/limits.conf\\n#\\n#E\"..., 4096) = 1973 read(3, \"\", 4096) = 0 close(3) = 0 munmap(0x7f2b03deb000, 4096) = 0 open(\"/etc/security/limits.d\", O_RDONLY|O_NONBLOCK|O_DIRECTORY|O_CLOEXEC) = 3 fcntl(3, F_GETFD) = 0x1 (flags FD_CLOEXEC) getdents(3, /* 2 entries */, 32768) = 48 open(\"/usr/lib64/gconv/gconv-modules.cache\", O_RDONLY) = 4 fstat(4, {st_mode=S_IFREG|0644, st_size=26060, ...}) = 0 mmap(NULL, 26060, PROT_READ, MAP_SHARED, 4, 0) = 0x7f2b03de5000 close(4) = 0 futex(0x7f2b037b6f60, FUTEX_WAKE_PRIVATE, 2147483647) = 0 getdents(3, /* 0 entries */, 32768) = 0 close(3) = 0 setrlimit(RLIMIT_CORE, {rlim_cur=RLIM_INFINITY, rlim_max=RLIM_INFINITY}) = 0 setrlimit(RLIMIT_NOFILE, {rlim_cur=150240, rlim_max=300240}) = -1 EPERM (Operation not permitted) # 设置nofile失败，返回权限不足，经查证setrlimit需要CAP_SYS_RESOURCE ... 整理下执行su的流程 进行pam认证，su配置文件在/etc/pam.d/su，更多pam信息可参考pam.d 根据文件内容逐行认证，下面是线上centos6基础镜像的配置 #%PAM-1.0 auth sufficient pam_rootok.so # Uncomment the following line to implicitly trust users in the \"wheel\" group. #auth sufficient pam_wheel.so trust use_uid # Uncomment the following line to require a user to be in the \"wheel\" group. #auth required pam_wheel.so use_uid auth include system-auth account sufficient pam_succeed_if.so uid = 0 use_uid quiet account include system-auth password include system-auth session include system-auth #认证失败出现在这步 session optional pam_xauth.so system-auth 真实内容存放在 system-auth-ac，内容为 # User changes will be destroyed the next time authconfig is run. auth required pam_env.so auth sufficient pam_fprintd.so auth sufficient pam_unix.so nullok try_first_pass auth requisite pam_succeed_if.so uid \u003e= 500 quiet auth required pam_deny.so account required pam_unix.so account sufficient pam_localuser.so account sufficient pam_succeed_if.so uid \u003c 500 quiet account required pam_permit.so password requisite pam_cracklib.so try_first_pass retry=3 type= password sufficient pam_unix.so md5 shadow nullok try_first_pass use_authtok password required pam_deny.so session optional pam_keyinit.so revoke session required pam_limits.so # limit 认证 session [success=1 default=ignore] pam_succeed_if.so service in crond quiet use_uid session required pam_unix.so system-auth调用pam_limit.so认证，并且类型为required，及若认证失败则继续执行最后返回失败信息 pam_limit会调用getrlimit获取当前ulimit信息，通过读取/et","date":"Jul 16, 2019","objectID":"/k8s-limit-fd-and-thread2/:2:0","series":null,"tags":["k8s","docker"],"title":"k8s中fd与thread限制(二)","uri":"/k8s-limit-fd-and-thread2/#探究"},{"categories":["cloud"],"content":" 解决办法由于limits.conf，以及pam.so等配置文件是镜像中的配置，解决冲突必须修改对应配置,有两种方式 通过dolphin将对应limits.conf以及limits.d目录下有关nofile的配置删除 基础镜像修改limits.conf配置 ","date":"Jul 16, 2019","objectID":"/k8s-limit-fd-and-thread2/:3:0","series":null,"tags":["k8s","docker"],"title":"k8s中fd与thread限制(二)","uri":"/k8s-limit-fd-and-thread2/#解决办法"},{"categories":["cloud"],"content":" 背景linux中为了防止进程恶意使用资源，系统使用ulimit来限制进程的资源使用情况（包括文件描述符，线程数，内存大小等）。同样地在容器化场景中，需要限制其系统资源的使用量。 ","date":"Jul 16, 2019","objectID":"/k8s-limit-fd-and-thread1/:1:0","series":null,"tags":["k8s","docker"],"title":"k8s中fd与thread限制(一)","uri":"/k8s-limit-fd-and-thread1/#背景"},{"categories":["cloud"],"content":" 限制方法 ulimit: docker 默认支持ulimit设置，可以在dockerd中配置 default-ulimits 可为宿主机所有容器配置默认的ulimit，docker启动时可添加 –ulimit 为每个容器配置ulimit会覆盖默认的设置；目前k8s暂不支持ulimit cgroup: docker 默认支持cgroup中内存、cpu、pid等的限制，对于线程限制可通过 –pids-limit 可限制每个容器的pid总数，dockerd暂无默认的pid limit设置；k8s 限制线程数，可通过在kubelet中开启SupportPodPidsLimit特性，设置pod级别pid limit /etc/securiy/limits.conf,systcl.confg: 通过ulimit命令设置只对当前登录用户有效，永久设置可通过limits.conf配置文件实现，以及系统级别限制可通过systcl.confg配置文件 ","date":"Jul 16, 2019","objectID":"/k8s-limit-fd-and-thread1/:2:0","series":null,"tags":["k8s","docker"],"title":"k8s中fd与thread限制(一)","uri":"/k8s-limit-fd-and-thread1/#限制方法"},{"categories":["cloud"],"content":" 实验对比","date":"Jul 16, 2019","objectID":"/k8s-limit-fd-and-thread1/:3:0","series":null,"tags":["k8s","docker"],"title":"k8s中fd与thread限制(一)","uri":"/k8s-limit-fd-and-thread1/#实验对比"},{"categories":["cloud"],"content":" 环境本地环境： os: Ubuntu 16.04.6 LTS 4.4.0-154-generic docker: 18.09.7 base-image: alpine:v3.9 k8s环境： kubelet: v1.10.11.1 docker: 18.09.6 ","date":"Jul 16, 2019","objectID":"/k8s-limit-fd-and-thread1/:3:1","series":null,"tags":["k8s","docker"],"title":"k8s中fd与thread限制(一)","uri":"/k8s-limit-fd-and-thread1/#环境"},{"categories":["cloud"],"content":" ulimit用户级别资源限制，分为soft限制与hard限制 soft ： 用户可修改，但不能超过硬限制 hard：只有root用户可修改 修改方式： ulimit命令，临时修改；/etc/security/limits.conf，永久修改 工作原理： 根据 PAM （ Pluggable Authentication Modules 简称 PAM）机制，应用程序启动时，按 /etc/pam.d 配置加载 pam_xxxx.so 模块。 /etc/pam.d 下包含了 login 、sshd 、su 、sudo 等程序的 PAM 配置文件， 因此用户重新登录时，将调用 pam_limits.so 加载 limits.conf 配置文件 文件描述符限制RLIMIT_NOFILE This specifies a value one greater than the maximum file descriptor number that can be opened by this process. Attempts (open(2), pipe(2), dup(2), etc.) to exceed this limit yield the error EMFILE. (Historically, this limit was named RLIMIT_OFILE on BSD.) Since Linux 4.5, this limit also defines the maximum number of file descriptors that an unprivileged process (one without the CAP_SYS_RESOURCE capability) may have \"in flight\" to other processes, by being passed across UNIX domain sockets. This limit applies to the sendmsg(2) system call. For further details, see unix(7). 根据定义，nofile 限制进程所能最多打开的文件数量，作用范围进程。 设置 ulimit nofile限制soft 100/hard 200，默认启动为root用户 $ docker run -d --ulimit nofile=100:200 cr.d.xiaomi.net/containercloud/alpine:webtool top 进入容器查看， fd soft限制为100个 / # ulimit -a -f: file size (blocks) unlimited -t: cpu time (seconds) unlimited -d: data seg size (kb) unlimited -s: stack size (kb) 8192 -c: core file size (blocks) unlimited -m: resident set size (kb) unlimited -l: locked memory (kb) 64 -p: processes unlimited -n: file descriptors 100 -v: address space (kb) unlimited -w: locks unlimited -e: scheduling priority 0 -r: real-time priority 0 使用ab测试，并发90个http请求，创建90个socket，正常运行 / # ab -n 1000000 -c 90 http://61.135.169.125:80/ \u0026 / # lsof | wc -l 108 / # lsof | grep -c ab 94 并发100个http请求，受到ulimit限制 / # ab -n 1000000 -c 100 http://61.135.169.125:80/ This is ApacheBench, Version 2.3 \u003c$Revision: 1843412 $\u003e Copyright 1996 Adam Twiss, Zeus Technology Ltd, http://www.zeustech.net/ Licensed to The Apache Software Foundation, http://www.apache.org/ Benchmarking 61.135.169.125 (be patient) socket: No file descriptors available (24) 线程限制RLIMIT_NPROC This is a limit on the number of extant process (or, more pre‐ cisely on Linux, threads) for the real user ID of the calling process. So long as the current number of processes belonging to this process's real user ID is greater than or equal to this limit, fork(2) fails with the error EAGAIN. The RLIMIT_NPROC limit is not enforced for processes that have either the CAP_SYS_ADMIN or the CAP_SYS_RESOURCE capability. 由定义可知，nproc进程限制的范围是对于每个uid，并且对于root用户无效。 容器uid同一主机上运行的所有容器共享同一个内核(主机的内核)，docker通过namspace对pid/utc/network等进行了隔离，虽然docker中已经实现了user namespace，但由于各种原因，默认没有开启，见docker user namespace $ docker run -d cr.d.xiaomi.net/containercloud/alpine:webtool top 宿主机中查看top进程，显示root用户 $ ps -ef |grep top root 4096 4080 0 15:01 ? 00:00:01 top 容器中查看id，uid为0对应宿主机的root用户,虽然同为root用户，但Linux Capabilities不同，实际权限与宿主机root要少很多 在容器中切换用户到operator(uid为11)，执行sleep命令，主机中查看对应进程用户为app，对应uid同样为11 / # id uid=0(root) gid=0(root) groups=0(root),1(bin),2(daemon),3(sys),4(adm),6(disk),10(wheel),11(floppy),20(dialout),26(tape),27(video) / # su operator / $ id uid=11(operator) gid=0(root) groups=0(root) / $ sleep 100 $ ps -ef |grep 'sleep 100' app 19302 19297 0 16:39 pts/0 00:00:00 sleep 100 $ cat /etc/passwd | grep app app❌11:0::/home/app: 验证不同用户下ulimit的限制设置 ulimit nproc限制soft 10/hard 20，默认启动为root用户 $ docker run -d --ulimit nproc=10:20 cr.d.xiaomi.net/containercloud/alpine:webtool top 进入容器查看， fd soft限制为100个 / # ulimit -a -f: file size (blocks) unlimited -t: cpu time (seconds) unlimited -d: data seg size (kb) unlimited -s: stack size (kb) 8192 -c: core file size (blocks) unlimited -m: resident set size (kb) unlimited -l: locked memory (kb) 64 -p: processes 10 -n: file descriptors 1048576 -v: address space (kb) unlimited -w: locks unlimited -e: scheduling priority 0 -r: real-time priority 0 启动30个进程 / # for i in `seq 30`;do sleep 100 \u0026; done / # ps | wc -l 36 切换到operator用户 / # su operator # 启动多个进程，到第11个进程无法进行fork / $ for i in `seq 8`; do \u003e sleep 100 \u0026 \u003e done / $ sleep 100 \u0026 / $ sleep ","date":"Jul 16, 2019","objectID":"/k8s-limit-fd-and-thread1/:3:2","series":null,"tags":["k8s","docker"],"title":"k8s中fd与thread限制(一)","uri":"/k8s-limit-fd-and-thread1/#ulimit"},{"categories":["cloud"],"content":" ulimit用户级别资源限制，分为soft限制与hard限制 soft ： 用户可修改，但不能超过硬限制 hard：只有root用户可修改 修改方式： ulimit命令，临时修改；/etc/security/limits.conf，永久修改 工作原理： 根据 PAM （ Pluggable Authentication Modules 简称 PAM）机制，应用程序启动时，按 /etc/pam.d 配置加载 pam_xxxx.so 模块。 /etc/pam.d 下包含了 login 、sshd 、su 、sudo 等程序的 PAM 配置文件， 因此用户重新登录时，将调用 pam_limits.so 加载 limits.conf 配置文件 文件描述符限制RLIMIT_NOFILE This specifies a value one greater than the maximum file descriptor number that can be opened by this process. Attempts (open(2), pipe(2), dup(2), etc.) to exceed this limit yield the error EMFILE. (Historically, this limit was named RLIMIT_OFILE on BSD.) Since Linux 4.5, this limit also defines the maximum number of file descriptors that an unprivileged process (one without the CAP_SYS_RESOURCE capability) may have \"in flight\" to other processes, by being passed across UNIX domain sockets. This limit applies to the sendmsg(2) system call. For further details, see unix(7). 根据定义，nofile 限制进程所能最多打开的文件数量，作用范围进程。 设置 ulimit nofile限制soft 100/hard 200，默认启动为root用户 $ docker run -d --ulimit nofile=100:200 cr.d.xiaomi.net/containercloud/alpine:webtool top 进入容器查看， fd soft限制为100个 / # ulimit -a -f: file size (blocks) unlimited -t: cpu time (seconds) unlimited -d: data seg size (kb) unlimited -s: stack size (kb) 8192 -c: core file size (blocks) unlimited -m: resident set size (kb) unlimited -l: locked memory (kb) 64 -p: processes unlimited -n: file descriptors 100 -v: address space (kb) unlimited -w: locks unlimited -e: scheduling priority 0 -r: real-time priority 0 使用ab测试，并发90个http请求，创建90个socket，正常运行 / # ab -n 1000000 -c 90 http://61.135.169.125:80/ \u0026 / # lsof | wc -l 108 / # lsof | grep -c ab 94 并发100个http请求，受到ulimit限制 / # ab -n 1000000 -c 100 http://61.135.169.125:80/ This is ApacheBench, Version 2.3 \u003c$Revision: 1843412 $\u003e Copyright 1996 Adam Twiss, Zeus Technology Ltd, http://www.zeustech.net/ Licensed to The Apache Software Foundation, http://www.apache.org/ Benchmarking 61.135.169.125 (be patient) socket: No file descriptors available (24) 线程限制RLIMIT_NPROC This is a limit on the number of extant process (or, more pre‐ cisely on Linux, threads) for the real user ID of the calling process. So long as the current number of processes belonging to this process's real user ID is greater than or equal to this limit, fork(2) fails with the error EAGAIN. The RLIMIT_NPROC limit is not enforced for processes that have either the CAP_SYS_ADMIN or the CAP_SYS_RESOURCE capability. 由定义可知，nproc进程限制的范围是对于每个uid，并且对于root用户无效。 容器uid同一主机上运行的所有容器共享同一个内核(主机的内核)，docker通过namspace对pid/utc/network等进行了隔离，虽然docker中已经实现了user namespace，但由于各种原因，默认没有开启，见docker user namespace $ docker run -d cr.d.xiaomi.net/containercloud/alpine:webtool top 宿主机中查看top进程，显示root用户 $ ps -ef |grep top root 4096 4080 0 15:01 ? 00:00:01 top 容器中查看id，uid为0对应宿主机的root用户,虽然同为root用户，但Linux Capabilities不同，实际权限与宿主机root要少很多 在容器中切换用户到operator(uid为11)，执行sleep命令，主机中查看对应进程用户为app，对应uid同样为11 / # id uid=0(root) gid=0(root) groups=0(root),1(bin),2(daemon),3(sys),4(adm),6(disk),10(wheel),11(floppy),20(dialout),26(tape),27(video) / # su operator / $ id uid=11(operator) gid=0(root) groups=0(root) / $ sleep 100 $ ps -ef |grep 'sleep 100' app 19302 19297 0 16:39 pts/0 00:00:00 sleep 100 $ cat /etc/passwd | grep app app❌11:0::/home/app: 验证不同用户下ulimit的限制设置 ulimit nproc限制soft 10/hard 20，默认启动为root用户 $ docker run -d --ulimit nproc=10:20 cr.d.xiaomi.net/containercloud/alpine:webtool top 进入容器查看， fd soft限制为100个 / # ulimit -a -f: file size (blocks) unlimited -t: cpu time (seconds) unlimited -d: data seg size (kb) unlimited -s: stack size (kb) 8192 -c: core file size (blocks) unlimited -m: resident set size (kb) unlimited -l: locked memory (kb) 64 -p: processes 10 -n: file descriptors 1048576 -v: address space (kb) unlimited -w: locks unlimited -e: scheduling priority 0 -r: real-time priority 0 启动30个进程 / # for i in `seq 30`;do sleep 100 \u0026; done / # ps | wc -l 36 切换到operator用户 / # su operator # 启动多个进程，到第11个进程无法进行fork / $ for i in `seq 8`; do \u003e sleep 100 \u0026 \u003e done / $ sleep 100 \u0026 / $ sleep ","date":"Jul 16, 2019","objectID":"/k8s-limit-fd-and-thread1/:3:2","series":null,"tags":["k8s","docker"],"title":"k8s中fd与thread限制(一)","uri":"/k8s-limit-fd-and-thread1/#文件描述符限制"},{"categories":["cloud"],"content":" ulimit用户级别资源限制，分为soft限制与hard限制 soft ： 用户可修改，但不能超过硬限制 hard：只有root用户可修改 修改方式： ulimit命令，临时修改；/etc/security/limits.conf，永久修改 工作原理： 根据 PAM （ Pluggable Authentication Modules 简称 PAM）机制，应用程序启动时，按 /etc/pam.d 配置加载 pam_xxxx.so 模块。 /etc/pam.d 下包含了 login 、sshd 、su 、sudo 等程序的 PAM 配置文件， 因此用户重新登录时，将调用 pam_limits.so 加载 limits.conf 配置文件 文件描述符限制RLIMIT_NOFILE This specifies a value one greater than the maximum file descriptor number that can be opened by this process. Attempts (open(2), pipe(2), dup(2), etc.) to exceed this limit yield the error EMFILE. (Historically, this limit was named RLIMIT_OFILE on BSD.) Since Linux 4.5, this limit also defines the maximum number of file descriptors that an unprivileged process (one without the CAP_SYS_RESOURCE capability) may have \"in flight\" to other processes, by being passed across UNIX domain sockets. This limit applies to the sendmsg(2) system call. For further details, see unix(7). 根据定义，nofile 限制进程所能最多打开的文件数量，作用范围进程。 设置 ulimit nofile限制soft 100/hard 200，默认启动为root用户 $ docker run -d --ulimit nofile=100:200 cr.d.xiaomi.net/containercloud/alpine:webtool top 进入容器查看， fd soft限制为100个 / # ulimit -a -f: file size (blocks) unlimited -t: cpu time (seconds) unlimited -d: data seg size (kb) unlimited -s: stack size (kb) 8192 -c: core file size (blocks) unlimited -m: resident set size (kb) unlimited -l: locked memory (kb) 64 -p: processes unlimited -n: file descriptors 100 -v: address space (kb) unlimited -w: locks unlimited -e: scheduling priority 0 -r: real-time priority 0 使用ab测试，并发90个http请求，创建90个socket，正常运行 / # ab -n 1000000 -c 90 http://61.135.169.125:80/ \u0026 / # lsof | wc -l 108 / # lsof | grep -c ab 94 并发100个http请求，受到ulimit限制 / # ab -n 1000000 -c 100 http://61.135.169.125:80/ This is ApacheBench, Version 2.3 \u003c$Revision: 1843412 $\u003e Copyright 1996 Adam Twiss, Zeus Technology Ltd, http://www.zeustech.net/ Licensed to The Apache Software Foundation, http://www.apache.org/ Benchmarking 61.135.169.125 (be patient) socket: No file descriptors available (24) 线程限制RLIMIT_NPROC This is a limit on the number of extant process (or, more pre‐ cisely on Linux, threads) for the real user ID of the calling process. So long as the current number of processes belonging to this process's real user ID is greater than or equal to this limit, fork(2) fails with the error EAGAIN. The RLIMIT_NPROC limit is not enforced for processes that have either the CAP_SYS_ADMIN or the CAP_SYS_RESOURCE capability. 由定义可知，nproc进程限制的范围是对于每个uid，并且对于root用户无效。 容器uid同一主机上运行的所有容器共享同一个内核(主机的内核)，docker通过namspace对pid/utc/network等进行了隔离，虽然docker中已经实现了user namespace，但由于各种原因，默认没有开启，见docker user namespace $ docker run -d cr.d.xiaomi.net/containercloud/alpine:webtool top 宿主机中查看top进程，显示root用户 $ ps -ef |grep top root 4096 4080 0 15:01 ? 00:00:01 top 容器中查看id，uid为0对应宿主机的root用户,虽然同为root用户，但Linux Capabilities不同，实际权限与宿主机root要少很多 在容器中切换用户到operator(uid为11)，执行sleep命令，主机中查看对应进程用户为app，对应uid同样为11 / # id uid=0(root) gid=0(root) groups=0(root),1(bin),2(daemon),3(sys),4(adm),6(disk),10(wheel),11(floppy),20(dialout),26(tape),27(video) / # su operator / $ id uid=11(operator) gid=0(root) groups=0(root) / $ sleep 100 $ ps -ef |grep 'sleep 100' app 19302 19297 0 16:39 pts/0 00:00:00 sleep 100 $ cat /etc/passwd | grep app app❌11:0::/home/app: 验证不同用户下ulimit的限制设置 ulimit nproc限制soft 10/hard 20，默认启动为root用户 $ docker run -d --ulimit nproc=10:20 cr.d.xiaomi.net/containercloud/alpine:webtool top 进入容器查看， fd soft限制为100个 / # ulimit -a -f: file size (blocks) unlimited -t: cpu time (seconds) unlimited -d: data seg size (kb) unlimited -s: stack size (kb) 8192 -c: core file size (blocks) unlimited -m: resident set size (kb) unlimited -l: locked memory (kb) 64 -p: processes 10 -n: file descriptors 1048576 -v: address space (kb) unlimited -w: locks unlimited -e: scheduling priority 0 -r: real-time priority 0 启动30个进程 / # for i in `seq 30`;do sleep 100 \u0026; done / # ps | wc -l 36 切换到operator用户 / # su operator # 启动多个进程，到第11个进程无法进行fork / $ for i in `seq 8`; do \u003e sleep 100 \u0026 \u003e done / $ sleep 100 \u0026 / $ sleep ","date":"Jul 16, 2019","objectID":"/k8s-limit-fd-and-thread1/:3:2","series":null,"tags":["k8s","docker"],"title":"k8s中fd与thread限制(一)","uri":"/k8s-limit-fd-and-thread1/#线程限制"},{"categories":["cloud"],"content":" ulimit用户级别资源限制，分为soft限制与hard限制 soft ： 用户可修改，但不能超过硬限制 hard：只有root用户可修改 修改方式： ulimit命令，临时修改；/etc/security/limits.conf，永久修改 工作原理： 根据 PAM （ Pluggable Authentication Modules 简称 PAM）机制，应用程序启动时，按 /etc/pam.d 配置加载 pam_xxxx.so 模块。 /etc/pam.d 下包含了 login 、sshd 、su 、sudo 等程序的 PAM 配置文件， 因此用户重新登录时，将调用 pam_limits.so 加载 limits.conf 配置文件 文件描述符限制RLIMIT_NOFILE This specifies a value one greater than the maximum file descriptor number that can be opened by this process. Attempts (open(2), pipe(2), dup(2), etc.) to exceed this limit yield the error EMFILE. (Historically, this limit was named RLIMIT_OFILE on BSD.) Since Linux 4.5, this limit also defines the maximum number of file descriptors that an unprivileged process (one without the CAP_SYS_RESOURCE capability) may have \"in flight\" to other processes, by being passed across UNIX domain sockets. This limit applies to the sendmsg(2) system call. For further details, see unix(7). 根据定义，nofile 限制进程所能最多打开的文件数量，作用范围进程。 设置 ulimit nofile限制soft 100/hard 200，默认启动为root用户 $ docker run -d --ulimit nofile=100:200 cr.d.xiaomi.net/containercloud/alpine:webtool top 进入容器查看， fd soft限制为100个 / # ulimit -a -f: file size (blocks) unlimited -t: cpu time (seconds) unlimited -d: data seg size (kb) unlimited -s: stack size (kb) 8192 -c: core file size (blocks) unlimited -m: resident set size (kb) unlimited -l: locked memory (kb) 64 -p: processes unlimited -n: file descriptors 100 -v: address space (kb) unlimited -w: locks unlimited -e: scheduling priority 0 -r: real-time priority 0 使用ab测试，并发90个http请求，创建90个socket，正常运行 / # ab -n 1000000 -c 90 http://61.135.169.125:80/ \u0026 / # lsof | wc -l 108 / # lsof | grep -c ab 94 并发100个http请求，受到ulimit限制 / # ab -n 1000000 -c 100 http://61.135.169.125:80/ This is ApacheBench, Version 2.3 \u003c$Revision: 1843412 $\u003e Copyright 1996 Adam Twiss, Zeus Technology Ltd, http://www.zeustech.net/ Licensed to The Apache Software Foundation, http://www.apache.org/ Benchmarking 61.135.169.125 (be patient) socket: No file descriptors available (24) 线程限制RLIMIT_NPROC This is a limit on the number of extant process (or, more pre‐ cisely on Linux, threads) for the real user ID of the calling process. So long as the current number of processes belonging to this process's real user ID is greater than or equal to this limit, fork(2) fails with the error EAGAIN. The RLIMIT_NPROC limit is not enforced for processes that have either the CAP_SYS_ADMIN or the CAP_SYS_RESOURCE capability. 由定义可知，nproc进程限制的范围是对于每个uid，并且对于root用户无效。 容器uid同一主机上运行的所有容器共享同一个内核(主机的内核)，docker通过namspace对pid/utc/network等进行了隔离，虽然docker中已经实现了user namespace，但由于各种原因，默认没有开启，见docker user namespace $ docker run -d cr.d.xiaomi.net/containercloud/alpine:webtool top 宿主机中查看top进程，显示root用户 $ ps -ef |grep top root 4096 4080 0 15:01 ? 00:00:01 top 容器中查看id，uid为0对应宿主机的root用户,虽然同为root用户，但Linux Capabilities不同，实际权限与宿主机root要少很多 在容器中切换用户到operator(uid为11)，执行sleep命令，主机中查看对应进程用户为app，对应uid同样为11 / # id uid=0(root) gid=0(root) groups=0(root),1(bin),2(daemon),3(sys),4(adm),6(disk),10(wheel),11(floppy),20(dialout),26(tape),27(video) / # su operator / $ id uid=11(operator) gid=0(root) groups=0(root) / $ sleep 100 $ ps -ef |grep 'sleep 100' app 19302 19297 0 16:39 pts/0 00:00:00 sleep 100 $ cat /etc/passwd | grep app app❌11:0::/home/app: 验证不同用户下ulimit的限制设置 ulimit nproc限制soft 10/hard 20，默认启动为root用户 $ docker run -d --ulimit nproc=10:20 cr.d.xiaomi.net/containercloud/alpine:webtool top 进入容器查看， fd soft限制为100个 / # ulimit -a -f: file size (blocks) unlimited -t: cpu time (seconds) unlimited -d: data seg size (kb) unlimited -s: stack size (kb) 8192 -c: core file size (blocks) unlimited -m: resident set size (kb) unlimited -l: locked memory (kb) 64 -p: processes 10 -n: file descriptors 1048576 -v: address space (kb) unlimited -w: locks unlimited -e: scheduling priority 0 -r: real-time priority 0 启动30个进程 / # for i in `seq 30`;do sleep 100 \u0026; done / # ps | wc -l 36 切换到operator用户 / # su operator # 启动多个进程，到第11个进程无法进行fork / $ for i in `seq 8`; do \u003e sleep 100 \u0026 \u003e done / $ sleep 100 \u0026 / $ sleep ","date":"Jul 16, 2019","objectID":"/k8s-limit-fd-and-thread1/:3:2","series":null,"tags":["k8s","docker"],"title":"k8s中fd与thread限制(一)","uri":"/k8s-limit-fd-and-thread1/#容器uid"},{"categories":["cloud"],"content":" ulimit用户级别资源限制，分为soft限制与hard限制 soft ： 用户可修改，但不能超过硬限制 hard：只有root用户可修改 修改方式： ulimit命令，临时修改；/etc/security/limits.conf，永久修改 工作原理： 根据 PAM （ Pluggable Authentication Modules 简称 PAM）机制，应用程序启动时，按 /etc/pam.d 配置加载 pam_xxxx.so 模块。 /etc/pam.d 下包含了 login 、sshd 、su 、sudo 等程序的 PAM 配置文件， 因此用户重新登录时，将调用 pam_limits.so 加载 limits.conf 配置文件 文件描述符限制RLIMIT_NOFILE This specifies a value one greater than the maximum file descriptor number that can be opened by this process. Attempts (open(2), pipe(2), dup(2), etc.) to exceed this limit yield the error EMFILE. (Historically, this limit was named RLIMIT_OFILE on BSD.) Since Linux 4.5, this limit also defines the maximum number of file descriptors that an unprivileged process (one without the CAP_SYS_RESOURCE capability) may have \"in flight\" to other processes, by being passed across UNIX domain sockets. This limit applies to the sendmsg(2) system call. For further details, see unix(7). 根据定义，nofile 限制进程所能最多打开的文件数量，作用范围进程。 设置 ulimit nofile限制soft 100/hard 200，默认启动为root用户 $ docker run -d --ulimit nofile=100:200 cr.d.xiaomi.net/containercloud/alpine:webtool top 进入容器查看， fd soft限制为100个 / # ulimit -a -f: file size (blocks) unlimited -t: cpu time (seconds) unlimited -d: data seg size (kb) unlimited -s: stack size (kb) 8192 -c: core file size (blocks) unlimited -m: resident set size (kb) unlimited -l: locked memory (kb) 64 -p: processes unlimited -n: file descriptors 100 -v: address space (kb) unlimited -w: locks unlimited -e: scheduling priority 0 -r: real-time priority 0 使用ab测试，并发90个http请求，创建90个socket，正常运行 / # ab -n 1000000 -c 90 http://61.135.169.125:80/ \u0026 / # lsof | wc -l 108 / # lsof | grep -c ab 94 并发100个http请求，受到ulimit限制 / # ab -n 1000000 -c 100 http://61.135.169.125:80/ This is ApacheBench, Version 2.3 \u003c$Revision: 1843412 $\u003e Copyright 1996 Adam Twiss, Zeus Technology Ltd, http://www.zeustech.net/ Licensed to The Apache Software Foundation, http://www.apache.org/ Benchmarking 61.135.169.125 (be patient) socket: No file descriptors available (24) 线程限制RLIMIT_NPROC This is a limit on the number of extant process (or, more pre‐ cisely on Linux, threads) for the real user ID of the calling process. So long as the current number of processes belonging to this process's real user ID is greater than or equal to this limit, fork(2) fails with the error EAGAIN. The RLIMIT_NPROC limit is not enforced for processes that have either the CAP_SYS_ADMIN or the CAP_SYS_RESOURCE capability. 由定义可知，nproc进程限制的范围是对于每个uid，并且对于root用户无效。 容器uid同一主机上运行的所有容器共享同一个内核(主机的内核)，docker通过namspace对pid/utc/network等进行了隔离，虽然docker中已经实现了user namespace，但由于各种原因，默认没有开启，见docker user namespace $ docker run -d cr.d.xiaomi.net/containercloud/alpine:webtool top 宿主机中查看top进程，显示root用户 $ ps -ef |grep top root 4096 4080 0 15:01 ? 00:00:01 top 容器中查看id，uid为0对应宿主机的root用户,虽然同为root用户，但Linux Capabilities不同，实际权限与宿主机root要少很多 在容器中切换用户到operator(uid为11)，执行sleep命令，主机中查看对应进程用户为app，对应uid同样为11 / # id uid=0(root) gid=0(root) groups=0(root),1(bin),2(daemon),3(sys),4(adm),6(disk),10(wheel),11(floppy),20(dialout),26(tape),27(video) / # su operator / $ id uid=11(operator) gid=0(root) groups=0(root) / $ sleep 100 $ ps -ef |grep 'sleep 100' app 19302 19297 0 16:39 pts/0 00:00:00 sleep 100 $ cat /etc/passwd | grep app app❌11:0::/home/app: 验证不同用户下ulimit的限制设置 ulimit nproc限制soft 10/hard 20，默认启动为root用户 $ docker run -d --ulimit nproc=10:20 cr.d.xiaomi.net/containercloud/alpine:webtool top 进入容器查看， fd soft限制为100个 / # ulimit -a -f: file size (blocks) unlimited -t: cpu time (seconds) unlimited -d: data seg size (kb) unlimited -s: stack size (kb) 8192 -c: core file size (blocks) unlimited -m: resident set size (kb) unlimited -l: locked memory (kb) 64 -p: processes 10 -n: file descriptors 1048576 -v: address space (kb) unlimited -w: locks unlimited -e: scheduling priority 0 -r: real-time priority 0 启动30个进程 / # for i in `seq 30`;do sleep 100 \u0026; done / # ps | wc -l 36 切换到operator用户 / # su operator # 启动多个进程，到第11个进程无法进行fork / $ for i in `seq 8`; do \u003e sleep 100 \u0026 \u003e done / $ sleep 100 \u0026 / $ sleep ","date":"Jul 16, 2019","objectID":"/k8s-limit-fd-and-thread1/:3:2","series":null,"tags":["k8s","docker"],"title":"k8s中fd与thread限制(一)","uri":"/k8s-limit-fd-and-thread1/#验证不同用户下ulimit的限制"},{"categories":["cloud"],"content":" ulimit用户级别资源限制，分为soft限制与hard限制 soft ： 用户可修改，但不能超过硬限制 hard：只有root用户可修改 修改方式： ulimit命令，临时修改；/etc/security/limits.conf，永久修改 工作原理： 根据 PAM （ Pluggable Authentication Modules 简称 PAM）机制，应用程序启动时，按 /etc/pam.d 配置加载 pam_xxxx.so 模块。 /etc/pam.d 下包含了 login 、sshd 、su 、sudo 等程序的 PAM 配置文件， 因此用户重新登录时，将调用 pam_limits.so 加载 limits.conf 配置文件 文件描述符限制RLIMIT_NOFILE This specifies a value one greater than the maximum file descriptor number that can be opened by this process. Attempts (open(2), pipe(2), dup(2), etc.) to exceed this limit yield the error EMFILE. (Historically, this limit was named RLIMIT_OFILE on BSD.) Since Linux 4.5, this limit also defines the maximum number of file descriptors that an unprivileged process (one without the CAP_SYS_RESOURCE capability) may have \"in flight\" to other processes, by being passed across UNIX domain sockets. This limit applies to the sendmsg(2) system call. For further details, see unix(7). 根据定义，nofile 限制进程所能最多打开的文件数量，作用范围进程。 设置 ulimit nofile限制soft 100/hard 200，默认启动为root用户 $ docker run -d --ulimit nofile=100:200 cr.d.xiaomi.net/containercloud/alpine:webtool top 进入容器查看， fd soft限制为100个 / # ulimit -a -f: file size (blocks) unlimited -t: cpu time (seconds) unlimited -d: data seg size (kb) unlimited -s: stack size (kb) 8192 -c: core file size (blocks) unlimited -m: resident set size (kb) unlimited -l: locked memory (kb) 64 -p: processes unlimited -n: file descriptors 100 -v: address space (kb) unlimited -w: locks unlimited -e: scheduling priority 0 -r: real-time priority 0 使用ab测试，并发90个http请求，创建90个socket，正常运行 / # ab -n 1000000 -c 90 http://61.135.169.125:80/ \u0026 / # lsof | wc -l 108 / # lsof | grep -c ab 94 并发100个http请求，受到ulimit限制 / # ab -n 1000000 -c 100 http://61.135.169.125:80/ This is ApacheBench, Version 2.3 \u003c$Revision: 1843412 $\u003e Copyright 1996 Adam Twiss, Zeus Technology Ltd, http://www.zeustech.net/ Licensed to The Apache Software Foundation, http://www.apache.org/ Benchmarking 61.135.169.125 (be patient) socket: No file descriptors available (24) 线程限制RLIMIT_NPROC This is a limit on the number of extant process (or, more pre‐ cisely on Linux, threads) for the real user ID of the calling process. So long as the current number of processes belonging to this process's real user ID is greater than or equal to this limit, fork(2) fails with the error EAGAIN. The RLIMIT_NPROC limit is not enforced for processes that have either the CAP_SYS_ADMIN or the CAP_SYS_RESOURCE capability. 由定义可知，nproc进程限制的范围是对于每个uid，并且对于root用户无效。 容器uid同一主机上运行的所有容器共享同一个内核(主机的内核)，docker通过namspace对pid/utc/network等进行了隔离，虽然docker中已经实现了user namespace，但由于各种原因，默认没有开启，见docker user namespace $ docker run -d cr.d.xiaomi.net/containercloud/alpine:webtool top 宿主机中查看top进程，显示root用户 $ ps -ef |grep top root 4096 4080 0 15:01 ? 00:00:01 top 容器中查看id，uid为0对应宿主机的root用户,虽然同为root用户，但Linux Capabilities不同，实际权限与宿主机root要少很多 在容器中切换用户到operator(uid为11)，执行sleep命令，主机中查看对应进程用户为app，对应uid同样为11 / # id uid=0(root) gid=0(root) groups=0(root),1(bin),2(daemon),3(sys),4(adm),6(disk),10(wheel),11(floppy),20(dialout),26(tape),27(video) / # su operator / $ id uid=11(operator) gid=0(root) groups=0(root) / $ sleep 100 $ ps -ef |grep 'sleep 100' app 19302 19297 0 16:39 pts/0 00:00:00 sleep 100 $ cat /etc/passwd | grep app app❌11:0::/home/app: 验证不同用户下ulimit的限制设置 ulimit nproc限制soft 10/hard 20，默认启动为root用户 $ docker run -d --ulimit nproc=10:20 cr.d.xiaomi.net/containercloud/alpine:webtool top 进入容器查看， fd soft限制为100个 / # ulimit -a -f: file size (blocks) unlimited -t: cpu time (seconds) unlimited -d: data seg size (kb) unlimited -s: stack size (kb) 8192 -c: core file size (blocks) unlimited -m: resident set size (kb) unlimited -l: locked memory (kb) 64 -p: processes 10 -n: file descriptors 1048576 -v: address space (kb) unlimited -w: locks unlimited -e: scheduling priority 0 -r: real-time priority 0 启动30个进程 / # for i in `seq 30`;do sleep 100 \u0026; done / # ps | wc -l 36 切换到operator用户 / # su operator # 启动多个进程，到第11个进程无法进行fork / $ for i in `seq 8`; do \u003e sleep 100 \u0026 \u003e done / $ sleep 100 \u0026 / $ sleep ","date":"Jul 16, 2019","objectID":"/k8s-limit-fd-and-thread1/:3:2","series":null,"tags":["k8s","docker"],"title":"k8s中fd与thread限制(一)","uri":"/k8s-limit-fd-and-thread1/#验证ulimit在不同容器相同uid下的限制"},{"categories":["cloud"],"content":" ulimit用户级别资源限制，分为soft限制与hard限制 soft ： 用户可修改，但不能超过硬限制 hard：只有root用户可修改 修改方式： ulimit命令，临时修改；/etc/security/limits.conf，永久修改 工作原理： 根据 PAM （ Pluggable Authentication Modules 简称 PAM）机制，应用程序启动时，按 /etc/pam.d 配置加载 pam_xxxx.so 模块。 /etc/pam.d 下包含了 login 、sshd 、su 、sudo 等程序的 PAM 配置文件， 因此用户重新登录时，将调用 pam_limits.so 加载 limits.conf 配置文件 文件描述符限制RLIMIT_NOFILE This specifies a value one greater than the maximum file descriptor number that can be opened by this process. Attempts (open(2), pipe(2), dup(2), etc.) to exceed this limit yield the error EMFILE. (Historically, this limit was named RLIMIT_OFILE on BSD.) Since Linux 4.5, this limit also defines the maximum number of file descriptors that an unprivileged process (one without the CAP_SYS_RESOURCE capability) may have \"in flight\" to other processes, by being passed across UNIX domain sockets. This limit applies to the sendmsg(2) system call. For further details, see unix(7). 根据定义，nofile 限制进程所能最多打开的文件数量，作用范围进程。 设置 ulimit nofile限制soft 100/hard 200，默认启动为root用户 $ docker run -d --ulimit nofile=100:200 cr.d.xiaomi.net/containercloud/alpine:webtool top 进入容器查看， fd soft限制为100个 / # ulimit -a -f: file size (blocks) unlimited -t: cpu time (seconds) unlimited -d: data seg size (kb) unlimited -s: stack size (kb) 8192 -c: core file size (blocks) unlimited -m: resident set size (kb) unlimited -l: locked memory (kb) 64 -p: processes unlimited -n: file descriptors 100 -v: address space (kb) unlimited -w: locks unlimited -e: scheduling priority 0 -r: real-time priority 0 使用ab测试，并发90个http请求，创建90个socket，正常运行 / # ab -n 1000000 -c 90 http://61.135.169.125:80/ \u0026 / # lsof | wc -l 108 / # lsof | grep -c ab 94 并发100个http请求，受到ulimit限制 / # ab -n 1000000 -c 100 http://61.135.169.125:80/ This is ApacheBench, Version 2.3 \u003c$Revision: 1843412 $\u003e Copyright 1996 Adam Twiss, Zeus Technology Ltd, http://www.zeustech.net/ Licensed to The Apache Software Foundation, http://www.apache.org/ Benchmarking 61.135.169.125 (be patient) socket: No file descriptors available (24) 线程限制RLIMIT_NPROC This is a limit on the number of extant process (or, more pre‐ cisely on Linux, threads) for the real user ID of the calling process. So long as the current number of processes belonging to this process's real user ID is greater than or equal to this limit, fork(2) fails with the error EAGAIN. The RLIMIT_NPROC limit is not enforced for processes that have either the CAP_SYS_ADMIN or the CAP_SYS_RESOURCE capability. 由定义可知，nproc进程限制的范围是对于每个uid，并且对于root用户无效。 容器uid同一主机上运行的所有容器共享同一个内核(主机的内核)，docker通过namspace对pid/utc/network等进行了隔离，虽然docker中已经实现了user namespace，但由于各种原因，默认没有开启，见docker user namespace $ docker run -d cr.d.xiaomi.net/containercloud/alpine:webtool top 宿主机中查看top进程，显示root用户 $ ps -ef |grep top root 4096 4080 0 15:01 ? 00:00:01 top 容器中查看id，uid为0对应宿主机的root用户,虽然同为root用户，但Linux Capabilities不同，实际权限与宿主机root要少很多 在容器中切换用户到operator(uid为11)，执行sleep命令，主机中查看对应进程用户为app，对应uid同样为11 / # id uid=0(root) gid=0(root) groups=0(root),1(bin),2(daemon),3(sys),4(adm),6(disk),10(wheel),11(floppy),20(dialout),26(tape),27(video) / # su operator / $ id uid=11(operator) gid=0(root) groups=0(root) / $ sleep 100 $ ps -ef |grep 'sleep 100' app 19302 19297 0 16:39 pts/0 00:00:00 sleep 100 $ cat /etc/passwd | grep app app❌11:0::/home/app: 验证不同用户下ulimit的限制设置 ulimit nproc限制soft 10/hard 20，默认启动为root用户 $ docker run -d --ulimit nproc=10:20 cr.d.xiaomi.net/containercloud/alpine:webtool top 进入容器查看， fd soft限制为100个 / # ulimit -a -f: file size (blocks) unlimited -t: cpu time (seconds) unlimited -d: data seg size (kb) unlimited -s: stack size (kb) 8192 -c: core file size (blocks) unlimited -m: resident set size (kb) unlimited -l: locked memory (kb) 64 -p: processes 10 -n: file descriptors 1048576 -v: address space (kb) unlimited -w: locks unlimited -e: scheduling priority 0 -r: real-time priority 0 启动30个进程 / # for i in `seq 30`;do sleep 100 \u0026; done / # ps | wc -l 36 切换到operator用户 / # su operator # 启动多个进程，到第11个进程无法进行fork / $ for i in `seq 8`; do \u003e sleep 100 \u0026 \u003e done / $ sleep 100 \u0026 / $ sleep ","date":"Jul 16, 2019","objectID":"/k8s-limit-fd-and-thread1/:3:2","series":null,"tags":["k8s","docker"],"title":"k8s中fd与thread限制(一)","uri":"/k8s-limit-fd-and-thread1/#总结"},{"categories":["cloud"],"content":" cgroupcgroup中对pid进行了隔离，通过更改docker/kubelet配置，可以限制pid总数，从而达到限制线程总数的目的。线程数限制与系统中多处配置有关，取最小值，参考stackoverflow上线程数的设置 docker，容器启动时设置 –pids-limit 参数，限制容器级别pid总数 kubelet，开启SupportPodPidsLimit特性，设置–pod-max-pids参数，限制node每个pod的pid总数 以kubelet为例，开启SupportPodPidsLimit，--feature-gates=SupportPodPidsLimit=true 配置kubelet，每个pod允许最大pid数目为150 [root@node01 ~]# ps -ef |grep kubelet root 18735 1 14 11:19 ? 00:53:28 ./kubelet --v=1 --address=0.0.0.0 --feature-gates=SupportPodPidsLimit=true --pod-max-pids=150 --allow-privileged=true --pod-infra-container-image=cr.d.xiaomi.net/kubernetes/pause-amd64:3.1 --root-dir=/home/kubelet --node-status-update-frequency=5s --kubeconfig=/home/xbox/kubelet/conf/kubelet-kubeconfig --fail-swap-on=false --max-pods=254 --runtime-cgroups=/systemd/system.slice/frigga.service --kubelet-cgroups=/systemd/system.slice/frigga.service --make-iptables-util-chains=false 在pod中起测试线程，root下起100个线程 / # for i in `seq 100`; do \u003e sleep 1000 \u0026 \u003e done / # ps | wc -l 106 operator 下，创建线程受到限制，系统最多只能创建150个 / # su operator / $ / $ for i in `seq 100`; do \u003e sleep 1000 \u0026 \u003e done sh: can't fork: Resource temporarily unavailable / $ ps | wc -l 150 在cgroup中查看，pids达到最大限制 [root@node01 ~]# cat /sys/fs/cgroup/pids/kubepods/besteffort/pod8b61d4de-a7ad-11e9-b5b9-246e96ad0900/pids.current 150 [root@node01 ~]# cat /sys/fs/cgroup/pids/kubepods/besteffort/pod8b61d4de-a7ad-11e9-b5b9-246e96ad0900/pids.max 150 总结 cgroup对于pid的限制能够达到限制线程数目的，目前docker只支持对每个容器的限制，不支持全局配置；kubelet只支持对于node所有pod的全局配置，不支持具体每个pod的配置 ","date":"Jul 16, 2019","objectID":"/k8s-limit-fd-and-thread1/:3:3","series":null,"tags":["k8s","docker"],"title":"k8s中fd与thread限制(一)","uri":"/k8s-limit-fd-and-thread1/#cgroup"},{"categories":["cloud"],"content":" limits.conf/sysctl.conflimits.conf是ulimit的具体配置，目录项/etc/security/limit.d/中的配置会覆盖limits.conf。 sysctl.conf为机器级别的资源限制，root用户可修改，目录项/etc/security/sysctl.d/中的配置会覆盖sysctl.conf，在/etc/sysctl.conf中添加对应配置（fd: fs.file-max = {}; pid: kernel.pid_max = {}） 测试容器中修改sysctl.conf文件 $ docker run -d --ulimit nofile=100:200 cr.d.xiaomi.net/containercloud/alpine:webtool top cb1250c8fd217258da51c6818fa2ce2e2f6e35bf1d52648f1f432e6ce579cf0d $ docker exec -it cb1250c sh / # ulimit -a -f: file size (blocks) unlimited -t: cpu time (seconds) unlimited -d: data seg size (kb) unlimited -s: stack size (kb) 8192 -c: core file size (blocks) unlimited -m: resident set size (kb) unlimited -l: locked memory (kb) 64 -p: processes unlimited -n: file descriptors 100 -v: address space (kb) unlimited -w: locks unlimited -e: scheduling priority 0 -r: real-time priority 0 / # / # echo 10 \u003e /proc/sys/kernel/pid_max sh: can't create /proc/sys/kernel/pid_max: Read-only file system / # echo 10 \u003e /proc/sys/kernel/pid_max sh: can't create /proc/sys/kernel/pid_max: Read-only file system / # echo \"fs.file-max=5\" \u003e\u003e /etc/sysctl.conf / # sysctl -p sysctl: error setting key 'fs.file-max': Read-only file system 以priviledged模式测试，谨慎测试 $ cat /proc/sys/kernel/pid_max 32768 $ docker run -d -- --ulimit nofile=100:200 cr.d.xiaomi.net/containercloud/alpine:webtool top $ docker exec -it pedantic_vaughan sh / # cat /proc/sys/kernel/pid_max 32768 / # echo 50000 \u003e /proc/sys/kernel/pid_max / # cat /proc/sys/kernel/pid_max 50000 / # exit $ cat /proc/sys/kernel/pid_max 50000 # 宿主机的文件也变成50000 总结 由于docker隔离的不彻底，在docker中修改sysctl会覆盖主机中的配置，不能用来实现容器级别资源限制 limits.conf可以在容器中设置，效果同ulimit ","date":"Jul 16, 2019","objectID":"/k8s-limit-fd-and-thread1/:3:4","series":null,"tags":["k8s","docker"],"title":"k8s中fd与thread限制(一)","uri":"/k8s-limit-fd-and-thread1/#limitsconfsysctlconf"},{"categories":["cloud"],"content":" 结论 推荐方案如下： fd限制： 修改dockerd配置default-ulimits，限制进程级别fd thread限制：修改kubelet配置--feature-gates=SupportPodPidsLimit=true --pod-max-pids={}，cgroup级别限制pid，从而限制线程数 其他注意事项，调整节点pid.max参数；放开或者调大镜像中ulimit对非root账户nproc限制 ","date":"Jul 16, 2019","objectID":"/k8s-limit-fd-and-thread1/:4:0","series":null,"tags":["k8s","docker"],"title":"k8s中fd与thread限制(一)","uri":"/k8s-limit-fd-and-thread1/#结论"},{"categories":["cloud"],"content":" 引用 https://docs.docker.com/engine/reference/commandline/run/#set-ulimits-in-container---ulimit http://man7.org/linux/man-pages/man2/getrlimit.2.html https://feichashao.com/ulimit_demo/ https://medium.com/@mccode/understanding-how-uid-and-gid-work-in-docker-containers-c37a01d01cf https://docs.docker.com/engine/security/userns-remap/ ","date":"Jul 16, 2019","objectID":"/k8s-limit-fd-and-thread1/:5:0","series":null,"tags":["k8s","docker"],"title":"k8s中fd与thread限制(一)","uri":"/k8s-limit-fd-and-thread1/#引用"},{"categories":["cloud"],"content":" 背景在容器化环境中，平台需要提供准确的业务监控指标，已方便业务查看。那么如何准确计算容器或Pod的内存使用率，k8s/docker又是如何计算，本文通过实验与源码阅读相结合来分析容器的内存实际使用量。 ","date":"May 29, 2019","objectID":"/container-memory/:1:0","series":null,"tags":["k8s","docker","cgroup"],"title":"容器内存分析","uri":"/container-memory/#背景"},{"categories":["cloud"],"content":" 预备知识不管docker还是k8s(通过cadvisor)最终都通过cgroup的memory group来得到内存的原始文件，memory相关的主要文件如下: cgroup.event_control #用于eventfd的接口 memory.usage_in_bytes #显示当前已用的内存 memory.limit_in_bytes #设置/显示当前限制的内存额度 memory.failcnt #显示内存使用量达到限制值的次数 memory.max_usage_in_bytes #历史内存最大使用量 memory.soft_limit_in_bytes #设置/显示当前限制的内存软额度 memory.stat #显示当前cgroup的内存使用情况 memory.use_hierarchy #设置/显示是否将子cgroup的内存使用情况统计到当前cgroup里面 memory.force_empty #触发系统立即尽可能的回收当前cgroup中可以回收的内存 memory.pressure_level #设置内存压力的通知事件，配合cgroup.event_control一起使用 memory.swappiness #设置和显示当前的swappiness memory.move_charge_at_immigrate #设置当进程移动到其他cgroup中时，它所占用的内存是否也随着移动过去 memory.oom_control #设置/显示oom controls相关的配置 memory.numa_stat #显示numa相关的内存 更多信息可参考Pod memory usage in k8s ","date":"May 29, 2019","objectID":"/container-memory/:2:0","series":null,"tags":["k8s","docker","cgroup"],"title":"容器内存分析","uri":"/container-memory/#预备知识"},{"categories":["cloud"],"content":" 查看源码","date":"May 29, 2019","objectID":"/container-memory/:3:0","series":null,"tags":["k8s","docker","cgroup"],"title":"容器内存分析","uri":"/container-memory/#查看源码"},{"categories":["cloud"],"content":" docker statdocker stat的源码在stats_helpers.go,如下： func calculateMemUsageUnixNoCache(mem types.MemoryStats) float64 { return float64(mem.Usage - mem.Stats[\"cache\"]) } 内存使用量为memory.usage=memory.usage_in_bytes-cache ","date":"May 29, 2019","objectID":"/container-memory/:3:1","series":null,"tags":["k8s","docker","cgroup"],"title":"容器内存分析","uri":"/container-memory/#docker-stat"},{"categories":["cloud"],"content":" kubectl top在k8s中，kubectl top命令通过metric-server/heapster获取cadvisor中working_set的值，来表示Pod实例使用内存大小(不包括pause),metrics-server 中pod内存获取如下： func decodeMemory(target *resource.Quantity, memStats *stats.MemoryStats) error { if memStats == nil || memStats.WorkingSetBytes == nil { return fmt.Errorf(\"missing memory usage metric\") } *target = *uint64Quantity(*memStats.WorkingSetBytes, 0) target.Format = resource.BinarySI return nil } cadvisor中working_set计算如下： func setMemoryStats(s *cgroups.Stats, ret *info.ContainerStats) { ret.Memory.Usage = s.MemoryStats.Usage.Usage ret.Memory.MaxUsage = s.MemoryStats.Usage.MaxUsage ret.Memory.Failcnt = s.MemoryStats.Usage.Failcnt if s.MemoryStats.UseHierarchy { ret.Memory.Cache = s.MemoryStats.Stats[\"total_cache\"] ret.Memory.RSS = s.MemoryStats.Stats[\"total_rss\"] ret.Memory.Swap = s.MemoryStats.Stats[\"total_swap\"] ret.Memory.MappedFile = s.MemoryStats.Stats[\"total_mapped_file\"] } else { ret.Memory.Cache = s.MemoryStats.Stats[\"cache\"] ret.Memory.RSS = s.MemoryStats.Stats[\"rss\"] ret.Memory.Swap = s.MemoryStats.Stats[\"swap\"] ret.Memory.MappedFile = s.MemoryStats.Stats[\"mapped_file\"] } if v, ok := s.MemoryStats.Stats[\"pgfault\"]; ok { ret.Memory.ContainerData.Pgfault = v ret.Memory.HierarchicalData.Pgfault = v } if v, ok := s.MemoryStats.Stats[\"pgmajfault\"]; ok { ret.Memory.ContainerData.Pgmajfault = v ret.Memory.HierarchicalData.Pgmajfault = v } workingSet := ret.Memory.Usage if v, ok := s.MemoryStats.Stats[\"total_inactive_file\"]; ok { if workingSet \u003c v { workingSet = 0 } else { workingSet -= v } } ret.Memory.WorkingSet = workingSet } working_set=memory.usage_in_bytes-total_inactive_file (\u003e=0) 在kubelet中节点内存不足时同样以working_set判断pod是否OOM的标准 ","date":"May 29, 2019","objectID":"/container-memory/:3:2","series":null,"tags":["k8s","docker","cgroup"],"title":"容器内存分析","uri":"/container-memory/#kubectl-top"},{"categories":["cloud"],"content":" 实验 创建Pod Pod的资源申请如下： resources: limits: cpu: \"1\" memory: 1Gi requests: cpu: \"0\" memory: \"0\" 查看cgroup内存情况 找到容器某个进程，查看memory cgroup # cat /proc/16062/cgroup ... 8:memory:/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod21a55da5_f9f8_11e9_b051_fa163e7e981a.slice/docker-57ba1991ab4ba50a9b2eaf5bf90e2c20073198d767653becf77d55ee25e1a6f9.scope 进入容器memory cgroup对应的目录 docker-57ba1991ab4ba50a9b2eaf5bf90e2c20073198d767653becf77d55ee25e1a6f9.scope]# ls cgroup.clone_children memory.kmem.failcnt memory.kmem.tcp.limit_in_bytes memory.max_usage_in_bytes memory.move_charge_at_immigrate memory.stat tasks cgroup.event_control memory.kmem.limit_in_bytes memory.kmem.tcp.max_usage_in_bytes memory.memsw.failcnt memory.numa_stat memory.swappiness cgroup.procs memory.kmem.max_usage_in_bytes memory.kmem.tcp.usage_in_bytes memory.memsw.limit_in_bytes memory.oom_control memory.usage_in_bytes memory.failcnt memory.kmem.slabinfo memory.kmem.usage_in_bytes memory.memsw.max_usage_in_bytes memory.pressure_level memory.use_hierarchy memory.force_empty memory.kmem.tcp.failcnt memory.limit_in_bytes memory.memsw.usage_in_bytes memory.soft_limit_in_bytes notify_on_release 查看主要memory文件 # cat memory.limit_in_bytes (容器memory limit值，即1Gi) 1073741824 [root@node01 docker-57ba1991ab4ba50a9b2eaf5bf90e2c20073198d767653becf77d55ee25e1a6f9.scope]# cat memory.kmem.limit_in_bytes (容器内核使用memory limit值) 9223372036854771712 [root@node01 docker-57ba1991ab4ba50a9b2eaf5bf90e2c20073198d767653becf77d55ee25e1a6f9.scope]# [root@node01 docker-57ba1991ab4ba50a9b2eaf5bf90e2c20073198d767653becf77d55ee25e1a6f9.scope]# cat memory.soft_limit_in_bytes 9223372036854771712 [docker-57ba1991ab4ba50a9b2eaf5bf90e2c20073198d767653becf77d55ee25e1a6f9.scope]# cat notify_on_release 0 [docker-57ba1991ab4ba50a9b2eaf5bf90e2c20073198d767653becf77d55ee25e1a6f9.scope]# cat memory.oom_control oom_kill_disable 0 under_oom 0 oom_kill 0 [docker-57ba1991ab4ba50a9b2eaf5bf90e2c20073198d767653becf77d55ee25e1a6f9.scope]# cat memory.usage_in_bytes 2265088 [docker-57ba1991ab4ba50a9b2eaf5bf90e2c20073198d767653becf77d55ee25e1a6f9.scope]# cat memory.kmem.usage_in_bytes 901120 [docker-57ba1991ab4ba50a9b2eaf5bf90e2c20073198d767653becf77d55ee25e1a6f9.scope]# cat memory.stat cache 12288 rss 1351680 rss_huge 0 shmem 4096 mapped_file 4096 dirty 0 writeback 0 swap 0 pgpgin 4544 pgpgout 4211 pgfault 1948 pgmajfault 0 inactive_anon 4096 active_anon 1351680 inactive_file 8192 active_file 0 unevictable 0 hierarchical_memory_limit 1073741824 hierarchical_memsw_limit 1073741824 total_cache 12288 total_rss 1351680 total_rss_huge 0 total_shmem 4096 total_mapped_file 4096 total_dirty 0 total_writeback 0 total_swap 0 total_pgpgin 4544 total_pgpgout 4211 total_pgfault 1948 total_pgmajfault 0 total_inactive_anon 4096 total_active_anon 1351680 total_inactive_file 8192 total_active_file 0 total_unevictable 0 根据memory可得到如下关系： memory.usage_in_bytes = memory.kmem.usage_in_bytes + rss + cache 即2265088=901120+1351680+12288 那么容器的真实内存即： memory.usage=memory.usage_in_bytes-cache 即rss+kmem_usage 通过docker stat查看，与公式相符合 CONTAINER ID NAME CPU % MEM USAGE / LIMIT MEM % NET I/O BLOCK I/O PIDS 57ba1991ab4b k8s...default_21a55da5-f9f8-11e9-b051-fa163e7e981a_0 0.00% 2.148MiB / 1GiB 0.21% 12MB / 68.8MB 0B / 0B 2 ","date":"May 29, 2019","objectID":"/container-memory/:4:0","series":null,"tags":["k8s","docker","cgroup"],"title":"容器内存分析","uri":"/container-memory/#实验"},{"categories":["cloud"],"content":" 结论实际环境中，docker与k8s两种内存表示方式不同，一般docker stat总体值会小于kubectl top docker中内存表示为： memory.usage = memory.usage_in_bytes - cache k8s中： memory.usage = working_set = memory.usage_in_bytes - total_inactive_file (\u003e=0) 根据cgroup memory关系有： memory.usage_in_bytes = memory.kmem.usage_in_bytes + rss + cache 真实环境中两种表示相差不大，但更推荐使用working_set作为容器内存真实使用量(kubelt判断OOM的依据)， 则容器内存使用率可表示为： container_memory_working_set_bytes / memory.limit_in_bytes ","date":"May 29, 2019","objectID":"/container-memory/:5:0","series":null,"tags":["k8s","docker","cgroup"],"title":"容器内存分析","uri":"/container-memory/#结论"},{"categories":["cloud"],"content":" 参考 https://www.kernel.org/doc/Documentation/cgroup-v1/memory.txt https://medium.com/@zhimin.wen/memory-limit-of-pod-and-oom-killer-891ee1f1cad8 ","date":"May 29, 2019","objectID":"/container-memory/:6:0","series":null,"tags":["k8s","docker","cgroup"],"title":"容器内存分析","uri":"/container-memory/#参考"},{"categories":["cloud"],"content":"k8s组件日志级别热更新 # 调整日志级别到3 curl -X PUT http://127.0.0.1:8081/debug/flags/v -d \"3\" controller manager wget http://localhost:10252/debug/pprof/profile wget http://localhost:10252/debug/pprof/heap curl http://127.0.0.1:10252/debug/pprof/goroutine?debug=1 \u003e\u003e debug1 curl http://127.0.0.1:10252/debug/pprof/goroutine?debug=2 \u003e\u003e debug2 scheduler kill -12 ${SCHED_PID} 获取scheduler cache信息，输出到日志 kubelet 堆栈信息 wget http://localhost:10250/debug/pprof/profile wget http://localhost:10250/debug/pprof/heap curl http://127.0.0.1:10250/debug/pprof/goroutine?debug=1 \u003e\u003e debug1 curl http://127.0.0.1:10250/debug/pprof/goroutine?debug=2 \u003e\u003e debug2 docker 堆栈信息 curl --unix-socket /var/run/docker.sock -X GET http://v1.2/debug/pprof/profile curl --unix-socket /var/run/docker.sock -X GET http://v1.2/debug/pprof/ curl --unix-socket /var/run/docker.sock -X GET http://v1.2/debug/pprof/ sudo kill -SIGUSR1 $(pidof dockerd) /var/run/docker/ curl --unix-socket /var/run/docker.sock -X GET http://v1.2/debug/pprof/profile \u003e\u003edocker.profile curl --unix-socket /var/run/docker.sock -X GET http://v1.2/debug/pprof/goroutine \u003e\u003e docker.goroutine curl --unix-socket /var/run/docker.sock -X GET http://v1.2/debug/pprof/goroutine?debug=2 \u003e\u003edocker.gorouting_debug_2 curl --unix-socket /var/run/docker.sock -X GET http://v1.2/debug/pprof/heap?debug=2 \u003e\u003edocker.heap docker-registry 堆栈信息 #先登入机器,然后执行 wget localhost:5002/debug/pprof/profile #这个是cpu占用时间的采样结果，要先等30s wget localhost:5002/debug/pprof/heap #内存的使用情况 wget localhost:5002/debug/pprof/goroutine?debug=2 #调用栈的全部信息 wget localhost:5002/debug/pprof/goroutine 其他可用的profile: allocs block goroutine cmdline mutex threadcreate trace，替换上面命令pprof/后面的词即可 ","date":"May 11, 2019","objectID":"/k8s-docker-stack/:0:0","series":null,"tags":["k8s","docker","pprof"],"title":"k8s与docker组件堆栈及Debug","uri":"/k8s-docker-stack/#"},{"categories":["tool"],"content":" 输出x509证书信息openssl x509 -noout -text -in ca.pem 结果如下 Certificate: Data: Version: 3 (0x2) Serial Number: 5f:11:aa:b3:70:18:fd:89:b0:25:7a:9e:36:c5:e7:ce:33:5a:cc:b7 Signature Algorithm: sha256WithRSAEncryption Issuer: C=CN, ST=BeiJing, L=BeiJing, O=xx, OU=xx, CN=xx Validity Not Before: Dec 26 06:17:00 2019 GMT Not After : Dec 2 06:17:00 2119 GMT #过期时间 Subject: C=CN, ST=BeiJing, L=BeiJing, O=xx, OU=xx, CN=xx Subject Public Key Info: ... ","date":"May 10, 2019","objectID":"/openssl-cmd/:0:1","series":null,"tags":["openssl"],"title":"openssl常用命令","uri":"/openssl-cmd/#输出x509证书信息"},{"categories":["tool"],"content":" 验证公钥私钥是否匹配diff -eq \u003c(openssl x509 -pubkey -noout -in cert.crt) \u003c(openssl rsa -pubout -in cert.key) 正常会输出 writing RSA key ","date":"May 10, 2019","objectID":"/openssl-cmd/:0:2","series":null,"tags":["openssl"],"title":"openssl常用命令","uri":"/openssl-cmd/#验证公钥私钥是否匹配"},{"categories":["tool"],"content":" 验证证书CAopenssl verify -CAfile ca.pem client.pem 正常输出 client.pem: OK ","date":"May 10, 2019","objectID":"/openssl-cmd/:0:3","series":null,"tags":["openssl"],"title":"openssl常用命令","uri":"/openssl-cmd/#验证证书ca"},{"categories":["cloud"],"content":" 背景单个Prometheus Server可以轻松的处理数以百万的时间序列。但当机器规模过大时，需要对其进行分区，Prometheus也提供了集群联邦的功能，方便对其扩展。 我们采用Prometheus来监控k8s集群，节点数400，采集的samples是280w，Prometheus官方的显示每秒可抓取10w samples。当集群规模扩大到上千节点时，单个Prometheus不足以处理大量数据，需要对其进行分区。 可以根据scrape_samples_scraped{job=${JOBNAME}}来统计各个job的samples数目 可以根据count({__name__=~\".*:.*\"})来统计metrics总数 ","date":"Feb 12, 2019","objectID":"/prometheus-federation/:1:0","series":null,"tags":["k8s","prometheus"],"title":"Prometheus分区实践","uri":"/prometheus-federation/#背景"},{"categories":["cloud"],"content":" 集群联邦在Promehtues的源码中，federate联邦功能在web中，是一个特殊的查询接口，允许一个prometheus抓取另一个prometheus的metrics 可以通过全局的prometheus抓取其他slave prometheus从而达到分区的目的 使用federate进行分区通过有两种方式 ","date":"Feb 12, 2019","objectID":"/prometheus-federation/:2:0","series":null,"tags":["k8s","prometheus"],"title":"Prometheus分区实践","uri":"/prometheus-federation/#集群联邦"},{"categories":["cloud"],"content":" 功能分区每个模块为一个分区，如node-exporter为一个分区，kube-state-metrics为一个分区，再使用全局的Prometheus汇总 实现简单，但当单个job采集任务过大（如node-exporter）时，单个Prometheus slave也会成为瓶颈 ","date":"Feb 12, 2019","objectID":"/prometheus-federation/:2:1","series":null,"tags":["k8s","prometheus"],"title":"Prometheus分区实践","uri":"/prometheus-federation/#功能分区"},{"categories":["cloud"],"content":" 水平扩展针对功能分区的不足，将同一任务的不同实例的监控数据采集任务划分到不同的Prometheus实例。通过relabel设置，我们可以确保当前Prometheus Server只收集当前采集任务的一部分实例的监控指标。 下为官方提供的配置 global: external_labels: slave: 1 # This is the 2nd slave. This prevents clashes between slaves. scrape_configs: - job_name: some_job # Add usual service discovery here, such as static_configs relabel_configs: - source_labels: [__address__] modulus: 4 # 4 slaves target_label: __tmp_hash action: hashmod - source_labels: [__tmp_hash] regex: ^1$ # This is the 2nd slave action: keep 并且通过当前数据中心的一个中心Prometheus Server将监控数据进行聚合到任务级别。 - scrape_config: - job_name: slaves honor_labels: true metrics_path: /federate params: match[]: - '{__name__=~\"^slave:.*\"}' # Request all slave-level time series static_configs: - targets: - slave0:9090 - slave1:9090 - slave3:9090 - slave4:9090 水平扩展，即通过联邦集群的特性在任务的实例级别对Prometheus采集任务进行划分，以支持规模的扩展。 ","date":"Feb 12, 2019","objectID":"/prometheus-federation/:2:2","series":null,"tags":["k8s","prometheus"],"title":"Prometheus分区实践","uri":"/prometheus-federation/#水平扩展"},{"categories":["cloud"],"content":" 我们的方案","date":"Feb 12, 2019","objectID":"/prometheus-federation/:3:0","series":null,"tags":["k8s","prometheus"],"title":"Prometheus分区实践","uri":"/prometheus-federation/#我们的方案"},{"categories":["cloud"],"content":" 整体架构 Promehtues以容器化的方式部署在k8s集群中 收集node-exporter、cadvisor、kubelet、kube-state-metrics、k8s核心组件、自定义metrics 通过实现opentsdb-adapter，对监控数据做持久化 通过falcon-adapter,为监控数据提供报警 ","date":"Feb 12, 2019","objectID":"/prometheus-federation/:3:1","series":null,"tags":["k8s","prometheus"],"title":"Prometheus分区实践","uri":"/prometheus-federation/#整体架构"},{"categories":["cloud"],"content":" 分区方案 Prometheus分区包括master Prometheus 与 slave Promehtues 我们将监控数据分为多个层次: cluster, namespace, deployment/daemonset, pod, node 由于kubelet, node-exporter, cadvisor等是以node为单位采集的，所以安装node节点来划分不同job slave Prometheus 按照node切片采集node，pod级别数据 kube-state-metrics暂时无法切片，可通过replicaset 设置多个，单独作为一个kube-state Prometheus，供其他slave Prometheus采集 其他etcd, apiserver等自定义组件可通过master Promehtues直接采集 整体架构如下 master Prometheus配置 global: scrape_interval: 60s scrape_timeout: 30s evaluation_interval: 60s external_labels: cluster: {{CLUSTER}} production_environment: {{ENV}} rule_files: - cluster.yml - namespace.yml - deployment.yml - daemonset.yml scrape_configs: - job_name: federate-slave honor_labels: true metrics_path: '/federate' params: 'match[]': - '{__name__=~\"pod:.*|node:.*\"}' kubernetes_sd_configs: - role: pod namespaces: names: - kube-system relabel_configs: - source_labels: - __meta_kubernetes_pod_label_app action: keep regex: prometheus-slave.* - source_labels: - __meta_kubernetes_pod_container_port_number action: keep regex: 9090 - job_name: federate-kubestate honor_labels: true metrics_path: '/federate' params: 'match[]': - '{__name__=~\"deployment:.*|daemonset:.*\"}' kubernetes_sd_configs: - role: pod namespaces: names: - kube-system relabel_configs: - source_labels: - __meta_kubernetes_pod_label_app action: keep regex: prometheus-kubestate.* - source_labels: - __meta_kubernetes_pod_container_port_number action: keep regex: 9090 slave Prometheus配置 global: scrape_interval: 60s scrape_timeout: 30s evaluation_interval: 60s external_labels: cluster: {{CLUSTER}} production_environment: {{ENV}} rule_files: - node.yml - pod.yml scrape_configs: - job_name: federate-kubestate honor_labels: true metrics_path: '/federate' params: 'match[]': - '{__name__=~\"pod:.*|node:.*\"}' kubernetes_sd_configs: - role: pod namespaces: names: - kube-system relabel_configs: - source_labels: - __meta_kubernetes_pod_label_app action: keep regex: prometheus-kubestate.* - source_labels: - __meta_kubernetes_pod_container_port_number action: keep regex: 9090 metric_relabel_configs: - source_labels: [node] modulus: {{MODULES}} target_label: __tmp_hash action: hashmod - source_labels: [__tmp_hash] regex: {{SLAVEID}} action: keep - job_name: kubelet scheme: https kubernetes_sd_configs: - role: node namespaces: names: [] tls_config: insecure_skip_verify: true relabel_configs: - source_labels: [] regex: __meta_kubernetes_node_label_(.+) replacement: \"$1\" action: labelmap - source_labels: [__meta_kubernetes_node_label_kubernetes_io_hostname] modulus: {{MODULES}} target_label: __tmp_hash action: hashmod - source_labels: [__tmp_hash] regex: {{SLAVEID}} action: keep - job_name: ... ","date":"Feb 12, 2019","objectID":"/prometheus-federation/:3:2","series":null,"tags":["k8s","prometheus"],"title":"Prometheus分区实践","uri":"/prometheus-federation/#分区方案"},{"categories":["cloud"],"content":" 可能的问题 如何部署，配置复杂，现在采用shell脚本加kustomize,是否有更简单的方法 分区的动态扩展随着node的规模 kube-state-metrics是否会成为瓶颈，目前的kube-state-metrics性能测试 由于分区同一个job的不同instance采集的时间有偏差，对聚合有一定影响 可靠性保证，如果一个或多个slave的挂了如何处理，使用k8s来保证prometheus的可用性是否可靠 ","date":"Feb 12, 2019","objectID":"/prometheus-federation/:3:3","series":null,"tags":["k8s","prometheus"],"title":"Prometheus分区实践","uri":"/prometheus-federation/#可能的问题"},{"categories":["cloud"],"content":" 环境k8s: 1.10.2 docker: 17.03 ","date":"Jan 18, 2019","objectID":"/pod-outofcpu-error/:1:0","series":null,"tags":["k8s"],"title":"k8s节点资源不足时OutOfcpu错误","uri":"/pod-outofcpu-error/#环境"},{"categories":["cloud"],"content":" 问题当指定nodeName并且节点资源不足时，会创建大量pod，并显示outofcpu/outofmem 类似下面: prometheus-slave01-68bd9bc854-slw92 0/2 OutOfcpu 0 1m prometheus-slave01-68bd9bc854-svxbq 0/2 OutOfcpu 0 20s prometheus-slave01-68bd9bc854-sw25t 0/2 OutOfcpu 0 1m ","date":"Jan 18, 2019","objectID":"/pod-outofcpu-error/:2:0","series":null,"tags":["k8s"],"title":"k8s节点资源不足时OutOfcpu错误","uri":"/pod-outofcpu-error/#问题"},{"categories":["cloud"],"content":" 相关issuehttps://github.com/kubernetes/kubernetes/issues/38806 ","date":"Jan 18, 2019","objectID":"/pod-outofcpu-error/:3:0","series":null,"tags":["k8s"],"title":"k8s节点资源不足时OutOfcpu错误","uri":"/pod-outofcpu-error/#相关issue"},{"categories":["cloud"],"content":" 解析设置nodeName会跳过调度，没有对容量做检测 分配到节点上显示资源不足，状态变为outofcpu/outofmem，k8s判断replicaset没有检测到期望pod的状态，会重新再起一个pod，而原pod不会主动删除，致使创建大量pod ","date":"Jan 18, 2019","objectID":"/pod-outofcpu-error/:4:0","series":null,"tags":["k8s"],"title":"k8s节点资源不足时OutOfcpu错误","uri":"/pod-outofcpu-error/#解析"},{"categories":["cloud"],"content":" 附件测试yaml apiVersion: extensions/v1beta1 kind: Deployment metadata: name: my-nginx namespace: kube-system spec: replicas: 1 template: metadata: labels: app: my-nginx spec: nodeName: tj1-jm-cc-stag05.kscn containers: - name: my-nginx image: nginx ports: - containerPort: 80 resources: limits: cpu: 200 requests: cpu: 100 ","date":"Jan 18, 2019","objectID":"/pod-outofcpu-error/:5:0","series":null,"tags":["k8s"],"title":"k8s节点资源不足时OutOfcpu错误","uri":"/pod-outofcpu-error/#附件"},{"categories":["cloud"],"content":" 写在前面当我开始大范围使用Kubernetes的时候，我开始考虑一个我做实验时没有遇到的问题：当集群里的节点没有足够资源的时候，Pod会卡在Pending状态。你是没有办法给节点增加CPU或者内存的，那么你该怎么做才能将这个Pod从这个节点拿走？最简单的办法是添加另一个节点，我承认我总是这么干。最终这个策略无法发挥出Kubernetes最重要的一个能力：即它优化计算资源使用的能力。这些场景里面实际的问题并不是节点太小，而是我们没有仔细为Pod计算过资源限制。 资源限制是我们可以向Kubernetes提供的诸多配置之一，它意味着两点：工作负载运行需要哪些资源；最多允许消费多少资源。第一点对于调度器而言十分重要，因为它要以此选择合适的节点。第二点对于Kubelet非常重要，每个节点上的守护进程Kubelet负责Pod的运行健康状态。大多数本文的读者可能对资源限制有一定的了解，实际上这里面有很多有趣的细节。在这个系列的两篇文章中我会先仔细分析内存资源限制，然后第二篇文章中分析CPU资源限制。 ","date":"Jan 09, 2019","objectID":"/understanding-resource-limits-in-kubernetes/:1:0","series":null,"tags":["k8s","docker","cgroup"],"title":"深入理解K8s资源限制","uri":"/understanding-resource-limits-in-kubernetes/#写在前面"},{"categories":["cloud"],"content":" 资源限制资源限制是通过每个容器containerSpec的resources字段进行设置的，它是v1版本的ResourceRequirements类型的API对象。每个指定了\"limits\"和\"requests\"的对象都可以控制对应的资源。目前只有CPU和内存两种资源。第三种资源类型，持久化存储仍然是beta版本，我会在以后的博客里进行分析。大多数情况下，deployment、statefulset、daemonset的定义里都包含了podSpec和多个containerSpec。这里有个完整的v1资源对象的yaml格式配置： resources: requests: cpu: 50m memory: 50Mi limits: cpu: 100m memory: 100Mi 这个对象可以这么理解：这个容器通常情况下，需要5%的CPU时间和50MiB的内存（requests），同时最多允许它使用10%的CPU时间和100MiB的内存（limits）。我会对requests和limits的区别做进一步讲解，但是一般来说，在调度的时候requests比较重要，在运行时limits比较重要。尽管资源限制配置在每个容器上，你可以认为Pod的资源限制就是它里面容器的资源限制之和，我们可以从系统的视角观察到这种关系。 ","date":"Jan 09, 2019","objectID":"/understanding-resource-limits-in-kubernetes/:2:0","series":null,"tags":["k8s","docker","cgroup"],"title":"深入理解K8s资源限制","uri":"/understanding-resource-limits-in-kubernetes/#资源限制"},{"categories":["cloud"],"content":" 内存限制通常情况下分析内存要比分析CPU简单一些，所以我从这里开始着手。我的一个目标是给大家展示内存在系统中是如何实现的，也就是Kubernetes对容器运行时（docker/containerd）所做的工作，容器运行时对Linux内核所做的工作。从分析内存资源限制开始也为后面分析CPU打好了基础。首先，让我们回顾一下前面的例子： resources: requests: memory: 50Mi limits: memory: 100Mi 单位后缀Mi表示的是MiB，所以这个资源对象定义了这个容器需要50MiB并且最多能使用100MiB的内存。当然还有其他单位可以进行表示。为了了解如何用这些值是来控制容器进程，我们首先创建一个没有配置内存限制的Pod: $ kubectl run limit-test --image=busybox --command -- /bin/sh -c \"while true; do sleep 2; done\" deployment.apps \"limit-test\" created 用Kubectl命令我们可以验证这个Pod是没有资源限制的： $ kubectl get pods limit-test-7cff9996fc-zpjps -o=jsonpath='{.spec.containers[0].resources}' map[] Kubernetes最酷的一点是你可以跳到系统以外的角度来观察每个构成部分，所以我们登录到运行Pod的节点，看看Docker是如何运行这个容器的： $ docker ps | grep busy | cut -d' ' -f1 5c3af3101afb $ docker inspect 5c3af3101afb -f \"{{.HostConfig.Memory}}\" 0 这个容器的.HostConfig.Memory域对应了docker run时的--memory参数，0值表示未设定。Docker会对这个值做什么？为了控制容器进程能够访问的内存数量，Docker配置了一组control group，或者叫cgroup。Cgroup在2008年1月时合并到Linux 2.6.24版本的内核。它是一个很重要的话题。我们说cgroup是容器的一组用来控制内核如何运行进程的相关属性集合。针对内存、CPU和各种设备都有对应的cgroup。Cgroup是具有层级的，这意味着每个cgroup拥有一个它可以继承属性的父亲，往上一直直到系统启动时创建的root cgroup。 Cgroup可以通过/proc和/sys伪文件系统轻松查看到，所以检查容器如何配置内存的cgroup就很简单了。在容器的Pid namespace里，根进程的pid为1，但是namespace以外它呈现的是系统级pid，我们可以用来查找它的cgroups： $ ps ax | grep /bin/sh 9513 ? Ss 0:00 /bin/sh -c while true; do sleep 2; done $ sudo cat /proc/9513/cgroup ... 6:memory:/kubepods/burstable/podfbc202d3-da21-11e8-ab5e-42010a80014b/0a1b22ec1361a97c3511db37a4bae932d41b22264e5b97611748f8b662312574 我列出了内存cgroup，这正是我们所关注的。你在路径里可以看到前面提到的cgroup层级。一些比较重要的点是：首先，这个路径是以kubepods开始的cgroup，所以我们的进程继承了这个group的每个属性，还有burstable的属性（Kubernetes将Pod设置为burstable QoS类别）和一组用于审计的Pod表示。最后一段路径是我们进程实际使用的cgroup。我们可以把它追加到/sys/fs/cgroups/memory后面查看更多信息： $ ls -l /sys/fs/cgroup/memory/kubepods/burstable/podfbc202d3-da21-11e8-ab5e-42010a80014b/0a1b22ec1361a97c3511db37a4bae932d41b22264e5b97611748f8b662312574 ... -rw-r--r-- 1 root root 0 Oct 27 19:53 memory.limit_in_bytes -rw-r--r-- 1 root root 0 Oct 27 19:53 memory.soft_limit_in_bytes 再一次，我只列出了我们所关心的记录。我们暂时不关注memory.soft_limit_in_bytes，而将重点转移到memory.limit_in_bytes属性，它设置了内存限制。它等价于Docker命令中的--memory参数，也就是Kubernetes里的内存资源限制。我们看看： $ sudo cat /sys/fs/cgroup/memory/kubepods/burstable/podfbc202d3-da21-11e8-ab5e-42010a80014b/0a1b22ec1361a97c3511db37a4bae932d41b22264e5b97611748f8b662312574/memory.limit_in_bytes 9223372036854771712 这是没有设置资源限制时我的节点上显示的情况。这里有对它的一个简单的解释(https://unix.stackexchange.com/questions/420906/what-is-the-value-for-the-cgroups-limit-in-bytes-if-the-memory-is-not-restricte)。 所以我们看到如果没有在Kubernetes里设置内存限制的话，会导致Docker设置HostConfig.Memory值为0，并进一步导致容器进程被放置在默认值为\"no limit\"的memory.limit_in_bytes内存cgroup下。我们现在创建使用100MiB内存限制的Pod： $ kubectl run limit-test --image=busybox --limits \"memory=100Mi\" --command -- /bin/sh -c \"while true; do sleep 2; done\" deployment.apps \"limit-test\" created 我们再一次使用kubectl验证我们的资源配置： $ kubectl get pods limit-test-5f5c7dc87d-8qtdx -o=jsonpath='{.spec.containers[0].resources}' map[limits:map[memory:100Mi] requests:map[memory:100Mi]] 你会注意到除了我们设置的limits外，Pod还增加了requests。当你设置limits而没有设置requests时，Kubernetes默认让requests等于limits。如果你从调度器的角度看这是非常有意义的。我会在下面进一步讨论requests。当这个Pod启动后，我们可以看到Docker如何配置的容器以及这个进程的内存cgroup： $ docker ps | grep busy | cut -d' ' -f1 8fec6c7b6119 $ docker inspect 8fec6c7b6119 --format '{{.HostConfig.Memory}}' 104857600 $ ps ax | grep /bin/sh 29532 ? Ss 0:00 /bin/sh -c while true; do sleep 2; done $ sudo cat /proc/29532/cgroup ... 6:memory:/kubepods/burstable/pod88f89108-daf7-11e8-b1e1-42010a800070/8fec6c7b61190e74cd9f88286181dd5fa3bbf9cf33c947574eb61462bc254d11 $ sudo cat /sys/fs/cgroup/memory/kubepods/burstable/pod88f89108-daf7-11e8-b1e1-42010a800070/8fec6c7b61190e74cd9f88286181dd5fa3bbf9cf33c947574eb61462bc254d11/memory.limit_in_bytes 104857600 正如你所见，Docker基于我们的containerSpec正确地设置了这个进程的内存cgroup。但是这对于运行时意味着什么？Linux内存管理是一个复杂的话题，Kubernetes工程师需要知道的是：当一个宿主机遇到了内存资源压力时，内核可能会有选择性地杀死进程。如果一个使用了多于限制内存的进程会有更高几率被杀死。因为Kubernetes的任务是尽可能多地向这些节点上安排Pod，这会导致节点内存压力异常。如果你的容器使用了过多内存，那么它很可能会被oom-killed。如果Docker收到了内核的通知，Kubernetes会找到这个容器并依据设置尝试重启这个Pod。 所以Kubernete","date":"Jan 09, 2019","objectID":"/understanding-resource-limits-in-kubernetes/:2:1","series":null,"tags":["k8s","docker","cgroup"],"title":"深入理解K8s资源限制","uri":"/understanding-resource-limits-in-kubernetes/#内存限制"},{"categories":["cloud"],"content":" CPU限制CPU 资源限制比内存资源限制更复杂，原因将在下文详述。幸运的是 CPU 资源限制和内存资源限制一样都是由 cgroup 控制的，上文中提到的思路和工具在这里同样适用，我们只需要关注他们的不同点就行了。首先，让我们将 CPU 资源限制添加到之前示例中的 yaml： resources: requests: memory: 50Mi cpu: 50m limits: memory: 100Mi cpu: 100m 单位后缀 m 表示千分之一核，也就是说 1 Core = 1000m。因此该资源对象指定容器进程需要 50/1000 核（5%）才能被调度，并且允许最多使用 100/1000 核（10%）。同样，2000m 表示两个完整的 CPU 核心，你也可以写成 2 或者 2.0。为了了解 Docker 和 cgroup 如何使用这些值来控制容器，我们首先创建一个只配置了 CPU requests 的 Pod： $ kubectl run limit-test --image=busybox --requests \"cpu=50m\" --command -- /bin/sh -c \"while true; do sleep 2; done\" deployment.apps \"limit-test\" created 通过 kubectl 命令我们可以验证这个 Pod 配置了 50m 的 CPU requests： $ kubectl get pods limit-test-5b4c495556-p2xkr -o=jsonpath='{.spec.containers[0].resources}' map[requests:map[cpu:50m]] 我们还可以看到 Docker 为容器配置了相同的资源限制： $ docker ps | grep busy | cut -d' ' -f1 f2321226620e $ docker inspect f2321226620e --format '{{.HostConfig.CpuShares}}' 51 这里显示的为什么是 51，而不是 50？这是因为 Linux cgroup 和 Docker 都将 CPU 核心数分成了 1024 个时间片（shares），而 Kubernetes 将它分成了 1000 个 shares。 shares 用来设置 CPU 的相对值，并且是针对所有的 CPU（内核），默认值是 1024，假如系统中有两个 cgroup，分别是 A 和 B，A 的 shares 值是 1024，B 的 shares 值是 512，那么 A 将获得 1024/(1204+512)=66% 的 CPU 资源，而 B 将获得 33% 的 CPU 资源。 shares 有两个特点： 如果 A 不忙，没有使用到 66% 的 CPU 时间，那么剩余的 CPU 时间将会被系统分配给 B，即 B 的 CPU 使用率可以超过 33%。 如果添加了一个新的 cgroup C，且它的 shares 值是 1024，那么 A 的限额变成了 1024/(1204+512+1024)=40%，B 的变成了 20%。 从上面两个特点可以看出： 在闲的时候，shares 基本上不起作用，只有在 CPU 忙的时候起作用，这是一个优点。 由于 shares 是一个绝对值，需要和其它 cgroup 的值进行比较才能得到自己的相对限额，而在一个部署很多容器的机器上，cgroup 的数量是变化的，所以这个限额也是变化的，自己设置了一个高的值，但别人可能设置了一个更高的值，所以这个功能没法精确的控制 CPU 使用率。 与配置内存资源限制时 Docker 配置容器进程的内存 cgroup 的方式相同，设置 CPU 资源限制时 Docker 会配置容器进程的 cpu,cpuacct cgroup： $ ps ax | grep /bin/sh 60554 ? Ss 0:00 /bin/sh -c while true; do sleep 2; done $ sudo cat /proc/60554/cgroup ... 4:cpu,cpuacct:/kubepods/burstable/pode12b33b1-db07-11e8-b1e1-42010a800070/3be263e7a8372b12d2f8f8f9b4251f110b79c2a3bb9e6857b2f1473e640e8e75 $ ls -l /sys/fs/cgroup/cpu,cpuacct/kubepods/burstable/pode12b33b1-db07-11e8-b1e1-42010a800070/3be263e7a8372b12d2f8f8f9b4251f110b79c2a3bb9e6857b2f1473e640e8e75 total 0 drwxr-xr-x 2 root root 0 Oct 28 23:19 . drwxr-xr-x 4 root root 0 Oct 28 23:19 .. ... -rw-r--r-- 1 root root 0 Oct 28 23:19 cpu.shares Docker 容器的 HostConfig.CpuShares 属性映射到 cgroup 的 cpu.shares 属性，可以验证一下： $ sudo cat /sys/fs/cgroup/cpu,cpuacct/kubepods/burstable/podb5c03ddf-db10-11e8-b1e1-42010a800070/64b5f1b636dafe6635ddd321c5b36854a8add51931c7117025a694281fb11444/cpu.shares 51 你可能会很惊讶，设置了 CPU requests 竟然会把值传播到 cgroup，而在上一篇文章中我们设置内存 requests 时并没有将值传播到 cgroup。这是因为内存的 soft limit 内核特性对 Kubernetes 不起作用，而设置了 cpu.shares 却对 Kubernetes 很有用。后面我会详细讨论为什么会这样。现在让我们先看看设置 CPU limits 时会发生什么： $ kubectl run limit-test --image=busybox --requests \"cpu=50m\" --limits \"cpu=100m\" --command -- /bin/sh -c \"while true; do sleep 2; done\" deployment.apps \"limit-test\" created 再一次使用 kubectl 验证我们的资源配置： $ kubectl get pods limit-test-5b4fb64549-qpd4n -o=jsonpath='{.spec.containers[0].resources}' map[limits:map[cpu:100m] requests:map[cpu:50m]] 查看对应的 Docker 容器的配置： $ docker ps | grep busy | cut -d' ' -f1 f2321226620e $ docker inspect 472abbce32a5 --format '{{.HostConfig.CpuShares}} {{.HostConfig.CpuQuota}} {{.HostConfig.CpuPeriod}}' 51 10000 100000 可以明显看出，CPU requests 对应于 Docker 容器的 HostConfig.CpuShares 属性。而 CPU limits 就不太明显了，它由两个属性控制：HostConfig.CpuPeriod 和 HostConfig.CpuQuota。Docker 容器中的这两个属性又会映射到进程的 cpu,couacct cgroup 的另外两个属性：cpu.cfs_period_us 和 cpu.cfs_quota_us。我们来看一下： $ sudo cat /sys/fs/cgroup/cpu,cpuacct/kubepods/burstable/pod2f1b50b6-db13-11e8-b1e1-42010a800070/f0845c65c3073e0b7b0b95ce0c1eb27f69d12b1fe2382b50096c4b59e78cdf71/cpu.cfs_period_us 100000 $ sudo cat /sys/fs/cgroup/cpu,cpuacct/kubepods/burstable/pod2f1b50b6-db13-11e8-b1e1-42010a800070/f0845c65c3073e0b7b0b95ce0c1eb27f69d12b1fe2382b50096c4b59e78cdf71/cpu.cfs_quota_us 10000 如我所说，这些值与容器配置中指定的值相同。但是这两个属性的值是如何从我们在 Pod 中设置的 100m cpu limits 得出的呢，他们是如何实现该 limits 的呢？这是因为 cpu requests 和 cpu limits 是使用两个独立的控制系统来实现的。Requests 使用的是 cpu shares 系统，cpu shares 将每个 CPU 核心划分为 1024 个时间片，并保证每个进程将获得固定比例份额的","date":"Jan 09, 2019","objectID":"/understanding-resource-limits-in-kubernetes/:2:2","series":null,"tags":["k8s","docker","cgroup"],"title":"深入理解K8s资源限制","uri":"/understanding-resource-limits-in-kubernetes/#cpu限制"},{"categories":["cloud"],"content":" 默认限制通过上文的讨论大家已经知道了忽略资源限制会对 Pod 产生负面影响，因此你可能会想，如果能够配置某个 namespace 默认的 request 和 limit 值就好了，这样每次创建新 Pod 都会默认加上这些限制。Kubernetes 允许我们通过 LimitRange 资源对每个命名空间设置资源限制。要创建默认的资源限制，需要在对应的命名空间中创建一个 LimitRange 资源。下面是一个例子： apiVersion: v1 kind: LimitRange metadata: name: default-limit spec: limits: - default: memory: 100Mi cpu: 100m defaultRequest: memory: 50Mi cpu: 50m - max: memory: 512Mi cpu: 500m - min: memory: 50Mi cpu: 50m type: Container 这里的几个字段可能会让你们有些困惑，我拆开来给你们分析一下。 limits 字段下面的 default 字段表示每个 Pod 的默认的 limits 配置，所以任何没有分配资源的 limits 的 Pod 都会被自动分配 100Mi limits 的内存和 100m limits 的 CPU。 defaultRequest 字段表示每个 Pod 的默认 requests 配置，所以任何没有分配资源的 requests 的 Pod 都会被自动分配 50Mi requests 的内存和 50m requests 的 CPU。 max 和 min 字段比较特殊，如果设置了这两个字段，那么只要这个命名空间中的 Pod 设置的 limits 和 requests 超过了这个上限和下限，就不会允许这个 Pod 被创建。我暂时还没有发现这两个字段的用途，如果你知道，欢迎在留言告诉我。 LimitRange 中设定的默认值最后由 Kubernetes 中的准入控制器 LimitRanger 插件来实现。准入控制器由一系列插件组成，它会在 API 接收对象之后创建 Pod 之前对 Pod 的 Spec - 字段进行修改。对于 LimitRanger 插件来说，它会检查每个 Pod 是否设置了 limits 和 requests，如果没有设置，就给它配置 LimitRange 中设定的默认值。通过检查 Pod 中的 annotations 注释，你可以看到 LimitRanger 插件已经在你的 Pod 中设置了默认值。例如： apiVersion: v1 kind: Pod metadata: annotations: kubernetes.io/limit-ranger: 'LimitRanger plugin set: cpu request for container limit-test' name: limit-test-859d78bc65-g6657 namespace: default spec: containers: - args: - /bin/sh - -c - while true; do sleep 2; done image: busybox imagePullPolicy: Always name: limit-test resources: requests: cpu: 100m 以上就是我对 Kubernetes 资源限制的全部见解，希望能对你有所帮助。如果你想了解更多关于 Kubernetes 中资源的 limits 和 requests、以及 linux cgroup 和内存管理的更多详细信息，可以查看我在文末提供的参考链接。 ","date":"Jan 09, 2019","objectID":"/understanding-resource-limits-in-kubernetes/:3:0","series":null,"tags":["k8s","docker","cgroup"],"title":"深入理解K8s资源限制","uri":"/understanding-resource-limits-in-kubernetes/#默认限制"},{"categories":["cloud"],"content":" 参考文档 https://medium.com/@betz.mark/understanding-resource-limits-in-kubernetes-cpu-time-9eff74d3161b https://medium.com/@betz.mark/understanding-resource-limits-in-kubernetes-memory-6b41e9a955f9 ","date":"Jan 09, 2019","objectID":"/understanding-resource-limits-in-kubernetes/:4:0","series":null,"tags":["k8s","docker","cgroup"],"title":"深入理解K8s资源限制","uri":"/understanding-resource-limits-in-kubernetes/#参考文档"},{"categories":["code"],"content":" 背景最近在做 prometheus 监控，需要将 prometheus 聚合数据打向 falcon, 写了个 falcon-adapter，部署到小集群上没问题，最后部署在线上集群 400 nodes k8s 集群出现 部分数据抓取不上， falcon断点严重， 部署节点 load 过高到达 50. ","date":"Nov 23, 2018","objectID":"/golang-pprof/:1:0","series":null,"tags":["golang"],"title":"记一次golang性能分析","uri":"/golang-pprof/#背景"},{"categories":["code"],"content":" 分析追溯某些数据发现 prometheus 采集不到，确认抓取组件没有问题 查看 falcon，数据采集时间过长，导致断点 查看 falcon-adapter, 有大量 TIME_WAIT 链接 无法确定哪一环出现问题，借助 pprof 进行性能分析 查看 falcon-adapter 得到火焰图如下 其中 falcon-adapter 的 metricFilter 操作占了 71.75%， 大部分是 regexp.MatchString 和 regexp.compile 占用的，这在一个转发程序是不正常的，通常应该 HTTP IO 操作占大头 在 metricFilter 中主要实现了prometheus metrics 的过滤，保存需要的 metrics, 其中有大量正则匹配，可能因此出现性能问题 func metricFilter(str string) string { for _, scope := range config.Scope { regStr := \"^\" + scope + \":\" if match, _ := regexp.MatchString(regStr, str); match { return scope } } return \"\" } 将所有正则操作全部替换为常规字符串操作后，得到火焰图如下 net 占有大部分 cpu 时间，meticFilter 只占用 1.67%！ ","date":"Nov 23, 2018","objectID":"/golang-pprof/:2:0","series":null,"tags":["golang"],"title":"记一次golang性能分析","uri":"/golang-pprof/#分析"},{"categories":["code"],"content":" 后记在大规模集群中编程一定要考虑性能问题，避免出现类似问题 善于利用 pprof 类似工具分析程序性能问题 ","date":"Nov 23, 2018","objectID":"/golang-pprof/:3:0","series":null,"tags":["golang"],"title":"记一次golang性能分析","uri":"/golang-pprof/#后记"},{"categories":["cloud"],"content":" Cadvisor内存使用率指标","date":"Nov 15, 2018","objectID":"/pod-memory-usage-in-k8s/:1:0","series":null,"tags":["k8s","docker","cadvisor"],"title":"Pod memory usage in k8s","uri":"/pod-memory-usage-in-k8s/#cadvisor内存使用率指标"},{"categories":["cloud"],"content":" Cadvisor中有关pod内存使用率的指标 指标 说明 container_memory_cache Number of bytes of page cache memory. container_memory_rss Size of RSS in bytes.(包括匿名映射页和交换区缓存) container_memory_swap Container swap usage in bytes. container_memory_usage_bytes Current memory usage in bytes,including all memory regardless ofwhen it was accessed. (包括 cache, rss, swap等) container_memory_max_usage_bytes Maximum memory usage recorded in bytes. container_memory_working_set_bytes Current working set in bytes. （工作区内存使用量=活跃的匿名与和缓存,以及file-baked页 \u003c=container_memory_usage_bytes） container_memory_failcnt Number of memory usage hits limits. container_memory_failures_total Cumulative count of memory allocation failures. 其中 container_memory_max_usage_bytes \u003e container_memory_usage_bytes \u003e= container_memory_working_set_bytes \u003e container_memory_rss ","date":"Nov 15, 2018","objectID":"/pod-memory-usage-in-k8s/:1:1","series":null,"tags":["k8s","docker","cadvisor"],"title":"Pod memory usage in k8s","uri":"/pod-memory-usage-in-k8s/#cadvisor中有关pod内存使用率的指标"},{"categories":["cloud"],"content":" Cadvisor中相关定义type MemoryStats struct { // Current memory usage, this includes all memory regardless of when it was // accessed. // Units: Bytes. Usage uint64 json:\"usage\" // Maximum memory usage recorded. // Units: Bytes. MaxUsage uint64 `json:\"max_usage\"` // Number of bytes of page cache memory. // Units: Bytes. Cache uint64 `json:\"cache\"` // The amount of anonymous and swap cache memory (includes transparent // hugepages). // Units: Bytes. RSS uint64 `json:\"rss\"` // The amount of swap currently used by the processes in this cgroup // Units: Bytes. Swap uint64 `json:\"swap\"` // The amount of working set memory, this includes recently accessed memory, // dirty memory, and kernel memory. Working set is \u003c= \"usage\". // Units: Bytes. WorkingSet uint64 `json:\"working_set\"` Failcnt uint64 `json:\"failcnt\"` ContainerData MemoryStatsMemoryData `json:\"container_data,omitempty\"` HierarchicalData MemoryStatsMemoryData `json:\"hierarchical_data,omitempty\"` } You might think that memory utilization is easily tracked with container_memory_usage_bytes, however, this metric also includes cached (think filesystem cache) items that can be evicted under memory pressure. The better metric is container_memory_working_set_bytes as this is what the OOM killer is watching for. To calculate container memory utilization we use: sum(container_memory_working_set_bytes{name!~“POD”}) by (name) kubelet 通过 watch container_memory_working_set_bytes 来判断是否OOM， 所以用 working set来评价容器内存使用量更科学 ","date":"Nov 15, 2018","objectID":"/pod-memory-usage-in-k8s/:1:2","series":null,"tags":["k8s","docker","cadvisor"],"title":"Pod memory usage in k8s","uri":"/pod-memory-usage-in-k8s/#cadvisor中相关定义"},{"categories":["cloud"],"content":" Cgroup中关于mem指标cgroup目录相关文件 文件名 说明 cadvisor中对应指标 memory.usage_in_bytes 已使用的内存量(包含cache和buffer)(字节)，相当于linux的used_meme container_memory_usage_bytes memory.limit_in_bytes 限制的内存总量(字节)，相当于linux的total_mem memory.failcnt 申请内存失败次数计数 memory.memsw.usage_in_bytes 已使用的内存和swap(字节) memory.memsw.limit_in_bytes 限制的内存和swap容量(字节) memory.memsw.failcnt 申请内存和swap失败次数计数 memory.stat 内存相关状态 memory.stat中包含有的内存信息 统计 描述 cadvisor中对应指标 cache 页缓存，包括 tmpfs（shmem），单位为字节 container_memory_cache rss 匿名和 swap 缓存，不包括 tmpfs（shmem），单位为字节 container_memory_rss mapped_file memory-mapped 映射的文件大小，包括 tmpfs（shmem），单位为字节 pgpgin 存入内存中的页数 pgpgout 从内存中读出的页数 swap swap 用量，单位为字节 container_memory_swap active_anon 在活跃的最近最少使用（least-recently-used，LRU）列表中的匿名和 swap 缓存，包括 tmpfs（shmem），单位为字节 inactive_anon 不活跃的 LRU 列表中的匿名和 swap 缓存，包括 tmpfs（shmem），单位为字节 active_file 活跃 LRU 列表中的 file-backed 内存，以字节为单位 inactive_file 不活跃 LRU 列表中的 file-backed 内存，以字节为单位 unevictable 无法再生的内存，以字节为单位 hierarchical_memory_limit 包含 memory cgroup 的层级的内存限制，单位为字节 hierarchical_memsw_limit 包含 memory cgroup 的层级的内存加 swap 限制，单位为字节 active_anon + inactive_anon = anonymous memory + file cache for tmpfs + swap cache = rss + file cache for tmpfs active_file + inactive_file = cache - size of tmpfs working set = usage - total_inactive(k8s根据workingset 来判断是否驱逐pod) mstat看到的active/inactive memory就分别是active list和inactive list中的内存大小。如果inactive list很大，表明在必要时可以回收的页面很多；而如果inactive list很小，说明可以回收的页面不多。 Active/inactive memory是针对用户进程所占用的内存而言的，内核占用的内存（包括slab）不在其中。 至于在源代码中看到的ACTIVE_ANON和ACTIVE_FILE，分别表示anonymous pages和file-backed pages。用户进程的内存页分为两种：与文件关联的内存（比如程序文件、数据文件所对应的内存页）和与文件无关的内存（比如进程的堆栈，用malloc申请的内存），前者称为file-backed pages，后者称为anonymous pages。File-backed pages在发生换页(page-in或page-out)时，是从它对应的文件读入或写出；anonymous pages在发生换页时，是对交换区进行读/写操作。 ","date":"Nov 15, 2018","objectID":"/pod-memory-usage-in-k8s/:2:0","series":null,"tags":["k8s","docker","cadvisor"],"title":"Pod memory usage in k8s","uri":"/pod-memory-usage-in-k8s/#cgroup中关于mem指标"},{"categories":["cloud"],"content":" 参考 https://blog.freshtracks.io/a-deep-dive-into-kubernetes-metrics-part-3-container-resource-metrics-361c5ee46e66 https://github.com/google/cadvisor/blob/08f0c2397cbca790a4db0f1212cb592cc88f6e26/info/v1/container.go#L338:6 ","date":"Nov 15, 2018","objectID":"/pod-memory-usage-in-k8s/:3:0","series":null,"tags":["k8s","docker","cadvisor"],"title":"Pod memory usage in k8s","uri":"/pod-memory-usage-in-k8s/#参考"},{"categories":["cloud"],"content":" 问题集群中有一个pod一直显示Terminating ","date":"Oct 26, 2018","objectID":"/pod-terminating-long-time/:1:0","series":null,"tags":["k8s"],"title":"Pod一直显示Terminating","uri":"/pod-terminating-long-time/#问题"},{"categories":["cloud"],"content":" eventNormal Scheduled 1h default-scheduler Successfully assigned feed-426565da19777e5d325f-5994dc5cff-znqmh to node01 Normal SuccessfulMountVolume 1h kubelet, node01 (combined from similar events): MountVolume.SetUp succeeded for volume \"lvm\" Normal Pulled 1h kubelet, node01 Container image already present on machine Normal Created 1h kubelet, node01 Created container Normal Started 1h kubelet, node01 Started container Warning Unhealthy 9m (x44 over 1h) kubelet, node01 Liveness probe failed: Get http://*:65318/state.json: dial tcp *.*.*.*:65318: getsockopt: connection refused Warning Unhealthy 9m (x45 over 1h) kubelet, node01 Readiness probe failed: Get http://*:65318/state.json: dial tcp *.*.*.*:65318: getsockopt: connection refused Normal Killing 9m kubelet, node01 Killing container with id docker://main:Need to kill Pod Warning FailedKillPod 5m (x2 over 7m) kubelet, node01 error killing pod: failed to \"KillPodSandbox\" for \"163f99a9-1aec-11e9-a7cd-246e96ab9970\" with KillPodSandboxError: \"rpc error: code = DeadlineExceeded desc = context deadline exceeded\" ","date":"Oct 26, 2018","objectID":"/pod-terminating-long-time/:1:1","series":null,"tags":["k8s"],"title":"Pod一直显示Terminating","uri":"/pod-terminating-long-time/#event"},{"categories":["cloud"],"content":" 日志kubelet: error killing pod: failed to \"KillPodSandbox\" for \"163f99a9-1aec-11e9-a7cd-246e96ab9970\" with KillPodSandboxError: \"rpc error: code = DeadlineExceeded desc = context deadline exceeded\" ","date":"Oct 26, 2018","objectID":"/pod-terminating-long-time/:1:2","series":null,"tags":["k8s"],"title":"Pod一直显示Terminating","uri":"/pod-terminating-long-time/#日志"},{"categories":["cloud"],"content":" 探究","date":"Oct 26, 2018","objectID":"/pod-terminating-long-time/:2:0","series":null,"tags":["k8s"],"title":"Pod一直显示Terminating","uri":"/pod-terminating-long-time/#探究"},{"categories":["cloud"],"content":" 查看进程ps aux |grep D #查看无法终止的进程（stat D） root 2626 0.0 0.0 0 0 ? Ds 14:42 0:00 [pause] ps afx |grep -C 10 2626 #显示父进程 root 2626 2603 0 14:42 ? 00:00:00 [pause] ps -ef |grep 2603 #查看父进程 root 2603 27573 0 14:42 ? 00:00:00 docker-containerd-shim -namespace moby -workdir /home/docker/containerd/daemon/io.containerd.runtime.v1.linux/moby/ba519a9f1a1102a922bcc74ced7a7fc9fd3f963feea4b8de ps -ef |grep 27573 root 27573 27553 0 2018 ? 17:02:40 docker-containerd --config /var/run/docker/containerd/containerd.toml ps -ef |grep 27553 root 27553 1 3 2018 ? 3-16:03:11 /usr/bin/dockerd --bip=10.126.64.193/26 --mtu=1500 -g /home/docker -D -H tcp://127.0.0.1:1983 -H unix:///var/run/docker.sock --tlsverify --iptables=false --storage-driver=devicemapper --storage-opt dm.override_udev_sync_check=true --storage-opt dm.datadev=/dev/vg_root/dmdata --storage-opt dm.metadatadev=/dev/vg_root/dmmeta --exec-opt native.cgroupdriver=cgroupfs docker ps |grep ba519a9f1a1 #查看docker ba519a9f1a11 k8s.gcr.io/pause-amd64:3.1 \"/pause\" 2 hours ago Up 2 hours k8s_POD_feed-426565da19777e5d325f-5994dc5cff-znqmh_ocean-feed_163f99a9-1aec-11e9-a7cd-246e96ab9970_0 ","date":"Oct 26, 2018","objectID":"/pod-terminating-long-time/:2:1","series":null,"tags":["k8s"],"title":"Pod一直显示Terminating","uri":"/pod-terminating-long-time/#查看进程"},{"categories":["cloud"],"content":" 可能原因可能是内核原因 https://stackoverflow.com/questions/34552232/cant-kill-processes-originating-in-a-docker-container 目前只有重启物理机才能解决 ","date":"Oct 26, 2018","objectID":"/pod-terminating-long-time/:2:2","series":null,"tags":["k8s"],"title":"Pod一直显示Terminating","uri":"/pod-terminating-long-time/#可能原因"},{"categories":["cloud"],"content":" Pod设置本地时区的两种方法我们下载的很多容器内的时区都是格林尼治时间，与北京时间差8小时，这将导致容器内的日志和文件创建时间与实际时区不符，有两种方式解决这个问题： 修改镜像中的时区配置文件 将宿主机的时区配置文件/etc/localtime使用volume方式挂载到容器中 ","date":"Oct 13, 2018","objectID":"/pod-timezone/:1:0","series":null,"tags":["k8s","docker"],"title":"Pod中时区设置","uri":"/pod-timezone/#pod设置本地时区的两种方法"},{"categories":["cloud"],"content":" 修改Dockfile修改前 $ docker run -d nginx:latest $ docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES ca7aacad1493 nginx \"nginx -g 'daemon of…\" About a minute ago Up About a minute 80/tcp inspiring_elbakyan $ docker exec -it inspiring_elbakyan date Wed Feb 13 06:51:41 UTC 2019 date Wed Feb 13 14:51:45 CST 2019 创建timezone-dockerfile FROM nginx RUN /bin/cp /usr/share/zoneinfo/Asia/Shanghai /etc/localtime \\ \u0026\u0026 echo 'Asia/Shanghai' \u003e/etc/timezone $ docker build -t timezone -f timezone-dockerfile . $ docker run -d timezone af39a27d8c8b48b80fb9b052144bd682d75d994dba2e03a02101514304f363d0 $ docker exec -it af39a27d8c8b date Wed Feb 13 15:05:14 CST 2019 $ date Wed Feb 13 15:05:16 CST 2019 ","date":"Oct 13, 2018","objectID":"/pod-timezone/:1:1","series":null,"tags":["k8s","docker"],"title":"Pod中时区设置","uri":"/pod-timezone/#修改dockfile"},{"categories":["cloud"],"content":" 挂载localtime文件第二种方式实现更简单，不需要更改镜像，只需要配置yaml文件，步骤如下： 创建测试pod，busybox-pod.yaml apiVersion: v1 kind: Pod metadata: name: busybox namespace: default spec: containers: - image: busybox:1.28.3 command: - sleep - \"3600\" imagePullPolicy: IfNotPresent name: busybox volumeMounts: - name: host-time mountPath: /etc/localtime readOnly: true volumes: - name: host-time hostPath: path: /etc/localtime restartPolicy: Always 测试时间 $ kubectl apply -f busybox-pod.yaml pod/busybox created $ kubectl exec -it busybox date Wed Feb 13 06:16:35 UTC 2019 $ date Wed Feb 13 14:16:39 CST 2019 将/etc/localtime挂载到pod中，配置如下: apiVersion: v1 kind: Pod metadata: name: busybox namespace: default spec: containers: - image: busybox:1.28.3 command: - sleep - \"3600\" imagePullPolicy: IfNotPresent name: busybox volumeMounts: - name: host-time mountPath: /etc/localtime readOnly: true volumes: - name: host-time hostPath: path: /etc/localtime restartPolicy: Always 测试时间 $ kubectl apply -f busybox-pod.yaml $ kubectl exec -it busybox date Wed Feb 13 14:17:50 CST 2019 #与当前时间一致 $ date Wed Feb 13 14:17:52 CST 2019 ","date":"Oct 13, 2018","objectID":"/pod-timezone/:1:2","series":null,"tags":["k8s","docker"],"title":"Pod中时区设置","uri":"/pod-timezone/#挂载localtime文件"},{"categories":["linux"],"content":" 背景由于项目要收集/var/log/messages的日志到es中，发现messages日志按天切割，但归档的时间却不一致，于是查了点资料探究下。 ","date":"Sep 27, 2018","objectID":"/var-log-message-logrotate/:1:0","series":null,"tags":["linux"],"title":"/var/log/message归档探究","uri":"/var-log-message-logrotate/#背景"},{"categories":["linux"],"content":" 介绍/var/log/messages是由journald生成的，流程如下 systemd --\u003e systemd-journald --\u003e ram DB --\u003e rsyslog -\u003e /var/log 当systemd启动后，systemd-journald也会立即启动。将日志存入RAM中，当rsyslog启动后会读取该RAM并完成筛选分类写入目录/var/log。 ","date":"Sep 27, 2018","objectID":"/var-log-message-logrotate/:2:0","series":null,"tags":["linux"],"title":"/var/log/message归档探究","uri":"/var-log-message-logrotate/#介绍"},{"categories":["linux"],"content":" 相关服务涉及的相关服务有： systemd-journald.service：最主要的信息收受者，由systemd提供； logrotate：主要进行日志的轮替功能 Centos7使用systemd提供的journalctl管理日志，那所有经由systemd启动服务的日志，会将该日志信息由systemd-journald.service以二进制的方式记录下来，之后再将信息发送到rsyslog.service作进一步的处理。 systemd-journald.service的记录主要都放置与内存中，因此性能较好，可以通过journalctl及systemctl status unit.service来查看各个服务的日志。 ","date":"Sep 27, 2018","objectID":"/var-log-message-logrotate/:2:1","series":null,"tags":["linux"],"title":"/var/log/message归档探究","uri":"/var-log-message-logrotate/#相关服务"},{"categories":["linux"],"content":" 相关配置journald配置文件 cat /etc/systemd/journald.conf # This file is part of systemd. # # systemd is free software; you can redistribute it and/or modify it # under the terms of the GNU Lesser General Public License as published by # the Free Software Foundation; either version 2.1 of the License, or # (at your option) any later version. # # Entries in this file show the compile time defaults. # You can change settings by editing this file. # Defaults can be restored by simply deleting this file. # # See journald.conf(5) for details. [Journal] #Storage=auto #Compress=yes #Seal=yes #SplitMode=uid #SyncIntervalSec=5m #RateLimitInterval=30s #RateLimitBurst=1000 #SystemMaxUse= #SystemKeepFree= #SystemMaxFileSize= #RuntimeMaxUse= #RuntimeKeepFree= #RuntimeMaxFileSize= #MaxRetentionSec= #MaxFileSec=1month #ForwardToSyslog=yes #默认转向syslog #ForwardToKMsg=no #ForwardToConsole=no #ForwardToWall=yes #TTYPath=/dev/console #MaxLevelStore=debug #MaxLevelSyslog=debug #MaxLevelKMsg=notice #MaxLevelConsole=info #MaxLevelWall=emerg 目前，centos log由rsyslog管理，设置文件/var/lib/rsyslog并兼容syslog的配置文件 其中messages文件记录系统日志，包括mail、定时任务、系统异常等（*.info;mail.none;authpriv.none;cron.none /var/log/messages） Logrotate实现日志切割，具体由CRON实现 logrotate配置文件 cat /etc/cron.daily/logrotate #!/bin/sh /usr/sbin/logrotate -s /var/lib/logrotate/logrotate.status /etc/logrotate.conf EXITVALUE=$? if [ $EXITVALUE != 0 ]; then /usr/bin/logger -t logrotate \"ALERT exited abnormally with [$EXITVALUE]\" fi exit 0 # 实际运行时，Logrotate会调用配置文件「/etc/logrotate.conf」 # see \"man logrotate\" for details # rotate log files weekly weekly # 每月归档一次 # keep 4 weeks worth of backlogs rotate 4 # 归档4个周期 # create new (empty) log files after rotating old ones create # use date as a suffix of the rotated file dateext # uncomment this if you want your log files compressed #compress # 默认不压缩 # RPM packages drop log rotation information into this directory include /etc/logrotate.d #包含其下配置文件 # no packages own wtmp and btmp -- we'll rotate them here /var/log/wtmp { monthly create 0664 root utmp minsize 1M rotate 1 } /var/log/btmp { missingok monthly create 0600 root utmp rotate 1 } # system-specific logs may be also be configured here. 设置特殊文件的归档方式 cat /etc/logrotate.d/syslog /var/log/cron /var/log/maillog /var/log/messages /var/log/secure /var/log/spooler { daily rotate 4 compress delaycompress # 延迟一个周期压缩 missingok # 日志丢失不报错 sharedscripts # 运行postrotate脚本，作用是在所有日志都轮转后统一执行一次脚本。如果没有配置这个，那么每个日志轮转后都会执行一次脚本 postrotate # 在logrotate转储之后需要执行的指令，例如重新启动 (kill -HUP) 某个服务！必须独立成行 /bin/kill -HUP `cat /var/run/syslogd.pid 2\u003e /dev/null` 2\u003e /dev/null || true endscript } messages中日志生成时间大多是晚上3点多，这是由cron控制的 cat /etc/anacrontab # /etc/anacrontab: configuration file for anacron # See anacron(8) and anacrontab(5) for details. SHELL=/bin/sh PATH=/sbin:/bin:/usr/sbin:/usr/bin MAILTO=root # the maximal random delay added to the base delay of the jobs RANDOM_DELAY=45 # 随机延迟最大时间 # the jobs will be started during the following hours only START_HOURS_RANGE=3-22 # 3点到22点执行 #period in days delay in minutes job-identifier command 1 5 cron.daily nice run-parts /etc/cron.daily # 第一天执行，延迟5分钟 7 25 cron.weekly nice run-parts /etc/cron.weekly @monthly 45 cron.monthly nice run-parts /etc/cron.monthly # 日志生成时间在03:05~03:50 随机延迟时间 5~5+45 ","date":"Sep 27, 2018","objectID":"/var-log-message-logrotate/:2:2","series":null,"tags":["linux"],"title":"/var/log/message归档探究","uri":"/var-log-message-logrotate/#相关配置"},{"categories":["cloud"],"content":" 添加时区 设置TZ与安装tzdata 添加work用户 addgroup -S work \u0026\u0026 adduser -S -G work work -s /bin/sh 设置字符格式 设置环境变量LANG与LC_ALL Dockerfile如下： FROM alpine ENV TZ=Asia/Shanghai \\ LANG=en_US.UTF-8 \\ LC_ALL=en_US.UTF8 RUN apk update \u0026\u0026 \\ apk add --no-cache tzdata \u0026\u0026 \\ addgroup -S work \u0026\u0026 adduser -S -G work work -s /bin/bash \u0026\u0026 \\ rm -rf /var/cache/apk/* /tmp/* /var/tmp/* $HOME/.cache ","date":"Aug 18, 2018","objectID":"/apline-timezone/:0:0","series":null,"tags":["docker"],"title":"apline镜像添加时区与字符设置","uri":"/apline-timezone/#"},{"categories":["cloud"],"content":" 之前的做法在 Docker 17.05 版本之前，我们构建 Docker 镜像时，通常会采用两种方式： ","date":"Aug 16, 2018","objectID":"/multi-stage-dockerfile/:1:0","series":null,"tags":["docker"],"title":"Docker多阶段构建","uri":"/multi-stage-dockerfile/#之前的做法"},{"categories":["cloud"],"content":" 全部放入一个 Dockerfile一种方式是将所有的构建过程编包含在一个 Dockerfile 中，包括项目及其依赖库的编译、测试、打包等流程，这里可能会带来的一些问题： Dockerfile 特别长，可维护性降低 镜像层次多，镜像体积较大，部署时间变长 源代码存在泄露的风险 例如编写 app.go 文件，该程序输出 Hello World! package main import \"fmt\" func main(){ fmt.Printf(\"Hello World!\"); } 编写 Dockerfile.one 文件 FROM golang:1.9-alpine RUN apk --no-cache add git ca-certificates WORKDIR /go/src/github.com/go/helloworld/ COPY app.go . RUN go get -d -v github.com/go-sql-driver/mysql \\ \u0026\u0026 CGO_ENABLED=0 GOOS=linux go build -a -installsuffix cgo -o app . \\ \u0026\u0026 cp /go/src/github.com/go/helloworld/app /root WORKDIR /root/ CMD [\"./app\"] 构建镜像 $ docker build -t go/helloworld:1 -f Dockerfile.one . ","date":"Aug 16, 2018","objectID":"/multi-stage-dockerfile/:1:1","series":null,"tags":["docker"],"title":"Docker多阶段构建","uri":"/multi-stage-dockerfile/#全部放入一个-dockerfile"},{"categories":["cloud"],"content":" 分散到多个 Dockerfile另一种方式，就是我们事先在一个 Dockerfile 将项目及其依赖库编译测试打包好后，再将其拷贝到运行环境中，这种方式需要我们编写两个 Dockerfile 和一些编译脚本才能将其两个阶段自动整合起来，这种方式虽然可以很好地规避第一种方式存在的风险，但明显部署过程较复杂。 例如，编写 Dockerfile.build 文件 FROM golang:1.9-alpine RUN apk --no-cache add git WORKDIR /go/src/github.com/go/helloworld COPY app.go . RUN go get -d -v github.com/go-sql-driver/mysql \\ \u0026\u0026 CGO_ENABLED=0 GOOS=linux go build -a -installsuffix cgo -o app . 编写 Dockerfile.copy 文件 FROM alpine:latest RUN apk --no-cache add ca-certificates WORKDIR /root/ COPY app . CMD [\"./app\"] 新建 build.sh #!/bin/sh echo Building go/helloworld:build docker build -t go/helloworld:build . -f Dockerfile.build docker create --name extract go/helloworld:build docker cp extract:/go/src/github.com/go/helloworld/app ./app docker rm -f extract echo Building go/helloworld:2 docker build --no-cache -t go/helloworld:2 . -f Dockerfile.copy rm ./app 现在运行脚本即可构建镜像 $ chmod +x build.sh $ ./build.sh 对比两种方式生成的镜像大小 $ docker image ls REPOSITORY TAG IMAGE ID CREATED SIZE go/helloworld 2 f7cf3465432c 22 seconds ago 6.47MB go/helloworld 1 f55d3e16affc 2 minutes ago 295MB ","date":"Aug 16, 2018","objectID":"/multi-stage-dockerfile/:1:2","series":null,"tags":["docker"],"title":"Docker多阶段构建","uri":"/multi-stage-dockerfile/#分散到多个-dockerfile"},{"categories":["cloud"],"content":" 使用多阶段构建为解决以上问题，Docker v17.05 开始支持多阶段构建 ( multistage builds )。使用多阶段构建我们就可以很容易解决前面提到的问题，并且只需要编写一个 Dockerfile ： 例如，编写 Dockerfile 文件 FROM golang:1.9-alpine as builder RUN apk --no-cache add git WORKDIR /go/src/github.com/go/helloworld/ RUN go get -d -v github.com/go-sql-driver/mysql COPY app.go . RUN CGO_ENABLED=0 GOOS=linux go build -a -installsuffix cgo -o app . FROM alpine:latest as prod RUN apk --no-cache add ca-certificates WORKDIR /root/ COPY --from=0 /go/src/github.com/go/helloworld/app . CMD [\"./app\"] 构建镜像 $ docker build -t go/helloworld:3 . 对比三个镜像大小 $ docker image ls REPOSITORY TAG IMAGE ID CREATED SIZE go/helloworld 3 d6911ed9c846 7 seconds ago 6.47MB go/helloworld 2 f7cf3465432c 22 seconds ago 6.47MB go/helloworld 1 f55d3e16affc 2 minutes ago 295MB 很明显使用多阶段构建的镜像体积小，同时也完美解决了上边提到的问题。 只构建某一阶段的镜像 我们可以使用 as 来为某一阶段命名，例如 FROM golang:1.9-alpine as builder 例如当我们只想构建 builder 阶段的镜像时，我们可以在使用 docker build 命令时加上 –target 参数即可 $ docker build --target builder -t username/imagename:tag . 构建时从其他镜像复制文件 上面例子中我们使用 COPY –from=0 /go/src/github.com/go/helloworld/app . 从上一阶段的镜像中复制文件，我们也可以复制任意镜像中的文件。 $ COPY --from=nginx:latest /etc/nginx/nginx.conf /nginx.conf ","date":"Aug 16, 2018","objectID":"/multi-stage-dockerfile/:2:0","series":null,"tags":["docker"],"title":"Docker多阶段构建","uri":"/multi-stage-dockerfile/#使用多阶段构建"},{"categories":null,"content":"Hi there, I’m qingwave 👋 🔭 I’m working on CloudNative, Kubernetes, Prometheus… 📫 Reach me Here, Github or Email Projects weave, Golang+Vue3 application starter mossdb, an in-memory, persistent, and embedded key-value database in Go gocorex, a collection golang useful utils for golang application, distributed system and microservice mycni, a simple CNI plugin for Kubernetes ring, ping but with rust ","date":"Jan 01, 0001","objectID":"/about/:0:0","series":null,"tags":null,"title":"About","uri":"/about/#"},{"categories":null,"content":"在这里留下你的足迹:) ","date":"Jan 01, 0001","objectID":"/guestbook/:0:0","series":null,"tags":null,"title":"Guestbook","uri":"/guestbook/#"}]